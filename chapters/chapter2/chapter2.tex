\documentclass[../../thesis.tex]{subfiles}

% chktex-file 18
\begin{document}

\chapter{%
Critical Point-Finding Algorithms
}\chapterlabel{two}
\onlyinsubfile{\begin{KeepFromToc}
		\tableofcontents
		\clearpage
		\listoffigures
		\listofalgorithms{}
	\end{KeepFromToc}}
\onlyinsubfile{\clearpage}
\onlyinsubfile{\linenumbers}

\section{Chapter Summary}\sectionlabel{summary}

In this chapter,
we define two classes of algorithms for finding critical points:
gradient norm minimization
and Newton methods.
All of these examples are
numerical, iterative methods
based on linear approximations of the gradient function
given by the Hessian function.
We refer to them as \emph{second-order}
critical point-finding algorithms

While the analytical critical points exactly satisfy
the analytical system of (possibly non-linear) equations
\begin{equation}\equationlabel{gradienteqns}
	\grad{L}{\theta} = 0
\end{equation}
\noindent numerical approaches cannot guarantee
exact equality.
Instead, they can at best guarantee \emph{approximate} equality,
\begin{equation}\equationlabel{gradienteps}
	\sgn{L}{\theta} \leq \eps
\end{equation}
\noindent for some $\eps > 0$.
So our algorithms will actually recover approximate critical points:

\begin{definition}{$\eps$-Critical Points}{epscritical}
	The set of all \emph{$\eps$-critical points} of a loss function $L$
	on a domain $\Theta$ for a fixed $\eps\geq0$ is denoted $\Theta^L_{\ecp}$
	and defined as
	\begin{equation}\equationlabel{def-epscp}
		\Theta^L_{\ecp} \defeq
		\Set{\theta \in \Theta \suchthat \sgn{L}{\theta} \leq \eps}
	\end{equation}
	When unambiguous, the super-script $L$ will be omitted.
	See \defref{critical}.
\end{definition}

If only introduced in the case of large-dimensional $\Theta$
and complex loss function $L$,
this definition and the problem setup can be somewhat intimidating.
\sectionref{sqrt} demonstrates a precise analogy
between finding critical points of complicated loss functions
and a fundamental arithmetic operation: taking the square root.

The lessons of this analogy are used to motivate
each of three algorithm classes in turn:
gradient norm minimization (\sectionref{gnm}),
exact Newton methods (\sectionref{exact}),
and inexact Newton methods (\sectionref{inexact}).
Practical versions of Newton methods
for high-dimensional problems
require additional tricks,
which are reviewed in \sectionref{practical}.

\section{Optimization Approach to Taking Square Roots}\sectionlabel{sqrt}

Addition ($+$) and multiplication ($\times$)
are simple operations, in the following sense:
given exact binary representations for two numbers,
an exact binary representation for the result of $+$ or $\times$
applied to those two numbers can be obtained in finite time\footnote{%
Specifically, addition is $O(n)$ and
multiplication is $O(n\log n)$~\cite{harvey2019},
courtesy of the Fast Fourier Transform and the convolution theorem.}.
The symbols $a + b$ and $a \times b$ represent the exact, finite output
of a concrete, finite-time algorithm.
That is, both operations define closed monoids over
binary strings of
(importantly!) finite length.

This is not true of division ($\div$), inversion ($\inv{}$),
taking the square root%
\footnote{We take the type signature of $\sqrt$ to be
$\sqrt{}\from\R_{\geq 0}\to\R_{\geq 0}$,
i.e.~we do not allow non-real or negative square roots.}
($\sqrt{}$).
In these cases, the operation is actually defined in terms of a promise regarding what happens
when the output of this operation is subjected to $\times$:
\begin{align}
    b = a \div c &\implies b \times c = a\equationlabel{div-promise}\\
    b = a^{-1} &\implies b \times a = 1\equationlabel{inv-promise} \\
    b = \sqrt{a} &\implies b \times b = a\equationlabel{sqrt-promise}
\end{align}
\noindent and for an exact representation of a number $a$,
the number that fulfills this promise might not have an exact representation,
as is famously the case for $\sqrt{2}$.
This makes algorithm design for these operations more complex than for $+$ and $\times$.

There are individual strategies for each,
but one general idea that turns out to be very powerful is
\emph{relaxation to an optimization problem}.
That is, we take the exact promises made above,
recognize them as statements of the optimality of the output for some criterion,
and then use that criterion to define an approximate promise,
true up to some tolerance $\eps$.

For $\sqrt{}$, we rearrange the promise~\ref{eq:sqrt-promise},
denoting its exact solution with a $\star$, into:
\begin{align}
    b^\star &\defeq \sqrt{a} \\
    b^\star \times b^\star &= a  \\
    b^\star \times b^\star - a &= 0  \\
    \snorm{b^\star \times b^\star - a} &=0\equationlabel{sqrt-opt-criter}
\end{align}

\noindent and then recognize that, due to the non-negativity of the norm,
\equationref{sqrt-opt-criter} is a statement of optimality about $b^\star$.
That is,
the value $b^\star$ that we are looking for is the
argument that minimizes the expression on the LHS:\@
\begin{equation}
	b^\star = \argmin_{b \in \R_{\geq 0}} \snorm{b \times b - a}
\end{equation}

Exactly minimizing this expression to arbitrary precision
might be impossible,
so we instead consider a set of approximate square roots,
to a tolerance $\eps$:
\begin{equation}\equationlabel{sqrteps}
	B_\eps \defeq
	\Set{b \in \R_{\geq 0} \suchthat \snorm{b^\star \times b^\star - a} \leq \eps }
\end{equation}
\noindent and so the problem of square root finding
is the problem of finding a member of $B_\eps$.

Analogously, the problem of critical point-finding
is the problem of \emph{gradient root finding}:
instead of trying to find a value $b$ such that
$b\times b - a$
is approximately $0$,
we are trying to find a value $\theta$ such that
$\grad{L}{\theta}$
is approximately $0$.
This $\theta$ is a member of $\Theta^L_{\ecp}$,
\defref{epscritical}.

\section{Gradient Norm Minimization}\sectionlabel{gnm}

The simplest way to attack the problem
of finding a point in
the sets defined by \equationref{def-epscp} or \equationref{sqrteps}
is to apply a first-order optimization algorithm.
In the case of critical point-finding,
we define an auxiliary function $g$,
equal to half the squared gradient norm:
\begin{equation}\equationlabel{def-g}
	g(\theta) = \frac{1}{2}\sgn{L}{\theta}
\end{equation}
\noindent and apply our first-order optimization algorithm to it.

This approach turns out to work poorly,
so it is not widely disseminated enough to have a standard name.
However, it is straightforward enough to be repeatedly discovered.
The earliest use, to the best of the author's knowledge,
was in chemical physics~\cite{mciver1972},
where the problem of finding saddle points arises
when computing transition states of chemical reactions.
They gave it the name \emph{gradient norm minimization},
which we use here.
For several reasons, an alternative method
for solving the problem of finding transition states
called eigenvector-following became popular instead~\cite{cerjan1981}.
Gradient norm minimization was then reinvented by two groups simultaneously in 2000
(\cite{angelani2000,broderix2000})
and its poor performance quickly noted~\cite{doye2002}.
The method was apparently then reinvented by the authors of%
~\cite{pennington2017}
and applied to the problem of finding critical points of neural networks.

While automatic differentiation can compute the derivatives of $g$ with no issue,
it is still instructive to know its gradients,
so let's derive them by hand.

\begin{theorem}{Gradient of Squared Gradient Norm}{gnm-grad}
	\emph{Let $L$ be a twice-differentiable function
	from $\Theta$ to $\R$
	and define $g(\theta) = \frac{1}{2}\sgn{L}{\theta}$.
	Then the gradient function of $g$, $\grad{g}{\theta}$,
	is given by
	\begin{equation}
		\grad{g}{\theta} = \hess{L}{\theta}\grad{L}{\theta}
	\end{equation}
	}

	If we apply $g$ to a point $\theta + \eps$,
	\begin{equation}
		g(\theta + \eps) = \frac{1}{2}\sgn{L}{\theta + \eps}
	\end{equation}
	\noindent we see that the right-hand-side
	contains the same expression as the left-hand-side
	of the definition of the Hessian function,
	\equationref{def-hessian}.
	We can therefore replace that expression
	with the decomposition on the right-hand-side:
	\begin{equation}
		g(\theta + \eps) = \frac{1}{2}\snorm{%
		\grad{L}{\theta} + \hess{L}{\theta}\epsilon + o(\epsilon)}
	\end{equation}
	\noindent Then, we expand the squared norm into its constituent terms,
	group like terms\footnote{%
		Importantly, any $\eps$ inside a squared norm makes that term $o(\eps)$,
	as does any $o(\eps)$.},
	and simplify:
	\begin{align}
		g(\theta + \eps) &= \frac{1}{2}\sgn{L}{\theta}
		+ \underbrace{\frac{1}{2}\snorm{\hess{L}{\theta}\eps}
		+ \frac{1}{2}\snorm{o(\eps)}}_{o(\eps)}
		+ \grad{L}{\theta}^\top\hess{L}{\theta}\eps
		+ o(\eps)\\
		g(\theta + \eps) &= g(\theta)
		+ {\left(\hess{L}{\theta}\grad{L}{\theta}\right)}^\top\epsilon
		+ o(\epsilon)
	\end{align}
	\noindent which implies,
	by pattern-matching to \defref{gradient},
	that the gradient of $g$ is
	\begin{equation}\equationlabel{gnm-grad}
		\grad{g}{\theta} = \hess{L}{\theta}\grad{L}{\theta}
	\end{equation}
\end{theorem}

The minimal form of gradient norm minimization
applies \algoref{gd} with these gradients,
giving rise to \algoref{gnm}.

\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $\alpha \in \R_{>0}$, $\theta_0 \in \R^N$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$\\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	$g \leftarrow \nabla{f}\left(\theta_t\right)$\\
	$H \leftarrow \nabla^2{f}\left(\theta_t\right)$\\
	$\theta_{t+1} \leftarrow \theta_t - \alpha Hg$\\
	$t \leftarrow t + 1$
    }%
    \algolabel{gnm}
    \caption{Gradient Norm Minimization by Gradient Descent}
\end{algorithm}

However, one advantage of the gradient norm minimization approach
is that, because it is framed as the optimization
of a scalar-valued function, any number of tricks
from the world of optimization can be applied to it:
stochastic methods, momentum, adaptive gradients,
and so on.
This is in contrast to the Newton methods reviewed below.

\section{Exact Newton Methods}\sectionlabel{exact}

Gradient norm minimization does not take any advantage
of the special structure of the problem,
namely that our optimization problem was introduced
as a proxy for solving a system of equations.
The other major class of algorithms
for critical point-finding,
\emph{Newton methods},
are specifically designed to solve systems of equations.
In this section,
we review the fundamental, textbook forms
of the method.
Then, in the following sections,
\sectionref{inexact} and \sectionref{practical},
we introduce extensions that make it feasible for
problems with large dimension and higher-order nonlinearity,
like many neural network loss functions.

\subsection{The Babylonian Algorithm}\sectionlabel{babylon}

The very earliest form of Newton's method
dates back perhaps to the 17th century BCE,
based on the evidence of an extremely precise
(to within 2e-6)
approximation to the square root of 2
on a Babylonian clay tablet.
It was certainly known as a method
for computing square roots by the first century CE,
when it appears in the work of Hero of Alexandria.
See~\cite{brown1999} for details.

The intuition for this algorithm is geometric:
if a square of area $x$
is smaller (larger) than a square with sides of length $\theta$,
then it is larger (smaller) than the square with sides of length $x \div \theta$.
In either case, the square with sides equal in length to the average
of $\theta$ and $x \div \theta$
is closer in area to $x$.
This improvement can be applied iteratively,
yielding the below algorithm:

\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $x, \theta_0 \in \R_{> 0}^2$\ \\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	    $\theta_{t+1} \leftarrow \frac{\theta_t}{2} + \frac{x \div \theta_t}{2}$
    }%
    \algolabel{babylon}
    \caption{Babylonian Algorithm for Square Roots}
\end{algorithm}
\noindent Recall that the motivation for considering the square root problem
was that $\sqrt{}$, unlike $+$ and $\times$,
does not have an exact finite-time algorithm.
This is also true of $\div$, which appears in \algoref{babylon}.
We will come back to this in \sectionref{div},
in order to better understand inexact Newton methods.

\subsection{Newton-Raphson}\sectionlabel{newton}

If we define
\begin{equation}
	f(\theta) = x - \theta^2
\end{equation}
\noindent then the update in \algoref{babylon}
can be written
\begin{align}
	\theta_{t+1} &= \theta_t - \frac{x - \theta_t^2}{2\theta_t}\\
	&= \theta_t - \frac{f(\theta_t)}{\grad{f}{\theta_t}}
\end{align}
\noindent which can be defined for any differentiable $f$,
so long as $\grad{f}{\theta_t}\neq 0$.
In this form, it is known as the \emph{Newton method}
or as \emph{Newton-Raphson}.
This is an interesting example of Stigler's law of eponymy%
~\cite{stigler1980}:
Thomas Simpson was the first to notice that
this algorithm could be generalized using calculus,
in 1740.
However, in part because Isaac Newton had developed an
algebraic version of the method
for generic polynomials and Joseph Raphson had refined it,
Joseph Fourier, Carl Runge, and other
prominent mathematicians mistakenly credited them with the technique.
See~\cite{kollerstrom1992} for details.

In the case of critical point-finding,
the equivalent of $f$ is a vector and already a gradient,
and so $\nabla f$ becomes the Hessian matrix
and the scalar division becomes multiplication by an inverse matrix.
The resulting algorithm is as follows:

\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $\theta_0 \in \R^N$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$\\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	$g \leftarrow \nabla{f}\left(\theta_t\right)$\\
	$H \leftarrow \nabla^2{f}\left(\theta_t\right)$\\
	$\theta_{t+1} \leftarrow \theta_t - \inv{H}g$\\
	$t \leftarrow t + 1$
    }%
    \algolabel{newton}
    \caption{Newton-Raphson}
\end{algorithm}

While the problem setup here was for equation solving,
this algorithm, in particular under the name Newton's method,
is better known as an optimization algorithm.
It works as such for the same reason that gradient descent
can be a global optimizer on smooth convex functions:
whenever all of the stationary points
are also global optima,
an algorithm for finding stationary points is also
an optimization algorithm.
Unlike gradient descent,
it is a second-order algorithm:
in addition to a gradient function, $\nabla f$,
it also needs a Hessian function, $\nabla^2 f$.

In the absence of geometric intuition
to explain why this is as sensible algorithm,
we turn to calculus.
First, we show that this is the best method possible for functions
with constant Hessian.

\begin{theorem}{Optimality of Newton Step}{newton}
	\emph{Let $f$ be a twice-differentiable function
	from $\Theta$ to $\R$
	with constant, non-singular Hessian function.
	From any point $\theta$, a point $\theta^\star$
	that solves the gradient equations~\ref{eq:gradienteqns}
	for $f$ can be obtained by computing
	\begin{equation}
		\theta^\star = \theta - \inv{\hess{f}{\theta}}\grad{f}{\theta}
	\end{equation}}

	The value of the gradient at a point $\theta + p$
	can be approximated by applying
	the Hessian at $\theta$ to $p$:
	\begin{equation}
		\grad{f}{\theta + p} = \grad{f}{\theta} + \hess{f}{\theta}p + o(p)
	\end{equation}
	\noindent By Taylor's theorem,
	$o(p)$ is governed by the integral of the third derivative
	of $f$ evaluated from $p$ to $\theta$.
	But the Hessian is constant, and so the third derivative is a
	zero tensor and	$o(p)$ can be replaced with $0$:
	\begin{equation}
		\grad{f}{\theta + p} = \grad{f}{\theta} + \hess{f}{\theta}p
	\end{equation}

	To solve the gradient equations,
	we need the right-hand side here to be equal to 0.
	Denoting the solution $p^\star$, we find that
	\begin{align}
		0 &= \grad{f}{\theta} + \hess{f}{\theta}p^\star\\
		-\inv{\hess{f}{\theta}}\grad{f}{\theta} = p^\star
	\end{align}
	\noindent from which the claim follows,
	where $\theta^\star = \theta + p^\star$.
\end{theorem}

We call the value $p^\star$ the \emph{pure Newton step}
or the \emph{exact Newton step}.
Note the absence of any step size.
We call the linear system of equations in $p$
defined by
\begin{equation}\equationlabel{newtonsystem}
	0 &= \grad{f}{\theta} + \hess{f}{\theta}p
\end{equation}
the \emph{Newton system} of equations.

For functions with non-constant Hessian,
i.e.~those which cannot be represented by a degree two polynomial,
this argument doesn't hold.
However, it holds approximately for functions whose Hessian
doesn't change too quickly.
In fact, the rate of convergence for this algorithm,
once the norm of $p$ is sufficiently small,
is quadratic:
for each iteration, the number of bits of precision doubles%
\footnote{In terms of the error, this is much faster than quadratic:
it is squared exponential.}.
This is to be contrasted with gradient descent,
which has at best linear improvement:
with each iteration, the number of bits of precision increases by a fixed amount%
\footnote{Again, in terms of the error, this is much faster:
it is exponential.}.

\subsection{Pseudo-Inverse Newton}\sectionlabel{pseudoinverse}

In defining the Babylonian algorithm,
\algoref{babylon},
it was important that the starting point not be $0$,
or else the update would be undefined due to division by $0$.
Equivalently,
the full Newton method \algoref{newton}
required that the Hessian was non-singular,
else the inverse is undefined.

But this is not a fundamental restriction,
which is lucky because the loss functions of neural networks
are highly singular~\cite{sagun2017}.
The key property of the update $p^\star$
was that it solved the Newton system
\begin{equation}\equationlabel{newtonsystem}
	0 = \grad{f}{\theta} + \hess{f}{\theta}p
\end{equation}
\noindent which has a solution whenever $-\grad{f}{\theta}$
is in the image of $\hess{f}{\theta}$,
the linear subspace of possible outputs of $\hess{f}{\theta}$,
defined below.

\begin{definition}{Image and Co-Image of a Matrix}{image}
	For a matrix $M$ in $\R^{m \times n}$,
	we define the \emph{image} of M, denoted $\im M$, as
	\begin{equation}
		\im M \defeq \Set{v \in \R^m \suchthat \exists w \suchthat Mw=v}
	\end{equation}
	and the \emph{co-image} of M, denoted $\co\im M$, as
	\begin{equation}
		\co\im M \defeq \Set{v \in \R^m \suchthat !\exists w \suchthat Mw=v}
	\end{equation}
	When the rank (\defref{kernel}) of $M$ is at least $m$,
	the co-image of $M$ is empty.
\end{definition}

In this case, the solution $p^\star$
can be obtained by applying the
(Moore-Penrose) pseudo-inverse of the Hessian
to the gradient.
We will define this pseudo-inverse
by means of the singular value decomposition.
We will need several properties
of this matrix later,
so we do this in detail.

\subsubsection{Pseudo-Inverses and the Singular Value Decomposition}

The singular value decomposition of a matrix
breaks down a matrix into the product,
or composition, of three matrices.
\begin{definition}{Singular Value Decomposition}{svd}
	For a matrix $M$ in $\R^{m \times n}$
	with rank $r$,
	we define the
	\emph{singular value decomposition} or SVD of $M$
	as the triple of matrices $V^\top$, $\Sigma$, $U$
	such that
	\begin{equation}
		M = U \Sigma V^\top
	\end{equation}
	where $V^\top \in \R^{r \times n}$,
	$\Sigma \in \R^{r \times r}$, and
	$U \in \R^{m \times r}$
	and $U$ and $V$ are orthonormal
	and $\Sigma$ is diagonal.
	The columns of $U$ are called the \emph{left-singular}
	vectors of $M$, the columns of $V$ the \emph{right-singular}.
	The diagonal entries of $\Sigma$ are the \emph{singular values} of $M$.
	This form is sometimes called the \emph{compact SVD}.
\end{definition}

It can be shown that every matrix has a
singular value decomposition~\cite{strang1993}.
The singular value decomposition arises
as the specialization of the First Isomorphism Theorem
to the category of vector spaces and linear maps.
We state it here in terms of functions,
for readers not familiar with category theory,
but it is more appropriately set in terms of morphisms.

\begin{theorem}{The First Isomorphism Theorem}{firstiso}
	\emph{Let $\cC$ be a collection of sets and all functions between them.
	For any function $f\from A \to B$, where $f$, $A$, and $B$ are members of the collection,
	there is a triple of functions $s$, $b$, and $i$ such that
	\begin{equation}
	      f = i \after b \after s
	\end{equation}
	where $s\from A \onto A/\sim$ is a surjection,
	$b\from A/\sim \to \im A$ is a bijection, and
	$i\from \im A \into B$ is an injection
	(surjectivty and injectivity indicated by arrow type).
	The elements of the equivalence relation $\sim$
	are given by pairs $(a, a^\prime) \in A \times A$
	where $f(a) = f(a^\prime)$.}
\end{theorem}

The First Isomorphism Theorem states that every function can be decomposed
into three constituent pieces:
an onto mapping, or surjection,
that classifies its inputs according to which output they are mapped to;
a one-to-one mapping, or bijection,
that picks the output corresponding to each class of input;
and an into mapping, or injection,
that inserts this output into the target set.

Though this decomposition is of limited use
for generic functions,
as opposed to structure-preserving functions
like group homomorphisms
or linear maps,
it is fruitful to consider a concrete example
from the world of generic functions
before connecting to the SVD.\@

\begin{example}{Decomposition of \texttt{is\_odd}}{isodd}
	Consider the function $\texttt{is\_odd}\from \N \to \cS$,
	where $\cS$ is the set of all strings in the English alphabet,
	that returns \enquote{True} if the number is odd,
	\enquote{False} otherwise.
	This function can be decomposed as
	\begin{equation}
		\texttt{is\_odd} = \texttt{to\_string} \after \texttt{to\_bool} \after {\%}_2
	\end{equation}
	where $\%_2\from\N \onto \Set{0, 1}$
	computes the value mod $2$ and
	\begin{equation*}
		\texttt{to\_string}(b) = %chktex 36
			\begin{cases}
				\mlq \text{False} \mrq & \text{if } b = \bot \\
				\mlq \text{True} \mrq & \text{if } b = \top
			\end{cases} \quad
		\texttt{to\_bool}(k) = %chktex 36
			\begin{cases}
			     \bot  & 	\text{if } k = 0 \\
			     \top  & 	\text{if } k = 1
			\end{cases}
	\end{equation*}
	This is summarized neatly by the commutative diagram below:
	\begin{center}
	    \begin{tikzcd}
		    \N \arrow[rr, "\texttt{is\_odd}"] \arrow[dd, "\%_2", two heads]
		    &  & \cS\\
		\ & \  & \\
		\Set{0, 1} \arrow[rr, "\texttt{to\_bool}"]
		&  & \mathbb{B} \arrow[uu, "\texttt{to\_string}", harpoon, hook]
	    \end{tikzcd}
	\end{center}
	Saying that this diagram \emph{commutes}
	is saying that any path from one domain to another
	results in the same function.
\end{example}

Now, let us view the SVD through this lens.
First, $V^\top$ is a surjection because
it is orthonormal and $r \leq n$.
Second, $U$ is an injection because
it is orthonormal and $m \geq r$.
Finally, $\Sigma$ is a bijection
because it is a diagonal matrix with non-zero diagonal entries.
This establishes that the (compact) SVD
is the same decomposition as in the First Isomorphism Theorem.
Furthermore, for linear functions,
two inputs can only be mapped to the same output
if they are both mapped to $0$.
If we take $0$ to be the representative of that equivalence class,
the target of the surjection becomes the co-kernel of M
(recall \defref{kernel}).
The target of the bijection is the image of M.
This set of relationships is summarized
in the commutative diagram below.

\begin{center}
    \begin{tikzcd}
	    \R^n \arrow[rr, "M" description] \arrow[dd, "V^\top" description, two heads]
	    &  & \R^m \\
	\ & \  & \\
	\co \ker M \arrow[rr, "\Sigma"]
	&  & \im M \arrow[uu, "U" description, harpoon, hook]
    \end{tikzcd}
\end{center}

We are now ready to define the pseudo-inverse.
We first define it for two simple classes of matrices,
then extend it for all matrices by means of the SVD.\@

\begin{definition}{Moore-Penrose Pseudo-Inverse}{pseudoinverse}
	Let $O$ be an orthogonal matrix.
	We define the pseudo-inverse of $O$,
	denoted $\pinv{O}$, as
	\begin{equation}
		\pinv{O} \defeq O^{\top}
	\end{equation}

	Let $D$ be a diagonal matrix of full rank.
	We define the pseudo-inverse of $D$ as
	\begin{equation}
		\pinv{D} \defeq \inv{D}
	\end{equation}

	Let $M$ be a matrix in $\R^{m \times n}$
	and let its singular value decomposition be the triple
	$V^\Top$, $\Sigma$, $U$.
	We define the \emph{pseudo-inverse} of $M$,
	denoted $\pinv{M}\from \R^m \to \R^n$, as
	\begin{align}
		\pinv{M}
		&\defeq \pinv{V^{\top}}\pinv{\Sigma}\pinv{U}\\
		&= V \inv{\Sigma} U^\top
	\end{align}
	This is sometimes known as the Moore-Penrose pseudo-inverse.
	The relationships between these matrices
	are summarized by the commutative diagram below.
	\begin{center}
	    \begin{tikzcd}
		    \R^n \arrow[rr, "M", bend left] \arrow[dd, "V^\top", swap, two heads, bend right]
		    &  & \R^m \arrow[dd, "U^\top", swap, two heads, bend right] \arrow[ll, "M^+", dotted, bend left]                       \\
		\ & \  & \\
		\co \ker M \arrow[uu, "V", harpoon, hook, bend right]  \arrow[rr, "\Sigma", bend right]
		&  & \im M \arrow[uu, "U", harpoon, hook, bend right] \arrow[ll, "\inv{\Sigma}", bend right]
	    \end{tikzcd}
	\end{center}
\end{definition}


\subsubsection{Defining Pseudo-Inverse Newton}

Let's return to the problem of solving
\equationref{newtonsystem}
for an update $p^{\star}$.
We see that whenever a vector $g$
is in the image of the Hessian, $\hess{f}{\theta}$,
we can apply the pseudoinverse, $\pinv{\hess{f}{\theta}}$,
to $g$ to obtain a vector that is in the pre-image of $g$
with respect to the Hessian.
We choose this as our update,
obtaining the algorithm below.

\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $\theta_0 \in \R^N$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$\\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	$g \leftarrow \nabla{f}\left(\theta_t\right)$\\
	$H \leftarrow \nabla^2{f}\left(\theta_t\right)$\\
	$\theta_{t+1} \leftarrow \theta_t - \pinv{H}g$\\
	$t \leftarrow t + 1$
    }%
    \algolabel{newtonPI}
    \caption{Pseudo-Inverse Newton}
\end{algorithm}

We will return to the problem of what happens
when the gradient is not in the image of the Hessian in \chapterref{three}.
This will turn out to be critically important for understanding
the behavior of Newton methods on neural network loss functions.

\section{Inexact Newton Methods}\sectionlabel{inexact}

\subsection{Optimization Approach to Division}\sectionlabel{div}

\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $x, \theta_0 \in \R^2$\ \\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	    $\theta_{t+1} \leftarrow \theta_t \times \left(2 - \theta_t \times x\right)$
    }%
    \algolabel{newtondiv}
    \caption{Newton Method for Division}
\end{algorithm}

\subsection{Linear Solvers for Inexact Newton}

\section{Practical Newton Methods}\sectionlabel{practical}

\subsection{Damped Newton}\sectionlabel{damped}

\subsection{Guarded Newton}\sectionlabel{guarded}

\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $\nabla f\from\R^N\to\R^N$,
    $p \in \R^N$,
    $\theta \in \R^N$,
    $\alpha \in \intervaloc{0}{1}$,
    $\beta \in (0, 1)$,
    $\rho \in (0, 1)$,
    $\rho^\prime \in (0, 1)$
    \DontPrintSemicolon
    \SetKwFunction{FCheck}{CheckConvergence}
    \SetKwProg{Fn}{Function}{:}{}
    \Fn{\FCheck{$\alpha, \rho$}}{%
        $\theta^\prime \leftarrow \theta + \alpha p$\;
        $s, s^\prime \leftarrow \sumsq{\grad{f}{\theta}} \sumsq{\grad{f}{\theta^\prime}}$\;
        $\Delta \leftarrow 2 \rho \alpha \cdot p^\top H \grad{f}{\theta}$\;
        \KwRet $s^\prime \leq s + \Delta$\;}
    \;
    converged $\leftarrow \texttt{CheckConvergence}\left(
    	1, \rho^\prime\right)$\;
    \If{$\mathrm{converged}$}{$\alpha \leftarrow 1$}
    \While{$\mathrm{not\ converged}$}{%
        converged $\leftarrow \texttt{CheckConvergence}\left(
            \alpha, \rho\right)$\;
        \If{$\mathrm{not\ converged}$}{$\alpha \leftarrow \beta \alpha$\;
        \If{$\alpha == 0$}{\textbf{break}}}
    }
    $\KwRet\ \alpha$
    \algolabel{roosta-btls}
    \caption{Backtracking Line Search for Newton-MR}
\end{algorithm}

\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $\theta_0 \in \R^N$,
    $f\from\R^N\to\R$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$,\
    $T\in \N$,\\
    $\texttt{maxit} \in \N$,
    $\texttt{maxxnorm} \in \R$,
    $\texttt{rtol} \in \R$,
    $\texttt{acondlim} \in \R$,
    $\alpha \in \intervaloc{0}{1}$,
    $\beta \in (0, 1)$,
    $\rho \in (0, 1)$,
    $\rho^\prime \in (0, 1)$
    $t \leftarrow 0$\\
    \While{$t < T$}{%
        $g \leftarrow \nabla{f}\left(\theta_t\right)$\\
        $H \leftarrow \nabla^2{f}\left(\theta_t\right)$\\
        $p \leftarrow \texttt{MRQLP}\left(
            H,\ -g,\ \texttt{maxit},\ \texttt{maxxnorm},\ \texttt{rtol},\ \texttt{acondlim}\right)$\\
        $\alpha \leftarrow \texttt{BTLS}\left(
            \nabla f, p, \theta_t, \alpha, \beta, \rho, \rho^\prime\right)$\\
        $\theta_{t+1} \leftarrow \theta_t + \alpha p$\\
	\If{$\theta_{t+1} == \theta_{t}$}{\textbf{break}}
        $t \leftarrow t + 1$
    }%
    \algolabel{nmr}
    \caption{Newton-MR}
\end{algorithm}


\onlyinsubfile{\printbibliography}

\end{document}

