\documentclass[../../thesis.tex]{subfiles}

% chktex-file 18
\begin{document}

\chapter{%
Second-Order Critical Point-Finding Algorithms
}\chapterlabel{two}
\onlyinsubfile{\begin{KeepFromToc}
		\tableofcontents
		\listoffigures
		\listofalgorithms{}
	\end{KeepFromToc}}
\onlyinsubfile{\clearpage}
\onlyinsubfile{\linenumbers}

\section{Chapter Summary}\sectionlabel{ch2-summary}

In order to characterize the local curvature properties
of neural network loss functions at critical points,
we need first to find critical points.
In this chapter,
we review two classes of algorithms for finding critical points:
gradient norm minimization methods
and Newton methods.
All of these algorithms are
numerical, iterative methods.
They are based on finding the zero values of linear approximations
of the gradient function.
These linear approximations are given by the Hessian function.
We refer to them as \emph{second-order}
critical point-finding algorithms.

While the analytical critical points exactly satisfy
the analytical system of (possibly non-linear) equations
\begin{equation}\equationlabel{gradienteqns}
	\grad{L}{\theta} = 0
\end{equation}
\noindent numerical approaches cannot guarantee
exact equality.
Instead, they can at best guarantee \emph{approximate} equality,
\begin{equation}\equationlabel{gradienteps}
	\sgn{L}{\theta} \leq \eps
\end{equation}
\noindent for some $\eps > 0$.
So our algorithms will actually recover approximate critical points:

\begin{definition}{$\eps$-Critical Points}{epscritical}
	The set of all \emph{$\eps$-critical points} of a loss function $L$
	on a domain $\Theta$ for a fixed $\eps\geq0$ is denoted $\Theta^L_{\ecp}$
	and defined as
	\begin{equation}\equationlabel{def-epscp}
		\Theta^L_{\ecp} \defeq
		\Set{\theta \in \Theta \suchthat \sgn{L}{\theta} \leq \eps}
	\end{equation}
	When unambiguous, the super-script $L$ will be omitted.
	See \defref{critical}.
\end{definition}

If only introduced in the case of large-dimensional $\Theta$
and complex loss function $L$,
this definition and the problem setup can be somewhat intimidating
and intuition can be hard to build.
\sectionref{sqrt} demonstrates a close analogy
between finding critical points
and a fundamental arithmetic operation:
taking the square root.
Specifically,
taking the square root will require us to define
approximate square roots,
set up a surrogate optimization problem based
on an inequality that relaxes an equality,
and develop an inexact, iterative method for scalar inversion%
\sectionref{div}.

This running example is used to intuitively motivate
each of three algorithm classes in turn:
gradient norm minimization methods (\sectionref{gnm}),
exact Newton methods (\sectionref{exact}),
and inexact Newton methods (\sectionref{inexact}).
Practical versions of Newton methods
for high-dimensional and non-linear problems
require additional tricks,
which are reviewed in \sectionref{practical}.

The presentation of the Newton methods
is intentionally didactic and lengthy.
This is because these methods are relatively unfamiliar
to the neural network community.
First,
second-order optimization methods,
like Newton methods for convex functions,
are not commonly used to train neural networks.
Second,
Newton methods for critical point-finding
are different in motivation
and implementation than those used in
convex optimization.
It is hoped that the
thorough elaboration of these methods
in this chapter will help ease
the incorporation of these methods
into the toolkits of future neural network researchers.

\section{Optimization Approach to Taking Square Roots}\sectionlabel{sqrt}

Addition ($+$) and multiplication ($\times$)
are simple operations, in the following sense:
given exact binary representations for two numbers,
an exact binary representation for the result of $+$ or $\times$
applied to those two numbers can be obtained in finite time\footnote{%
Specifically, for $n$-digit numbers addition is $O(n)$ and
multiplication is $O(n\log n)$~\cite{harvey2019},
courtesy of the Fast Fourier Transform and the convolution theorem.}.
The symbols $a + b$ and $a \times b$ represent the exact, finite output
of a concrete, finite-time algorithm.
That is, both operations define closed monoids over
binary strings of
(importantly!) finite length.

This is not true of division ($\div$), inversion ($\inv{}$),
or taking the square root%
\footnote{We take the type signature of $\sqrt$ to be
$\sqrt{}\from\R_{\geq 0}\to\R_{\geq 0}$,
i.e.~we do not allow non-real or negative square roots.}
($\sqrt{}$).
In these cases,
the operation is actually defined in terms of a promise regarding what happens
when the output of this operation is subjected to $\times$:
\begin{align}
    b = a \div c &\implies b \times c = a\equationlabel{div-promise}\\
    b = a^{-1} &\implies b \times a = 1\equationlabel{inv-promise} \\
    b = \sqrt{a} &\implies b \times b = a\equationlabel{sqrt-promise}
\end{align}
\noindent and for an exact representation of a number $a$,
the number that fulfills this promise might not have an exact representation,
as is famously the case for $\sqrt{2}$.
This makes algorithm design for these operations more complex than for $+$ and $\times$.

There are individual strategies for each,
but one general idea that turns out to be very powerful is
\emph{relaxation to an optimization problem}.
That is, we take the exact promises made above,
recognize them as statements of the optimality of the output for some criterion,
and then use that criterion to define an approximate promise,
true up to some tolerance $\eps$.

For $\sqrt{}$, we rearrange the promise~\ref{eq:sqrt-promise},
denoting its exact solution with a $\star$, into:
\begin{align}
    b^\star &\defeq \sqrt{a} \\
    b^\star \times b^\star &= a  \\
    b^\star \times b^\star - a &= 0  \\
    \snorm{b^\star \times b^\star - a} &=0\equationlabel{sqrt-opt-criter}
\end{align}

\noindent and then recognize that, due to the non-negativity of the norm,
\equationref{sqrt-opt-criter} is a statement of optimality about $b^\star$.
That is,
the value $b^\star$ that we are looking for is the
argument that minimizes the expression on the LHS:\@
\begin{equation}
	b^\star = \argmin_{b \in \R_{\geq 0}} \snorm{b \times b - a}
\end{equation}

Exactly minimizing this expression to arbitrary precision
might be impossible,
so we instead consider a set of approximate square roots,
to a tolerance $\eps$:
\begin{equation}\equationlabel{sqrteps}
	B_\eps \defeq
	\Set{b \in \R_{\geq 0} \suchthat \snorm{b \times b - a} \leq \eps }
\end{equation}
\noindent and so the problem of square root finding
is the problem of finding a member of $B_\eps$.

Analogously, the problem of critical point-finding
is the problem of \emph{gradient root finding}:
instead of trying to find a value $b$ such that
$b\times b - a$
is approximately $0$,
we are trying to find a value $\theta$ such that
$\grad{L}{\theta}$
is approximately $0$.
This $\theta$ is a member of $\Theta^L_{\ecp}$,
\defref{epscritical}.

\section{Gradient Norm Minimization}\sectionlabel{gnm}

The simplest way to attack the problem
of finding a point in
the sets defined by \equationref{def-epscp} or \equationref{sqrteps}
is to apply a first-order optimization algorithm.
In the case of critical point-finding,
we define an auxiliary function $g$,
equal to half the squared gradient norm:
\begin{equation}\equationlabel{def-g}
	g(\theta) = \frac{1}{2}\sgn{L}{\theta}
\end{equation}
\noindent and apply our first-order optimization algorithm to it.

This approach turns out to work poorly,
so it is not widely disseminated enough to have a standard name.
However, it is straightforward enough to be repeatedly discovered.
The earliest use, to the best of the author's knowledge,
was in chemical physics~\cite{mciver1972},
where the problem of finding saddle points arises
when computing transition states of chemical reactions.
They gave it the name \emph{gradient norm minimization},
which we use here.
For several reasons, an alternative method
for solving the problem of finding transition states
called eigenvector-following became popular instead~\cite{cerjan1981}.
Gradient norm minimization was then reinvented by two groups simultaneously in 2000
(\cite{angelani2000,broderix2000})
and its poor performance quickly noted~\cite{doye2002}.
The method was apparently then reinvented by the authors of%
~\cite{pennington2017}
and applied to the problem of finding critical points of neural networks.

While automatic differentiation can compute the derivatives of $g$ with no issue,
it is still instructive to know its gradients,
so let's derive them by hand.

\begin{theorem}{Gradient of Squared Gradient Norm}{gnm-grad}
	\emph{Let $L$ be a twice-differentiable function
	from $\Theta$ to $\R$
	and define $g(\theta) = \frac{1}{2}\sgn{L}{\theta}$.
	Then the gradient function of $g$, $\grad{g}{\theta}$,
	is given by
	\begin{equation}
		\grad{g}{\theta} = \hess{L}{\theta}\grad{L}{\theta}
	\end{equation}
	}
\end{theorem}

\noindent \textit{Proof of \thmref{gnm-grad}}:\\
If we apply $g$ to a point $\theta + \eps$,
\begin{equation}
	g(\theta + \eps) = \frac{1}{2}\sgn{L}{\theta + \eps}
\end{equation}
\noindent we see that the right-hand-side
contains the same expression as the left-hand-side
of the definition of the Hessian function,
\equationref{def-hessian}.
We can therefore replace that expression
with the decomposition on the right-hand-side:
\begin{equation}
	g(\theta + \eps) = \frac{1}{2}\snorm{%
	\grad{L}{\theta} + \hess{L}{\theta}\epsilon + o(\epsilon)}
\end{equation}
\noindent Then, we expand the squared norm into its constituent terms,
group like terms\footnote{%
	Importantly, any $\eps$ inside a squared norm makes that term $o(\eps)$,
as does any $o(\eps)$.},
and simplify:
\begin{align}
	g(\theta + \eps) &= \frac{1}{2}\sgn{L}{\theta}
	+ \underbrace{\frac{1}{2}\snorm{\hess{L}{\theta}\eps}
	+ \frac{1}{2}\snorm{o(\eps)}}_{o(\eps)}
	+ \grad{L}{\theta}^\top\hess{L}{\theta}\eps
	+ o(\eps)\\
	g(\theta + \eps) &= g(\theta)
	+ {\left(\hess{L}{\theta}\grad{L}{\theta}\right)}^\top\epsilon
	+ o(\epsilon)
\end{align}
\noindent which implies,
by pattern-matching to \defref{gradient},
that the gradient function of $g$ is
\begin{equation}\equationlabel{gnm-grad}
	\grad{g}{\theta} = \hess{L}{\theta}\grad{L}{\theta}
\end{equation}
\QED\\

The minimal form of gradient norm minimization
applies \algoref{gd} with these gradients,
giving rise to \algoref{gnm}.
\\
\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $\alpha \in \R_{>0}$, $\theta_0 \in \R^N$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$\\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	$g \leftarrow \nabla{f}\left(\theta_t\right)$\\
	$H \leftarrow \nabla^2{f}\left(\theta_t\right)$\\
	$\theta_{t+1} \leftarrow \theta_t - \alpha Hg$\\
	$t \leftarrow t + 1$
    }%
    \algolabel{gnm}
    \caption{Gradient Norm Minimization by Gradient Descent}
\end{algorithm}

However, one advantage of the gradient norm minimization approach
is that, because it is framed as the optimization
of a scalar-valued function, any number of tricks
from the world of optimization can be applied to it:
stochastic methods, momentum, adaptive gradients,
and so on.
This is in contrast to the Newton methods reviewed below.

\section{Exact Newton Methods}\sectionlabel{exact}

Gradient norm minimization does not take any advantage
of the special structure of our optimization problem,
namely that it was introduced
as a proxy for solving a system of equations.
The other major class of algorithms
for critical point-finding,
\emph{Newton methods},
are specifically designed to solve systems of equations.
In this section,
we review the fundamental, textbook forms
of the method.
Then, in the following sections,
\sectionref{inexact} and \sectionref{practical},
we introduce extensions that make it feasible for
problems with large dimension and higher-order nonlinearity,
like many neural network loss functions.

\subsection{The Babylonian Algorithm}\sectionlabel{babylon}

The very earliest form of Newton's method
dates back perhaps to the 17th century BCE,
based on the evidence of an extremely precise
(to within 2e-6)
approximation to the square root of 2
on a Babylonian clay tablet.
It was certainly known as a method
for computing square roots by the first century CE,
when it appears in the work of Hero of Alexandria.
See~\cite{brown1999} for details.

The intuition for this algorithm is geometric:
if a square of area $x$
is smaller (larger) than a square with sides of length $\theta$,
then it is larger (smaller) than the square with sides of length $x \div \theta$.
In either case, the square with sides equal in length to the average
of $\theta$ and $x \div \theta$
is closer in area to $x$
(see \figureref{babylon}).
Therefore, its side length is a closer approximation to $\sqrt{x}$.
This improvement can be applied iteratively,
yielding the below algorithm:
\\
\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $x, \theta_0 \in \R_{> 0}^2$\ \\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	    $\theta_{t+1} \leftarrow \frac{\theta_t}{2} + \frac{x \div \theta_t}{2}$
    }%
    \algolabel{babylon}
    \caption{Babylonian Algorithm}
\end{algorithm}
\noindent Recall that the motivation for considering the square root problem
was that $\sqrt{}$, unlike $+$ and $\times$,
does not have an exact finite-time algorithm.
This is also true of $\div$, which appears in \algoref{babylon}.
We will come back to this in \sectionref{div},
in order to better understand inexact Newton methods.
Note that this algorithm converges extremely quickly,
as indicated by \figureref{babylon}B-C.
The error goes from order 1 to order 1e-10
in just 5 iterations, then drops to $0$.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=\textwidth]{img/chapter2/babylon.pdf}
	\end{center}
	\caption{\textbf{The Babylonian Algorithm
	for Computing Square Roots}}{%
	\textbf{A}:
	Depiction of geometric intuition for the method.
	The square with side length $\theta_t$ (gold)
	is larger than that
	with side length $\sqrt{x}$ and area $x$ (black),
	while that with side length
	$x \div \theta_t$ (blue) is smaller.
	The average of these two values is the value $\theta_{t+1}$,
	which results in a square closer in area to $x$ (red).
	\textbf{B}:
	Value of $\theta$ across iterations of \algoref{babylon}
	for arguments $T=6$, $x = 4$, and $\theta_0=0.8$.
	The value of $4$ was chosen because the exact square root is known,
	unlike e.g.~$2$.
	The iterations indicated by color correspond to values in \emph{A}.
	$x$-axis is shared with \emph{C}.
	\textbf{C}:
	Absolute value of the difference between $\theta_t$ and $\sqrt{x}$
	for Babylonian algorithm as in \emph{B}, plotted on log-scale.
	Difference at iteration $6$ is $0$, and so is plotted off the chart.
	}
	\figurelabel{babylon}
\end{figure}

\subsection{Newton-Raphson}\sectionlabel{newton}

If we define
\begin{equation}
	f(\theta) = x - \theta^2
\end{equation}
\noindent then the update in \algoref{babylon}
can be written
\begin{align}
	\theta_{t+1} &= \theta_t - \frac{x - \theta_t^2}{2\theta_t}\\
	&= \theta_t - \frac{f(\theta_t)}{\grad{f}{\theta_t}}\equationlabel{newton-iter}
\end{align}
\noindent which can be defined for any differentiable $f$,
so long as $\grad{f}{\theta_t}\neq 0$.
In this form, it is known as the \emph{Newton method}
or as \emph{Newton-Raphson}.
This is an interesting example of Stigler's law of eponymy%
~\cite{stigler1980}:
Thomas Simpson was the first to notice that
this algorithm could be generalized using calculus,
in 1740.
However, in part because Isaac Newton had developed an
algebraic version of the method
for generic polynomials and Joseph Raphson had refined it,
Joseph Fourier, Carl Runge, and other
prominent mathematicians mistakenly credited them with the technique.
See~\cite{kollerstrom1992} for details.

In the case of critical point-finding,
the equivalent of $f$ is a vector and already a gradient,
and so $\nabla f$ becomes the Hessian matrix
and the scalar division becomes multiplication by an inverse matrix.
The resulting algorithm is as follows:
\\
\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $\theta_0 \in \R^N$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$\\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	$g \leftarrow \nabla{f}\left(\theta_t\right)$\\
	$H \leftarrow \nabla^2{f}\left(\theta_t\right)$\\
	$\theta_{t+1} \leftarrow \theta_t - \inv{H}g$\\
	$t \leftarrow t + 1$
    }%
    \algolabel{newton}
    \caption{Newton-Raphson}
\end{algorithm}

While the problem setup here was for equation solving,
this algorithm, in particular under the name Newton's method,
is better known as an optimization algorithm.
It works as such for the same reason that gradient descent
can be a global optimizer on smooth convex functions:
whenever all of the stationary points
are also global optima,
an algorithm for finding stationary points is also
an optimization algorithm.
Unlike gradient descent,
it is a second-order algorithm:
in addition to a gradient function, $\nabla f$,
it also needs a Hessian function, $\nabla^2 f$.

In the absence of geometric intuition
to explain why this is as sensible algorithm,
we turn to calculus.
First, we show that this is the best method possible for functions
with constant Hessian.

\begin{theorem}{Optimality of Newton Step}{newton}
	\emph{Let $f$ be a twice-differentiable function
	from $\Theta$ to $\R$
	with constant, non-singular Hessian function.
	From any point $\theta$, a point $\theta^\star$
	that solves the gradient equations~\ref{eq:gradienteqns}
	for $f$ can be obtained by computing
	\begin{equation}
		\theta^\star = \theta - \inv{\hess{f}{\theta}}\grad{f}{\theta}
	\end{equation}}
\end{theorem}
\noindent\textit{Proof of \thmref{newton}}:\\
The value of the gradient at a point $\theta + p$
can be approximated by applying
the Hessian at $\theta$ to $p$:
\begin{equation}
	\grad{f}{\theta + p} = \grad{f}{\theta} + \hess{f}{\theta}p + o(p)
\end{equation}
\noindent By Taylor's theorem,
$o(p)$ is governed by the integral of the third derivative
of $f$ evaluated from $p$ to $\theta$.
But the Hessian is constant, and so the third derivative is a
zero tensor and	$o(p)$ can be replaced with $0$:
\begin{equation}
	\grad{f}{\theta + p} = \grad{f}{\theta} + \hess{f}{\theta}p
\end{equation}

To solve the gradient equations,
we need the right-hand side here to be equal to 0.
Denoting the solution $p^\star$, we find that
\begin{align}
	0 &= \grad{f}{\theta} + \hess{f}{\theta}p^\star\\
	p^\star &= -\inv{\hess{f}{\theta}}\grad{f}{\theta}
\end{align}
\noindent from which the claim follows,
with $\theta^\star = \theta + p^\star$.\\
\QED\\

We call the value $p^\star$ the \emph{pure Newton step}
or the \emph{exact Newton step}.
Note the absence of any step size.
We call the linear system of equations in $p$
defined by
\begin{equation}\equationlabel{newtonsystem}
	0 &= \grad{f}{\theta} + \hess{f}{\theta}p
\end{equation}
the \emph{Newton system} of equations.

For functions with non-constant Hessian,
i.e.~those which cannot be represented by a degree two polynomial,
this argument doesn't hold,
and the pure Newton step is not optimal in the same sense.
However, it holds approximately for functions whose Hessian
doesn't change too quickly.
In fact, the rate of convergence for this algorithm,
once sufficiently close to a critical point,
is quadratic:
for each iteration, the number of bits of precision doubles%
\footnote{In terms of the error, this is much faster than quadratic:
it is squared exponential.}.
This is to be contrasted with gradient descent,
which has at best linear improvement:
with each iteration, the number of bits of precision increases by a fixed amount%
\footnote{Again, in terms of the error, this is much faster:
it is exponential.}.
This rapid convergence is visible in
\figureref{babylon}C,
where the quality of the solution doubles,
from approximately 1e-5
to approximately 1e-10,
between iterations $4$ and $5$.

\subsection{Pseudo-Inverse Newton}\sectionlabel{pseudoinverse}

In defining the Babylonian algorithm,
\algoref{babylon},
it was important that the starting point not be $0$,
or else the update would be undefined due to division by $0$.
Equivalently,
the full Newton method \algoref{newton}
required that the Hessian was non-singular,
else the inverse is undefined.

But this is not a fundamental restriction,
which is lucky because the loss functions of neural networks
are highly singular~\cite{sagun2017}.
The key property of the update $p^\star$
was that it solved the Newton system
\begin{equation}\equationlabel{newtonsystem}
	0 = \grad{f}{\theta} + \hess{f}{\theta}p
\end{equation}
\noindent which has a solution whenever $-\grad{f}{\theta}$
is in the image of $\hess{f}{\theta}$,
the linear subspace of possible outputs of $\hess{f}{\theta}$,
defined below.

\begin{definition}{Image and Co-Image of a Matrix}{image}
	For a matrix $M$ in $\R^{m \times n}$,
	we define the \emph{image} of M, denoted $\im M$, as
	\begin{equation}
		\im M \defeq \Set{v \in \R^m \suchthat \exists w \suchthat Mw=v}
	\end{equation}
	and the \emph{co-image} of M, denoted $\co\im M$, as
	\begin{equation}
		\co\im M \defeq \Set{v \in \R^m \suchthat !\exists w \suchthat Mw=v}
	\end{equation}
	When the rank (\defref{kernel}) of $M$ is at least $m$,
	the co-image of $M$ is empty.
\end{definition}

In this case, the solution $p^\star$
can be obtained by applying the
(Moore-Penrose) pseudo-inverse of the Hessian
to the gradient.
We will define this pseudo-inverse
by means of the singular value decomposition.
We will need several properties
of this matrix later,
so we do this in detail.

\subsubsection{Pseudo-Inverses and the Singular Value Decomposition}

The singular value decomposition of a matrix
breaks down a matrix into the product,
or composition, of three matrices.
\begin{definition}{Singular Value Decomposition}{svd}
	For a matrix $M$ in $\R^{m \times n}$
	with rank $r$,
	we define the
	\emph{singular value decomposition} or SVD of $M$
	as the triple of matrices $V^\top$, $\Sigma$, $U$
	such that
	\begin{equation}
		M = U \Sigma V^\top
	\end{equation}
	where $V^\top \in \R^{r \times n}$,
	$\Sigma \in \R^{r \times r}$, and
	$U \in \R^{m \times r}$
	and $U$ and $V$ are orthonormal
	and $\Sigma$ is diagonal.
	The columns of $U$ are called the \emph{left-singular}
	vectors of $M$, the columns of $V$ the \emph{right-singular}.
	The diagonal entries of $\Sigma$ are the \emph{singular values} of $M$.
	This form is sometimes called the \emph{compact SVD}.
\end{definition}

It can be shown that every matrix has a
singular value decomposition~\cite{strang1993}.
The singular value decomposition arises
as the specialization of the First Isomorphism Theorem
to the category of vector spaces and linear maps.
We state it here in terms of functions,
for readers not familiar with category theory,
but it is more appropriately set in terms of morphisms.

\begin{theorem}{The First Isomorphism Theorem}{firstiso}
	\emph{Let $\cC$ be a collection of sets and all functions between them.
	For any function $f\from A \to B$, where $f$, $A$, and $B$ are members of the collection,
	there is a triple of functions $s$, $b$, and $i$ such that
	\begin{equation}
	      f = i \after b \after s
	\end{equation}
	where $s\from A \onto A/\sim$ is a surjection,
	$b\from A/\sim \to \im A$ is a bijection, and
	$i\from \im A \into B$ is an injection
	(surjectivty and injectivity indicated by arrow type).
	The elements of the equivalence relation $\sim$
	are given by pairs $(a, a^\prime) \in A \times A$
	where $f(a) = f(a^\prime)$.}
\end{theorem}

The First Isomorphism Theorem states that every function can be decomposed
into three constituent pieces:
an onto mapping, or surjection,
that classifies its inputs according to which output they are mapped to;
a one-to-one mapping, or bijection,
that picks the output corresponding to each class of input;
and an into mapping, or injection,
that inserts this output into the target set.

Though this decomposition is of limited use
for generic functions,
as opposed to structure-preserving functions
like group homomorphisms
or linear maps,
it is fruitful to consider a concrete example
from the world of generic functions
before connecting to the SVD.\@

\begin{example}{Decomposition of \texttt{is\_odd}}{isodd}
	Consider the function $\texttt{is\_odd}\from \N \to \cS$,
	where $\cS$ is the set of all strings in the English alphabet,
	that returns \enquote{True} if the number is odd,
	\enquote{False} otherwise.
	This function can be decomposed as
	\begin{equation}
		\texttt{is\_odd} =
		\texttt{to\_string} \after \texttt{to\_bool} \after {\%}_2
	\end{equation}
	where $\%_2\from\N \onto \Set{0, 1}$
	computes the value mod $2$ and
	\begin{equation*}
		\texttt{to\_string}(b) = %chktex 36
			\begin{cases}
				\mlq \text{False} \mrq & \text{if } b = \bot \\
				\mlq \text{True} \mrq & \text{if } b = \top
			\end{cases} \quad
		\texttt{to\_bool}(k) = %chktex 36
			\begin{cases}
			     \bot  & 	\text{if } k = 0 \\
			     \top  & 	\text{if } k = 1
			\end{cases}
	\end{equation*}
\end{example}

This decomposition is summarized neatly by the commutative diagram below:
\begin{center}
    \begin{tikzcd}
	    \N \arrow[rr, "\texttt{is\_odd}"] \arrow[dd, "\%_2", two heads]
	    &  & \cS\\
	\ & \  & \\
	\Set{0, 1} \arrow[rr, "\texttt{to\_bool}"]
	&  & \mathbb{B} \arrow[uu, "\texttt{to\_string}", harpoon, hook]
    \end{tikzcd}
\end{center}
Saying that this is a commutative diagram
means that any path from one domain to another
results in the same function.

Now, let us view the SVD through this lens.
First, $V^\top$ is a surjection because
it is orthonormal and $r \leq n$.
Second, $U$ is an injection because
it is orthonormal and $m \geq r$.
Finally, $\Sigma$ is a bijection
because it is a diagonal matrix with non-zero diagonal entries.
This establishes that the (compact) SVD
is the same decomposition as in the First Isomorphism Theorem.
Furthermore, for linear functions,
two inputs can only be mapped to the same output
if they are both mapped to $0$.
If we take $0$ to be the representative of that equivalence class,
the target of the surjection becomes the co-kernel of M
(recall \defref{kernel}).
The target of the bijection is the image of M.
This set of relationships is summarized
in the commutative diagram below.

\begin{center}
    \begin{tikzcd}
	    \R^n \arrow[rr, "M" description] \arrow[dd, "V^\top" description, two heads]
	    &  & \R^m \\
	\ & \  & \\
	\co \ker M \arrow[rr, "\Sigma"]
	&  & \im M \arrow[uu, "U" description, harpoon, hook]
    \end{tikzcd}
\end{center}

We are now ready to define the pseudo-inverse.
We first define it for two simple classes of matrices,
then extend it for all matrices by means of the SVD.\@

\begin{definition}{Moore-Penrose Pseudo-Inverse}{pseudoinverse}
	Let $O$ be an orthogonal matrix
	and let $D$ be a diagonal matrix of full rank.
	We define the pseudo-inverses of $O$ and $D$,
	denoted $\pinv{O}$ and $\pinv{D}$, as
	\begin{align}
		\pinv{O} \defeq O^{\top},\
		& \pinv{D} \defeq \inv{D}
	\end{align}

	Let $M$ be a matrix in $\R^{m \times n}$
	and let its singular value decomposition be the triple
	$V^\Top$, $\Sigma$, $U$.
	We define the \emph{pseudo-inverse} of $M$,
	denoted $\pinv{M}\from \R^m \to \R^n$, as
	\begin{align}
		\pinv{M}
		&\defeq \pinv{\left(V^{\top}\right)}\pinv{\Sigma}\pinv{U}\\
		&= V \inv{\Sigma} U^\top
	\end{align}
\end{definition}

This is sometimes known as the Moore-Penrose pseudo-inverse.
The relationships between these matrices
are summarized by the commutative diagram below.
\begin{center}
    \begin{tikzcd}
	    \R^n \arrow[rr, "M", bend left] \arrow[dd, "V^\top", swap, two heads, bend right]
	    &  & \R^m \arrow[dd, "U^\top", swap, two heads, bend right] \arrow[ll, "M^+", dotted, bend left]                       \\
	\ & \  & \\
	\co \ker M \arrow[uu, "V", harpoon, hook, bend right]  \arrow[rr, "\Sigma", bend right]
	&  & \im M \arrow[uu, "U", harpoon, hook, bend right] \arrow[ll, "\inv{\Sigma}", bend right]
    \end{tikzcd}
\end{center}


\subsubsection{Defining Pseudo-Inverse Newton}

Let's return to the problem of solving
\equationref{newtonsystem}
for an update $p^{\star}$.
We see that whenever a vector $g$
is in the image of the Hessian, $\hess{f}{\theta}$,
we can apply the pseudoinverse, $\pinv{\hess{f}{\theta}}$,
to $-g$ to obtain a vector that is the pre-image of $-g$
with respect to the Hessian.
We choose this as our update,
obtaining the algorithm below.
\\
\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $\theta_0 \in \R^N$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$\\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	$g \leftarrow \nabla{f}\left(\theta_t\right)$\\
	$H \leftarrow \nabla^2{f}\left(\theta_t\right)$\\
	$\theta_{t+1} \leftarrow \theta_t - \pinv{H}g$\\
	$t \leftarrow t + 1$
    }%
    \algolabel{newtonPI}
    \caption{Pseudo-Inverse Newton}
\end{algorithm}

We will return to the problem of what happens
when the gradient is not in the image of the Hessian in \chapterref{three}.
This will turn out to be critically important for understanding
the behavior of Newton methods on neural network loss functions.

\section{Inexact Newton Methods}\sectionlabel{inexact}

In \sectionref{exact},
we solved the Newton system algebraically and exactly
\begin{equation}
	0 = \grad{f}{\theta} + \hess{f}{\theta}p^\star
\end{equation}
\noindent
But as noted in the beginning of this chapter,
numerical methods are generically incapable of providing exact solutions.
While we don't often think of this as applying to algebraic operations,
we have already seen that it applies to square roots.
In \sectionref{div},
we will see that it also applies to division,
which is a special case of the matrix inversion
we need to perform exact Newton methods.

We will then consider how we might replace the matrix inversion
in our exact Newton algorithms, \algoref{newton} and \algoref{newtonPI},
with an approximate optimization,
based on iterative solutions to linear systems of equations.
We will focus on the Krylov subspace methods
(\secref{krylov}).

\subsection{Optimization Approach to Division}\sectionlabel{div}

The Babylonian algorithm for computing the square root
of a positive real number $x$,
\algoref{babylon},
had the following iteration rule:
\begin{equation}\equationlabel{babylon-iter}
	\theta_{t+1} = \theta_t / 2 + x \div \theta_t / 2
\end{equation}
\noindent For binary representations,
division by $2$ can be done by an $O(1)$ bit-shift,
much like division by $10$ in decimal representation.
However, division by other numbers is,
like the square root operation,
not guaranteed to result in a finite binary string:
$1/3$, for example\footnote{%
We could commit to exact arithmetic with rational numbers,
but the sizes of our number representations would grow with each operation,
which is infeasible for a neural network loss
defined on thousands of examples.}.
Note that this is evident in the typical
\enquote{long division} algorithm
taught in elementary schools:
children are often told to simply stop when they
hit some number of decimal places.

To obtain an alternate division algorithm
that's more explicitly an optimization,
we first rewrite the iteration rule as
\begin{equation}\equationlabel{babylon-inv-iter}
	\theta_{t+1} = \theta_t / 2 + \inv{\theta_t} \times x / 2
\end{equation}
\noindent which makes the connection
to the Newton algorithm more clear,
because it is in terms of an inverse,
but doesn't solve our problem,
because the inverse is also an operation
that takes us outside finite binary strings
(e.g., $\inv{3}$).
Let us denote the inverse of $\theta_t$
as $\gamma^\star$.
This pair satisfies
\begin{equation}
	\gamma^\star = \argmin_{\gamma \in \R_{>0}} \inv{\gamma} - \theta_t
\end{equation}
\noindent
to which we can apply Newton-Raphson
by introducing the function
\begin{equation}
	g(\gamma) = \inv{\gamma} - \theta_t
\end{equation}
\noindent with gradient
\begin{equation}
	\grad{g}{\gamma} = -\gamma^{-2}
\end{equation}

This hardly seems like an improvement,
but when we plug it into the Newton iteration formula
\equationref{newton-iter}
and rearrange,
we obtain an update entirely in terms of
multiplication and subtraction:
\begin{align}
	\gamma_{k+1}
	&= \gamma_k - \frac{\inv{\gamma_k} - \theta_t}{-\gamma_k^{-2}}\\
	&= \gamma_k + \gamma_k^2 \times \left(\inv{\gamma_k} - \theta_t\right)\\
	&= \gamma_k + \gamma_k - \gamma_k^2\times \theta_t\\
	&= \gamma_k \times \left(2 - \gamma_k \times \theta_t\right)
\end{align}
\noindent which results in the approximate inverse algorithm below:
\\
\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $K\in \N$, $(\theta, \gamma_0) \in {\left(\R \setminus \Set{0} \right)}^2 $\ \\
    $k \leftarrow 0$\\
    \While{$k < K$}{%
	    $\gamma_{k+1}
	    \leftarrow \gamma_k \times \left(2 - \gamma_k \times \theta\right)$\\
	    $k \leftarrow k + 1$
    }%
    \algolabel{newtoninv}
    \caption{Newton Inversion}
\end{algorithm}

Our full square-root finding algorithm, then,
looks like this:
in an outer loop,
update our estimate for the square root
by applying Newton-Raphson to
the current estimate,
as in \algoref{babylon}.
The division operation requires the computation of the inverse,
which occurs in an inner loop
according to \algoref{newtoninv}.
Note that this inverse is approximate as well.

\subsection{Least-Squares Inexact Newton}\sectionlabel{newton-ls}

The motivation for inexact Newton methods
is precisely the same as for the optimization approach to division:
we cannot exactly perform the inversion,
so we search for a sensible way to do it approximately.

We proceed slightly differently to \sectionref{div}.
We first note that an exact solution to the Newton system,
$p^\star$,
also satisfies
\begin{equation}\equationlabel{newton-ls}
	p^\star = \argmin_p \snorm{\grad{f}{\theta} + \hess{f}{\theta}p}
\end{equation}
\noindent where the expression inside the squared norm
is the right-hand-side of the Newton system.
This is another example of relaxation to an optimization problem,
as described in~\sectionref{sqrt}.
We can therefore re-write our
exact Newton algorithms
in terms of this $\argmin$ operation.
\\
\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $\theta_0 \in \R^N$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$\\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	$g \leftarrow \grad{f}{\theta_t}$\\
	$H \leftarrow \hess{f}{\theta_t}$\\
	$p^\star{} \leftarrow \argmin_{p}\ \snorm{Hp + g}$\\
	$\theta_{t+1} \leftarrow \theta_t + p^\star{}$\\
	$t \leftarrow t + 1$
    }%
    \algolabel{newtonLS}
    \caption{Least-Squares Exact Newton}
\end{algorithm}

Our two exact Newton methods above
were different choices of solution to the problem
of computing the $\argmin$ in this meta-algorithm,
appropriate for different assumptions about $\nabla^2 f$.
If we relax the requirement to compute the exact $\argmin$,
which cannot be satisfied in a numerical setting anyway,
we obtain a new meta-algorithm,
which we call \emph{least-squares inexact Newton}.
\\
\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $\theta_0 \in \R^N$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$,
    \text{lsq-solve}\\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	$p \leftarrow \text{lsq-solve}\left(\theta_t, \nabla^2{f}, \nabla{f}\right)$\\
	$\theta_{t+1} \leftarrow \theta_t + p$\\
	$t \leftarrow t + 1$
    }%
    \algolabel{newtonLS}
    \caption{Least-Squares Inexact Newton}
\end{algorithm}

Here, lsq-solve is a function
that takes in the current parameter value,
Hessian function, and gradient function,
and applies an approximate method to minimize
the least-square value of the right-hand-side
of the Newton system of equations.
This setup has several advantages over the exact setup.

First,
it allows for direct control over numerical issues.
Iterative methods for computing least-squares solutions
generally come with hyperparameters for stopping
before a solution within numerical tolerance is obtained.
For example, when a matrix $M$ is non-invertible,
its numerical representation is typically invertible,
because its eigenvalues are non-zero, but very small.
The numerically-inverted matrix will have large eigenvalues
and the result of applying it to some vectors will be very large.
An iterative solver can detect that its solution has grown above
a threshold and terminate.
This is just one of many ways that iterative solvers
can better handle numerical issues.

Second, it transfers immediately to the case
where the Hessian matrix is non-invertible.
There are two issues in this case.
Firstly,
the $\argmin$ in \algoref{newtonLS}
need no longer be a single value,
but can instead be a set
(indeed, a linear subspace!).
This occurs because
any portion of $p$
in the kernel of $H$ can take on any value.
A least-squares solver
will always return only
a single value from this set,
typically according to some criterion.
The exact solution computed by
pseudo-inverse Newton,
\algoref{newtonPI},
corresponds to choosing the element of the $\argmin$
with minimum norm,
as in the algorithm below.
Secondly,
the updates in the exact case were taken to be solutions
of the Newton system,
which need not exist in the non-invertible case.
When $g$ is in the co-image of $H$,
no vector $p$ satisfies the Newton system.
Such a system is called \emph{unsatisfiable},
and we will have more to say about this case in~\chapterref{three}.
When the Newton system is unsatisfiable
the updates are not solutions,
but merely elements of the $\argmin$ in \equationref{newton-ls}.
\\
\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $\theta_0 \in \R^N$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$\\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	$g \leftarrow \grad{f}{\theta_t}$\\
	$H \leftarrow \hess{f}{\theta_t}$\\
	$P \leftarrow \argmin_{p}\ \snorm{Hp + g}$\\
	$p^\star{} \leftarrow \argmin_{p \in P}\ \snorm{p}$\\
	$\theta_{t+1} \leftarrow \theta_t + p^\star{}$\\
	$t \leftarrow t + 1$
    }%
    \algolabel{newtonLS-incompatible}
    \caption{Least-Squares Exact Newton for Incompatible Systems}
\end{algorithm}

\subsection{Krylov Subspace Methods for Least-Squares Inexact Newton}\sectionlabel{krylov}

We now turn to the choice of method for lsq-solve.
This discussion closely follows that in~\cite{roosta2018}.

In principle, the least-squares sub-problem
could be solved by generic convex optimization methods.
However, this is unwise, because it is a very special type
of convex problem:
it is a \emph{linear} system of equations.
Linearity is a powerful property that enables
specialized algorithms to achieve very high performance.
These algorithms are called \emph{linear solvers}.

The classic choice for linear solver is the
\emph{conjugate gradient} algorithm.
On each iteration, conjugate gradient
produces a step that is \emph{conjugate}
to all previous steps
with respect to the current Hessian $H$,
as defined below.
See~\cite{boyd2004} for details on this algorithm.
\begin{definition}{Conjugate Vectors}{conjugate}
	Two vectors $x$ and $y$ are \emph{conjugate}
	with respect to the symmetric matrix $M$ if
	\begin{equation}
		x^\top M y = 0
	\end{equation}
	\noindent By the symmetry of $M$,
	conjugacy is a symmetric relation.
\end{definition}
\noindent

Conjugate gradient has several nice properties
as a linear solver for inexact Newton.
With exact arithmetic, it produces an exact solution
in no more than $N$ steps,
where the dimension of the system is $N\times N$.
This is to be contrasted with generic optimization approaches,
like gradient descent,
which need never produce exact solutions,
even with exact arithmetic.
Furthermore, conjugate gradient can be implemented entirely
in terms of matrix-vector products,
with no need to form any $N\times N$ matrices
besides the linear system's matrix of coefficients,
which reduces memory footprint.
This is particularly advantageous when the matrix is a Hessian,
because Hessian-vector products can also be computed in $O(N)$ time,
versus $O(N^2)$ time for generic matrix-vector products,
and don't even require the explicit formation of the $N \times N$ Hessian%
~\cite{pearlmutter1994}.

These properties stem from the fact that it is a
\emph{Krylov subspace method},
meaning that it computes
its solution in a sequence of special subspaces
of increasing dimension, up to $N$,
defined below.
\begin{definition}{Krylov Subspace Methods}{krylov}
	Let $M$ be a matrix in $\R^{N\times N}$
	and $x$ be a vector in $\R^N$.
	The \emph{Krylov subspace}
	of order $r$
	for $M$ and $x$, denoted
	$\cK_r\left(M, x\right)$,
	is defined as
	\begin{equation}
		\cK_r\left(M, x\right)
		\defeq \mathrm{span}\Set{x, Mx, M^2x, \dots M^{r-1}x}
	\end{equation}

	An optimization method is a
	\emph{Krylov subspace method}
	if its iterate at step $r$, $\p_r$,
	is an element of the Krylov subspace of order $r$.
\end{definition}
\noindent Note that the Krylov subspaces
are constructed iteratively,
with each new vector produced by
means of matrix-vector multiplication.

The choice of conjugate gradient is motivated
by the fact that, for a smooth, convex
function $f(\theta) \from \Theta \to \R$,
the $k$th iterate of conjugate gradient, $p_k$,
applied to the Newton system at a point $\theta_t$ satisfies
\begin{align}
	p_k
	&= \argmin_{p \in \cK_k\left(\hess{f}{\theta_t}, \grad{f}{\theta_t}\right)}
	\grad{f}{\theta_t}^\top p + \frac{1}{2} p^\top\hess{f}{\theta_t}p\\
	&= \argmin_{p \in \cK_k\left(\hess{f}{\theta_t}, \grad{f}{\theta_t}\right)}
	\hat{f}_{\theta_t}(p)
\end{align}
\noindent where $\hat{f}_{\theta_t}$
is the second-order Taylor expansion of $f$
at $\theta_t$.
That is, conjugate gradient produces
the point within each Krylov subspace
that minimizes the quadratic approxmation
of the function $f$ at $\theta_t$.

This is ideal for the minimization of convex functions,
but our goal is different:
we wish to minimize the gradient.
We should therefore like, instead,
that our iterates minimize
the norm of our approximation of the gradient,
$r(p) = \grad{f}{x} + \hess{f}{x}p$,
within each Krylov subspace.
The quantity $r$ is known as the \emph{residual},
and so we call a Newton method that uses
a linear solver with that property
a \emph{minimum residual Newton method},
as defined below.
Note that we explicitly consider
the possibility that the Newton system is unsatisfiable
in these algorithms and select the final update $p^\star{}$
according to some criterion function $c$.
Furthermore, for simplicity's sake,
we write that the algorithm selects the optimal
iterate in a Krylov subspace of a fixed order,
$\cK_K$, but practical algorithms
would iteratively determine  the Krylov subspace
based on numerical considerations,
e.g.~norm of the residual or the solution.
\\
\begin{algorithm}[H]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $K \in \N$, $\theta_0 \in \R^N$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$,
    $c\from \R^N to \R$\\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	$g \leftarrow \grad{f}{\theta_t}$\\
	$H \leftarrow \hess{f}{\theta_t}$\\
	$\cK \leftarrow \cK_K\left(H, g\right)$\\
	$P \leftarrow \argmin_{p \in \cK}\ \snorm{g + Hp}$\\
	$p^\star{} \leftarrow \argmin_{p \in P}\ c(p)$\\
	$\theta_{t+1} \leftarrow \theta_t + p^\star{}$\\
	$t \leftarrow t + 1$
    }%
    \algolabel{newton-minres}
    \caption{Minimum Residual Newton for Incompatible Systems}
\end{algorithm}

There are two major linear solvers
that have the minimum residual property%
\footnote{SYMMLQ, the symmetric LQ method,
can also be used as a generic linear solver
for compatible systems,
but does not satisfy the minimum residual property%
~\cite{paige1975}.}:
MINRES~\cite{paige1975},
short for \enquote{minimum residual},
and MINRES-QLP~\cite{choi2011},
also known as MR-QLP,
which is similar to MINRES but uses the
QLP decomposition~\cite{huckaby2005}
in place of the QR decomposition%
\footnote{Conjugate gradient methods use the
Cholesky decomposition,
which is only defined for positive semi-definite matrices.}.
In addition to MR-QLP having superior performance
on systems that are ill-conditioned outside of their kernel,
it also corresponds to the choice of $c(p) = \snorm{p}$
in~\algoref{newton-minres} above.
The exact minimum norm solution is the one computed by
the pseudo-inverse method,
\algoref{newtonLS-incompatible}.
Therefore, an inexact Newton method with MR-QLP
as its linear solver is a natural relaxation
of an exact pseudo-inverse Newton method.

The choice of MR-QLP as a linear solver
for inexact Newton methods
was proposed in~\cite{roosta2018},
which noted all of the favorable numerical
and analytical properties described above.
The final Newton algorithm proposed in~\cite{roosta2018}
is, however, outside the class of algorithms considered so far.
We next turn to the additional tricks required to
define Newton algorithms used in practice.

\section{Practical Newton Methods}\sectionlabel{practical}

The definition of the Newton step in
the basic Newton-Raphson algorithm in~\algoref{newton}
was motivated by its optimality for functions with constant Hessian.
For functions with non-constant Hessian,
i.e.~those with non-zero third or higher partial derivatives,
this step is no longer optimal.
Indeed, Newton's method can diverge,
for example when used as a root finding method
on the function
$f(\theta) = \theta^{1/3}$.
It can oscillate, for example when applied to find the critical points of
$f(\theta) = \frac{x^4}{4} - x^2 + 2x$.
It can even behave chaotically%
~\cite{griewank1983}.
We encourage the reader to apply a few iterations
of~\algoref{newton} to these functions by hand,
using $1$ for $\theta_0$.

The two most common methods for avoiding
these issues are damping (\sectionref{damped})
and back-tracking line search (\sectionref{guarded}).
The former takes the approach of generating
multiple Newton-type steps and selecting
the best one.
The latter uses a standard Newton step
to choose a direction and then performs
a one-dimensional optimization
to select a step size.
In both cases, candidate updates are selected
according to a scalar-valued \emph{merit function}.
For critical point-finding,
the appropriate merit function is based on a norm of the gradient,
here the squared Euclidean norm.
For optimization, the appropriate merit function
is the loss.

With these additions,
we will have finally defined Newton methods
sufficiently robust to handle the case of
linear neural networks, as we will see in the first part of
\chapterref{three}.


\subsection{Damped Newton}\sectionlabel{damped}

The motivation for damped Newton methods
comes from the case of convex functions
with positive semi-definite but not positive definite Hessians.
In that case,
the Hessian can be made positive definite
by adding an arbitrarily small
perturbation $\gamma$ to the diagional.
However, this results in an inverse matrix
with arbitarily large maximal eigenvalue $\gamma^{-1}$.
Depending on how much of the gradient lies in the eigenspace
corresponding to this eigenvalue,
this could still result in
a very large step.
Since the Newton step is motivated by a local analysis,
this is undesirable.
Therefore,
we consider a finite number of possible perturbations\footnote{%
Denoting the set of all finite subsets of a set $S$ as
$\cP_\omega(S)$, the type of $\Gamma$ is written
$\Gamma \in \cP_\omega\left(\R_{\geq 0}\right)$},
$\Gamma = \Set{\gamma_i : \gamma_i \in \R_{\geq 0}}$
and produce a Newton step for each one,
with the Hessian matrix $H$ replaced by
$H + I\gamma_i$ for each $i$.
Here, $I$ is the identity matrix of
the same shape as $H$.

We provide an example damped Newton method below,
in \algoref{damped-newton}
based on the exact minimum residual Newton method,
\algoref{newton-minres}.
Damping can be combined with any method for computing
the Newton iteration, and is compatible
with fast Hessian-vector products.
\\
\begin{algorithm}[h]
    \SetAlgoLined{}
    \textbf{Require}
    $T\in \N$, $K \in \N$, $\theta_0 \in \R^N$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$,
    \Gamma \in \cP_\omega(\R_{\geq 0})\\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
	$g \leftarrow \grad{f}{\theta_t}$\\
	$P^\star{} \leftarrow \Set{}$\\
	\For{$\gamma \in \Gamma$}{%
		$H \leftarrow \hess{f}{\theta_t} + I\gamma$\\
		$\cK \leftarrow \cK_K\left(H, g\right)$\\
		$P \leftarrow \argmin_{p \in \cK}\ \snorm{g + Hp}$\\
		$P^\star{} \leftarrow P^\star{} \cup \argmin_{p \in P}\ \snorm{p}$
	}
	$p^\star{} \leftarrow \argmin_{p \in P^\star{}}\ \sgn{f}{\theta_t  + p}$\\
	$\theta_{t+1} \leftarrow \theta_t + p^\star{}$\\
	$t \leftarrow t + 1$
    }%
    \algolabel{damped-newton}
    \caption{Minimum Residual Damped Newton Method}
\end{algorithm}

Damping methods are, however,
not particularly well-suited to Hessians that are indefinite,
with both positive and negative eigenvalues.
Indeed, they can cause the matrix to become singular,
even if it is initially non-singular.
The eigenvalues of the matrix $M + \gamma I$
are equal to the eigenvalues of $M$ plus $\gamma$,
so any eigenvalue equal to $-\gamma$ becomes $0$.
More broadly,
the goal of damping in the positive semi-definite case
was to reduce the maximum step size by controlling
the maximum eigenvalue of the inverse matrix.
In the indefinite case, this is no longer achieved.
This method was, however, used in%
~\cite{dauphin2014}
as a critical point-finding method
on neural network loss surfaces,
so we will examine its performance in~\chapterref{three}.

\subsection{Guarded Newton}\sectionlabel{guarded}

Unlike typical first-order optimization algorithms,
the Newton methods presented thus far have had no step size hyperparameter.
These Newton methods are sometimes called \emph{pure} Newton methods.
Effectively, the step size has been fixed at $1$.
This is not entirely artificial.
First, the Newton step is invariant to affine transformations
of the parameterization of the function,
and so there is a meaning to unit step size
that is absent in first-order methods.
Second, and more critically,
the rapid local convergence of Newton methods
requires a unit step size
and collapses if non-unit step sizes are chosen
(for both claims, see~\cite{boyd2004,nocedal2006,roosta2018}).
However, when outside the region
of local convergence,
the pure Newton step is frequently too long.

One solution is to select the step size adaptively,
so that it can be $1$ inside the region of rapid convergence
and less than $1$ outside it.
Such methods are sometimes called \emph{guarded Newton methods}:
the method is \enquote{guarded} against taking steps that are too large.
In particular, one common approach to selecting the step size
is \emph{back-tracking line search}%
~\cite{armijo1966},
abbreviated BTLS.\@

In a line search, the step size $\alpha^\star{}$
for a direction $p$ at a point $\theta$
is cast as the solution to an optimization problem:
\begin{equation}
	\alpha^\star{}
	\defeq \argmin_{\alpha \in \R_{\geq 0}} c(\theta + \alpha p)
\end{equation}
\noindent for some merit function $c$.
When selecting a step size for a typical optimization problem,
the natural choice is $c(x) = f(x)$, where $f$ is the loss.
For critical point-finding,
the natural choice is $c(x) = \sgn{f}{x}$.

Of course, this problem cannot typically be solved exactly.
In back-tracking line search,
the step size is selected by starting with a large value
$0 < \alpha \leq 1$
and then decreasing it multiplicatively by a factor $0 < \beta < 1$
on each failed iteration.
A failed iteration $k$ is one where the step size choice
$\alpha \times \beta^k$
produces insufficient decrease in the merit function $c$.
The degree of decrease considered sufficient
is typically an increasing function of the step size
and controlled by another \enquote{tolerance} hyperparameter
$0 < \rho < 1$.
The authors of~\cite{roosta2018}
determined the appropriate criterion for decrease
in the case where $c$ is the squared gradient norm
by considering the case of a function with constant Hessian.
Effectively, a step size is chosen
when the squared gradient norm decreases by an amount within
a factor of $\rho$ of how much it would decrease
in that case.

\begin{figure}[Hp!]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{img/chapter2/btls.pdf}
	\end{center}
	\caption{\textbf{Back-Tracking Line Search
	for Guarded Newton}}{%
	\textbf{A}:
	$f(\theta) = \|\theta\|^{3/2}$
	is plotted as a solid gold line,
	and its second-order Taylor approximation
	from the point $-1$ as a dashed blue line.
	Values of $\theta$ that result in
	sufficient decrease of the squared gradient
	with the tolerance parameter $\rho=0.2$
	indicated in dark red.
	The point selected by BTLS
	with $\beta=0.9$ indicated with a gold star.
	\textbf{B}:
	The squared gradient of $f$ is plotted as a
	solid gold line.
	The sufficient decrease criterion that appears
	in the line search convergence check in \algoref{roosta-btls}
	is plotted as a dashed gray line.
	Values of $\theta$ that result in squared gradients
	below this line in dark red.
	Tick marks indicate points possibly visited
	by the back-tracking line search.
	Hyperparameters as in $A$.
	}
	\figurelabel{btls}
\end{figure}

It is instructive to consider a concrete example.
The behavior of pure Newton and
guarded Newton on the function
$f(\theta) = \|\theta\|^{3/2}$
are contrasted in~\figureref{btls}.
Starting at the point $\theta_0=-1$,
(gold circle)
the pure Newton iteration $\theta_1 + p^\star$
(blue star)
is $1$, which results in oscillation:
$\theta_2=1, \theta_3=-1=\theta_0$.
Back-tracking line search avoids this pathology.
Only step sizes that result in values of $\theta$
that decrease the squared gradient by a sufficient amount
(below dashed gray line in \figureref{btls}B)
are acceptable.
These values of $\theta$ are indicated in dark red.
Back-tracking line search starts at the pure Newton step
and decreases its length by a multiplicative factor
(black ticks)
until it lands inside the red region.
The result is the next iterate,
$\theta_1$ (gold star).

We present a back-tracking line search
appropriate for selecting Newton step sizes
in \algoref{roosta-btls}.
We make a few modifications to the basic scheme described above.
First, we allow the starting value of $\alpha$ to be non-unit,
but enforce that, before the multiplicative BTLS begins,
the value of $1$ is always checked first.
Second, we allow that
the criterion for convergence used in this check,
$\rho^\prime$,
is different from the value
used in the BTLS, $\rho$.
Finally,
we allow the specification of a minimum step size, $\eps$,
below which $\alpha$ is set to $0$.
We set this to the largest value for which
$\alpha = \beta\alpha$.
\\
\begin{algorithm}[h]
    \SetAlgoLined{}
    \textbf{Require}
    $\nabla f\from\R^N\to\R^N$,
    $\nabla^2 f\from\R^N\to\R^{N\times N}$,
    $p \in \R^N$,
    $\theta \in \R^N$,
    $\alpha \in \intervaloc{0}{1}$,
    $\beta \in (0, 1)$,
    $\rho \in (0, 1)$,
    $\rho^\prime \in (0, 1)$,
    $\eps \in \R_{\geq 0}$\\ \ \\
    \DontPrintSemicolon
    $H \leftarrow \hess{f}{\theta}$\;
    \SetKwFunction{FCheck}{CheckConvergence}
    \SetKwProg{Fn}{Function}{:}{}
    \Fn{\FCheck{$\alpha, \rho$}}{%
        $\theta^\prime \leftarrow \theta + \alpha p$\;
	$s, s^\prime \leftarrow \sgn{f}{\theta}, \sgn{f}{\theta^\prime}$\;
        $\Delta \leftarrow 2 \rho \alpha \cdot p^\top H \grad{f}{\theta}$\;
        \KwRet $s^\prime \leq s + \Delta$\;}
    \;
    converged $\leftarrow \texttt{CheckConvergence}\left(
    	1, \rho^\prime\right)$\;
    \If{$\mathrm{converged}$}{$\alpha \leftarrow 1$\;
    $\KwRet\ \alpha$}
    \While{$\mathrm{not\ converged}$}{%
        converged $\leftarrow \texttt{CheckConvergence}\left(
            \alpha, \rho\right)$\;
        \If{$\mathrm{not\ converged}$}{$\alpha \leftarrow \beta \alpha$\;
	\If{$\alpha < \eps$}{\alpha \leftarrow 0\;\\ \KwRet\ \alpha\;\\}}
    }%
    $\KwRet\ \alpha$%
    \caption{Backtracking Line Search for Guarded Newton}
    \algolabel{roosta-btls}
\end{algorithm}

\subsubsection{Newton-MR}

The combination of MR-QLP
for inexact Newton step selection
and back-tracking line search
for step size selection
is known as \emph{Newton-MR}%
~\cite{roosta2018}.
This will prove to be the most effective
algorithm for critical point-finding in
\chapterref{three},
so we provide it in pseudocode here
(\algoref{nmr}).
\\
\begin{algorithm}[h]
    \SetAlgoLined{}
    \textbf{Require}
    $\theta_0 \in \R^N$,
    $f\from\R^N\to\R$,
    $\nabla{f}\from\R^N\to\R^N$,
    $\nabla^2{f}\from\R^N\to\R^{N\times N}$,\
    $T\in \N$,\\
    $\texttt{maxit} \in \N$,
    $\texttt{maxxnorm} \in \R$,
    $\texttt{rtol} \in \R$,
    $\texttt{acondlim} \in \R$,
    $\alpha \in \intervaloc{0}{1}$,
    $\beta \in (0, 1)$,
    $\rho \in (0, 1)$,
    $\rho^\prime \in (0, 1)$\\ \ \\
    $t \leftarrow 0$\\
    \While{$t < T$}{%
        $g \leftarrow \nabla{f}\left(\theta_t\right)$\\
        $H \leftarrow \nabla^2{f}\left(\theta_t\right)$\\
        $p \leftarrow \textrm{MR-QLP}\left(
            H,\ -g,\ \texttt{maxit},\ \texttt{maxxnorm},\ \texttt{rtol},\ \texttt{acondlim}\right)$\\
        $\alpha \leftarrow \textrm{BTLS}\left(
            \nabla f, p, \theta_t, \alpha, \beta, \rho, \rho^\prime\right)$\\
        $\theta_{t+1} \leftarrow \theta_t + \alpha p$\\
	\If{$\theta_{t+1} == \theta_{t}$}{\textbf{break}\\}
	$\alpha \leftarrow \max(1, \beta^{-1} \alpha)$\\
        $t \leftarrow t + 1$
    }%
    \caption{Newton-MR}
    \algolabel{nmr}

\end{algorithm}
\noindent In this pseudocode,
MR-QLP is the minimum residual solver
based on the QLP decomposition
from~\cite{choi2011}
described in \sectionref{krylov} and
BTLS is the back-tracking line search method
\algoref{roosta-btls}.

We briefly review the hyperparameters of
MR-QLP here.
\texttt{maxit} sets the maximum number of iterations.
In exact arithmetic, this would never need be greater than $N$.
We found that solutions didn't tend to increase in quality
for values above $N$.
\texttt{maxxnorm} sets the largest acceptable value for the norm of $p$
while \texttt{acondlim} sets the largest acceptable value
for the estimated condition number of the Hessian restricted
to the subspace in which the iterations lie.
We found that MR-QLP did not terminate due to violating these constraints
for reasonable values,
and so we set them to 1e4 and 1e7, respectively.
The critical hyperparameter for stopping behavior was \texttt{rtol},
short for relative tolerance.
When either the
residual or the residual projected onto the Hessian co-kernel
is less than \texttt{rtol},
following suitable normalization,
the algorithm terminates.
Details on the definitions of these quantities
are deferred to \chapterref{three}.
Not listed above is the \texttt{trancond} hyperparameter,
which determines when the algorithm transitions between
using the QR and QLP decompositions,
based on an estimate of the Hessian's condition number.
This hyperparameter was also not critical
for determining stopping behavior of MR-QLP\@.
We set it to 1e4.

\section{Conclusion}\sectionlabel{ch2-conclusion}

In this chapter,
we introduced two major classes of critical point-finding algorithms:
gradient norm minimization methods,
which apply first-order optimization algorithms
to the squared gradient norm,
and
Newton methods,
which use repeated solution of the Newton system of equations.
Both classes of algorithms
use the Hessian matrix of second partial derivatives,
and so are second-order critical point-finding methods.
In part because of the complexity of Newton methods
and in part because of the relative lack of experience and expertise
with these methods in the neural network community,
the Newton methods formed the majority of this chapter.
We approached their explanation pedagogically,
based on an analogy to the calculation of the square root
to high accuracy.
We reviewed exact Newton methods,
which use algebraic solutions of the Newton system,
and inexact Newton methods,
which use iterative methods to approximate those solutions.
Finally,
we developed two inexact Newton methods
of sufficient robustness for practical use,
damped Newton and Newton-MR.\@

In the next chapter,
we will apply these algorithms to neural network loss functions.
After a warm up on a linear neural network
on which we can check our answers,
we apply these methods to non-linear neural networks.
In this case, they will generally fail
to find critical points,
which will motivate a re-analysis
of the behavior second-order critical point-finding algorithms
when the Hessian is singular.

\onlyinsubfile{\printbibliography}

\end{document}
