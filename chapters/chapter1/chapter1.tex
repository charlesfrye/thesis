\documentclass[../../thesis.tex]{subfiles}
\begin{document}

\chapter{Critical Points and the No-Bad-Local-Minima Theory of Neural Network Losses}\chapterlabel{one}
% \onlyinsubfile{\addcontentsline{toc}{chapter}{List of Algorithms}}
\onlyinsubfile{\begin{KeepFromToc}
		\tableofcontents
		\clearpage
		\listoffigures
		\listoftables
	\end{KeepFromToc}}
\onlyinsubfile{\clearpage}
\onlyinsubfile{\linenumbers}

\section{Overview}\sectionlabel{overview}

It is typical to present only the polished final
products of scientific research,
rather than the process itself.
This is exemplified by the advice to
\enquote{write papers backwards},
from the results and conclusions to the introduction and rationale.
While this perhaps makes the research more digestible and
certainly makes it more impressive,
it hides the confusion and failure
that are the day-to-day reality of research.
In this brief overview,
I will try to tell the story of the research project
laid out in this thesis as it was experienced,
warts and all,
and in terms comprehensible to a wide audience.
If you're only interested in the technical material,
proceed to the next section,~\sectionref{nnls},
to get started,
or double back to the abstract for an overview.

Neural networks are machine learning systems
that are, as of the writing of this thesis in 2020,
widely used but poorly understood.
The original purpose of the research project
that became this dissertation was to
understand how the
architecture, dataset, and training method
interact to determine which neural network training problems
are easy.
The approach was inspired by methods from chemical physics~\cite{ballard2017}
and based on an analogy between a physical system minimizing energy
and a machine learning system maximizing performance.
Conceptually, the goal is to characterize all of the configurations
in which the system is stable,
the \emph{critical points} or stationary points
of the system.
The details of this problem setup are the substance
of the remainder of this chapter.

Our intent was to build on the work
of~\cite{dauphin2014} and~\cite{pennington2017},
who had reported a characterization of critical points in
some small, simple neural networks.
We hoped to increase the scale of the networks
closer to what is used in practice,
to try more types of neural networks,
and especially to examine the role of the dataset.

The first step in characterizing the critical points is finding them.
In general, they can't be derived or written in elementary mathematical terms,
and so need to be discovered numerically,
just as highly-performant neural networks have their parameters set
by numerical algorithms rather than by analytical derivations.
\chapterref{two} is a didactic survey of algorithms
for finding critical points.
Our early attempts to reproduce the results
in~\cite{dauphin2014} and~\cite{pennington2017}
appeared to be failures.
The metric usually used to measure how close one is
to a critical point, the squared gradient norm,
stubbornly refused to decrease.

The algorithms used to find critical points are complicated ---
both in terms of their implementation and
in terms of the number of knobs, or hyperparameters,
we can to twiddle to configure them.
Furthermore, they behave quite differently
from typical machine learning algorithms,
and so intuition gained from experience working with those algorithms
can be misleading.
We had implemented these critical point-finding algorithms ourselves,
due to the absence, at the beginning of this research project,
of important technical tools in typical neural network software packages.
We furthermore had limited expertise and experience in this domain,
so our first thought was that we had either implemented the algorithms incorrectly
or weren't configuring them properly.

As it turned out, both of those hypotheses were correct,
but verifying them became a research project in itself.
The key innovation was the introduction of the \emph{deep linear autoencoder}
as a test problem.
For this very special neural network,
the critical points actually are known mathematically,
and have been since the late 80s~\cite{baldi1989}.
With these \enquote{correct answers} in hand,
we can check the work of our algorithms.
These results were written up for the arXiV
in~\cite{frye2019}
and rejected from ICML2019.
They form the first part of~\chapterref{three}.

Unfortunately, the process of debugging and tuning critical point-finding
algorithms on the deep linear autoencoder did not solve
our performance problems on non-linear networks.
It remained the case that the squared gradient norm metric was abnormally high,
along with other signatures of bad behavior on the part of our algorithms.

However, the exercise of verifying our algorithms on the deep linear autoencoder
gave us the confidence to consider other, more fundamental causes for failure.
In reviewing the literature on the methods used to find critical points,
it became clear that a particular failure mode for these methods
was not well-appreciated.
Implicit in the literature on critical point-finding was the fact that,
whenever a certain vector (the gradient)
was mapped to 0 by a certain matrix (the Hessian),
critical point-finding would fail%
~\cite{griewank1983,boyd2004,roosta2018}.
We named this condition \emph{gradient-flatness}
and, on reviewing the outputs of our critical point-finding algorithms
when applied to neural networks,
we observed it ubiquitously.
The concept of, evidence for, and consequences of gradient-flatness
in neural networks are the focus of the second part of \chapterref{three}.
These results were written up separately for the arXiV
in~\cite{frye2020}.

The biggest take-home message of our observations for the field
is that the famous results
in~\cite{dauphin2014} and~\cite{pennington2017}
need an asterisk:
the points characterized by those papers appear to be
gradient-flat points, not critical points,
which has distinct consequences for our understanding of neural networks.

In the remainder of this chapter,
I will set up the problem of training neural networks
and describe the critical point-based perspective on it,
the no-bad-local-minima theory.

\section{Neural Network Losses}\sectionlabel{nnls}

Neural networks are a highly flexible class of
differentiable functions
suitable to a wide variety of
machine learning tasks~\cite{lecun2015}.
\emph{Neural networks} are constructed by interleaving
parameterized linear transformations
with (optionally parameterized) non-linear transformations.
In order to be able to apply calculus
to our networks, we will assume that
the parameters are stored in a vector, $\theta$,
and converted into a function by a network constructor
$\mathrm{NN}$:
\begin{equation}
	\mathrm{NN}(\theta) \from \cX \to \cY
\end{equation}

While neural networks can be used for any machine learning task,
we will focus on the important special case of supervised learning.
In supervised learning, a collection of inputs $X$ and targets $Y$
are provided, and the goal is to find a function from $\cX$ to $\cY$
such that the value of some cost function $c$ is low, on average, when applied to
matching pairs of targets and inputs
after being passed through the network.
Optionally, certain parameter values may be considered more \enquote{costly}
than others, and this is enforced by a regularizer $r$
that is added to the cost.
The result is called the \emph{loss function}
and is defined below.
\begin{definition}{Loss Function}{loss}
	Given a neural network constructor $\mathrm{NN}$,
	a cost function $c$,
	a regularizer $r$,
	and input data $X$ and targets $Y$,
	we define the \emph{loss function} as
	\begin{equation}\equationlabel{def-loss}
		L(\theta) \defeq
		\frac{1}{n}\sum_{x_i, y_i \in X, Y}
			c(\mathrm{NN}(\theta)(x_i), y_i) + r(\theta)
	\end{equation}
\end{definition}
The notation for this setup is summarized in \tableref{loss-notation}.
Note that,
because the parameters are the thing over which we have control,
we think of this as a function of the parameters $\theta$,
even though it might in other contexts be considered
a function of the network architecture, the data, or both.

For example,
a \emph{fully-connected network}
has as its parameters a tuple of weight matrices $W_i$
and applies a non-linear function $\sigma$
after applying each in turn:

\begin{equation}
	\mathrm{NN}\left(W_1, \dots W_{k-1}, W_k\right)
	= W_k \after \sigma \after W_{k-1} \after \sigma \after
	\dots \sigma \after W_1
\end{equation}

\begin{table}[h]
	\begin{center}
		\begin{tabular}{rl}
			\textbf{Name}       & \textbf{Type}                          \\
			Inputs              & $X \from \cX^n \defeq \R^{m\times n}$  \\
			Targets, Outputs    & $Y \from \cY^n \defeq \R^{p\times n}$  \\
			Parameters          & $\theta \from \Theta \cong \R^N$        \\
			Loss Function       & $L \from \Theta \to \R$                \\
			Network Constructor & $\mathrm{NN} \from \Theta \to \cY^\cX$ \\
			Cost Function       & $c \from \cY \times \cY \to \R$        \\
			Regularizer         & $r \from \Theta \to \R$
		\end{tabular}
	\end{center}
	\caption{\textbf{Definitions of Terms and Symbols for Neural Network Loss Functions}.}{%
		The \enquote{type} of an object is either set of which it is an element
		or, for a function, the types of its inputs and outputs,
		denoted by $\from$ the input type $\to$ the output type.
		The symbol $\cY^{\cX}$
		denotes the set of all functions from the domain $\cX$ to $\cY$.
	}
	\tablelabel{loss-notation}
\end{table}

The process of \enquote{training} a neural network is the process
of selecting a value of the parameters, $\theta^\star$, that minimizes the loss:
\begin{equation}\equationlabel{minimize}
	\theta^\star \in \argmin_{\theta \in \Theta} L(\theta)
\end{equation}
That is, we treat the process of inferring the best parameters for our network,
the process of programming our machine-learned algorithm,
as an optimization problem.
This is the \emph{variational approach},
which is a ubiquitous method in mathematics%
\footnote{E.g.~the Courant-Fischer-Weyl characterization of eigenvalues,
the variational approach to inference~\cite{wainwright2007},
the Lagrangian approach to mechanics,
and even the universal construction approach in category theory~\cite{milewski2014}.
}.
One might think of machine learning in general as
a variational approach to programming computers.
An element of an $\argmin$ is known as a \emph{global minimum}.
Finding global minima is generically a hard problem in the strictest sense,
precisely because almost any problem in mathematics can be formulated as an
optimization problem.
The variational approach is particularly useful
when the resulting optimization problem
has a fast solution algorithm.

Almost all methods for optimizing neural networks
are gradient-based.
That is, they use the gradient function,
which satisfies the following relation:

\begin{definition}{Gradient Function}{gradient}
	The \emph{gradient function} of a function $L \from \Theta \to \R$
	is denoted $\nabla{L} \from \Theta \to \Theta$
	and satisfies
	\begin{align}
		&L(\theta + \eps) = L(\theta) + \langle\grad{L}{\theta},\eps\rangle + o(\eps)\equationlabel{grad-def}
	\end{align}
	for all $\theta, \eps \in \Theta$.
	The values returned by this function are called \emph{gradients}
	or \emph{gradient vectors}.
\end{definition}
\noindent If a method only uses the function and the gradient,
we call it a \emph{first-order} method.

The gradient at a point $\theta$ is a vector that can be used
as a linear functional applied to a perturbation $\eps$ that approximates
the value of $L$ at $\theta + \eps$.
If we drop the little-$o$ term,
we obtain this linear approximation,
also known as a first-order Taylor expansion:

\begin{align}
	&\hat{L}(\theta + \eps) = L(\theta) + \langle\grad{L}{\theta},\eps\rangle
\end{align}

Minimizing $\hat{L}$ would mean selecting an infinitely-long step $\eps$
in a direction with negative inner product with the gradient.
But our goal is to minimize $L$, not $\hat{L}$,
and as $\eps$ grows, so does the approximation error $o(\eps)$,
and so we select a finite length vector.
We choose the one that makes that inner product most negative,
for its length.
This is the negative gradient.
We then typically apply a scaling factor $\eta$,
called the \emph{learning rate} or \emph{step size}.
The resulting optimization method,
defined in \algoref{gd},
is known as \emph{gradient descent}.
\\\\
\begin{algorithm}[H]
	\SetAlgoLined{}
	\textbf{Require} $\theta_0\in\Theta, \eta\in\R^+, \nabla{L}\from\Theta\to\Theta, T\in\N^+$\\
	\While{$t < T$}{$\theta_{t+1} \leftarrow \theta_t - \eta\grad{L}{\theta_t}\\t \leftarrow t+1$\\}
	\algolabel{gd}
	\caption{Gradient Descent}
\end{algorithm}
\ \\
This method
and its stochastic and accelerated variants
have some hope of working on smooth functions because
whenever the parameter is a minimizer,
the gradient
is $0$:
\begin{equation}\equationlabel{minimizers-critical}
	\theta^\star \in \argmin_{\theta \in \Theta} L(\theta)
	\Rightarrow \grad{L}{\theta^\star} = 0
\end{equation}
and so, if initialized from a minimizer,
any gradient-based algorithm will stay there.
Below, we will demonstrate that for
convex smooth functions,
this algorithm, for an appropriate choice of $\eta$,
will converge to a minimizer.

Unfortunately,
neural network loss surfaces are not convex,
and so the theory built up around convex optimization
(see~\cite{bubeck2015,boyd2004})
would suggest that training neural networks should be hard.
And indeed, the experience of practitioners
working on neural networks in the 80s and 90s
was that training them was difficult.
Nowadays, however,
it is recognized that training large neural networks
with gradient-based methods
is actually quite easy,
in that many problems can be avoided with
a few generic tricks~\cite{sun2019}.
One key hypothesis as to why is the
no-bad-local-minima theory.
To understand it,
we need to consider the kinds of structures
that can appear in a non-convex function,
and which of them are compatible with gradient-based optimization.

\section{Critical Points}\sectionlabel{cp}

One approach to analyzing the behavior of optimization algorithms
is to split the task of determining convergence
into two steps:
first, identify the points which are \emph{stationary},
at which the update is $0$,
and then determine which of those points are actual targets of convergence.
We call the stationary points of the loss
for the gradient descent algorithm its critical points.
\begin{definition}{Critical Points}{critical}
	The set of all \emph{critical points} of a loss function $L$
	on a domain $\Theta$ is denoted $\Theta^L_{\cp}$ and defined as
	\begin{equation}\equationlabel{def-thetacp}
		\Theta^L_{\cp} \defeq \Set{\theta \in \Theta \suchthat \grad{L}{\theta} = 0}
	\end{equation}
	When unambiguous, the super-script $L$ will be omitted.
\end{definition}
In a na{\"\i}ve first pass,
it would seem that all $\theta \in \Theta_{\cp}$
are also targets of convergence.
If we initialize \algoref{gd} to one of these points
($\theta_{0} \in \Theta_{\cp}$),
then for $\theta_t = \theta_0$ for all $t$.
Therefore, if initialized from a critical point,
the algorithm will converge to that critical point.

This picture is mis-leading for practical purposes,
but even this coarse approach is sufficient to guarantee
that gradient-based methods converge on smooth convex functions.
A smooth function $f$ is convex iff
$f(y) \geq f(x) + \langle\grad{f}{x}(y - x)\rangle$
for all $x$ and $y$ in its domain $\Omega$.
The loss functions for linear regression and
logistic regression are convex,
including when convex regularization is applied,
e.g.~LASSO or ridge~\cite{hastie2016}.
See~\figureref{convex-vs-nonconvex}.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=\textwidth]{img/chapter1/convex-vs-nonconvex.pdf}
	\end{center}
	\caption{\textbf{The Critical Points and Global Minima
	of a Convex and a Non-Convex Function}}{\textbf{A}:
	The convex function $L(\theta) = \theta^2$ (black)
	and its critical points (gold)
	and minimizers (blue outline).
	\textbf{B}:
	Same as in \emph{A}, but for the non-convex function
	$L(\theta) = \cos(\theta) + \theta +\theta^2$.}
	\figurelabel{convex-vs-nonconvex}
\end{figure}

Applying this definition of convexity at a point $x_{\cp}$
among the critical points of $f$, $\Omega^f_{\cp}$,
we have that
\begin{align}
	f(y) &\geq f\left(x_{\cp}\right) + \langle\grad{f}{x_{\cp}}, y - x_{\cp} \rangle\\
	f(y) &\geq f\left(x_{\cp}\right) + 0 = f\left(x_{\cp}\right)
\end{align}
for \emph{any} point $y$,
This is part of the power of convexity:
local information (encoded in the gradient)
gives global information (in the form of a global lower bound).
With it, we can improve~\ref{eq:minimizers-critical}
from a one-way implication to a biconditional,
from \enquote{if-then} to
\enquote{if and only if}:

\begin{equation}\equationlabel{convexity-critical}
	L \text{\ smooth, convex\ } \Rightarrow\\
	\theta \in \argmin_{\theta \in \Theta} L(\theta)
	\Leftrightarrow \grad{L}{\theta} = 0
	\Leftrightarrow \theta \in \Theta_{\cp}
\end{equation}

Another way to characterize smooth convex functions is through their Hessian function.
The Hessian function returns matrices that satisfy the relation below:
\begin{definition}{Hessian Function}{hessian}
	The \emph{Hessian function} of a function $L \from \Theta \to \R$
	is a function $\nabla^2{L} \from \Theta \to \R^{N\times N}$,
	where $N$ is the dimensionality of $\Theta$,
	that satisfies
	\begin{equation}\equationlabel{def-hessian}
		\grad{L}{\theta + \eps} = \grad{L}{\theta} + \hess{L}{\theta}\eps + o(\eps)
	\end{equation}
	for all $\theta, \eps \in \Theta$.
	The matrices returned by this function are called \emph{Hessians}
	or \emph{Hessian matrices}.
\end{definition}
It is the \enquote{gradient of the gradient},
in that it returns a linear function (a matrix)
that approximates the gradient function,
which itself returns a linear functional (a vector)
that approximates the original scalar function.

Combined, the gradient function and the Hessian function produce a quadratic approximation
of the original function $L$:
\begin{equation}\equationlabel{taylor-two}
	L\left(\theta + \eps\right) =
	L(\theta)
	+ \langle\grad{L}{\theta},\eps\rangle
	+ \eps^\top\hess{L}{\theta}\eps + o(\snorm{\eps})
\end{equation}

Note that the Hessian appears in~\equationref{taylor-two}
as a \emph{quadratic form}:
a symmetric matrix pre- and post-multiplied with the same vector.
Quadratic forms are classified,
up to a change of basis, by the eigenvalue spectrum
of the underlying matrix:
the number of positive, negative, and 0 eigenvalues\footnote{%
Because the matrices are real and symmetric,
there are no complex eigenvalues.}.
We will later classify critical points in the same fashion.

Smooth convex functions are precisely those functions whose
Hessian matrix has no negative eigenvalues at any
point.
Such a matrix $M$ is called \emph{positive semi-definite},
denoted%
\footnote{Specifically,
$\succeq$ is the Loewner partial order
on symmetric matrices.
$A \succeq B$ if the smallest eigenvalue
of $A - B$ is greater than or equal to $0$.
$\succ$ is defined using strict inequality.}
$M \succeq 0$.
If $M \succ 0$, then its smallest eigenvalue is positive,
and the matrix is \emph{positive definite}.

If the Hessian $M$ at a point is positive definite
then $x^\top M x$ is always positive.
This implies that the second-order term in~\equationref{taylor-two}
is positive.
Since the second-order term dominates the higher-order terms
for sufficiently small $\eps$,
at a point $\theta^*$ where the gradient is $0$
and the Hessian is positive definite,
we have that
\begin{align}
	L\left(\theta^* + \eps\right) &=
	L(\theta^*)
	+ \langle\grad{L}{\theta^*},\eps\rangle
	+ \eps^\top\hess{L}{\theta^*}\eps + o(\snorm{\eps})\\
	&=
	L(\theta^*)
	+ 0
	+ \eps^\top\hess{L}{\theta^*}\eps + o(\snorm{\eps})\\
	&\geq
	L(\theta^*)
\end{align}

Such a $\theta^*$ is a called a \emph{local minimum},
since it is a minimizer of $L$ in all its neighborhoods
under a given size.
\begin{definition}{Local Minima}{lm}
	The set of all \emph{local minima} of a scalar function $L$
	on a domain $\Theta$ is denoted $\Theta^L_{\lm}$ and defined as
	\begin{equation}\equationlabel{def-thetalm}
		\Theta^L_{\lm} \defeq
		\Set{\theta \in \Theta \suchthat L(\theta + \eps) \geq L(\theta)}
	\end{equation}
	for some $\eps > 0$.
	When unambiguous, the super-script $L$ will be omitted.
\end{definition}

By a small extension of the above
argument\footnote{The extension demonstrates that,
for convex functions,
points with positive semi-definite Hessians
are still minimizers.
See~\cite{boyd2004}.
This is untrue in the non-convex case,
and checking whether a point is a minimum becomes NP-Hard at worst,
see~\cite{murty1987}.},
we come to a final characterization
of why gradient descent converges on smooth, convex
functions\footnote{In fact
the class of functions for which the implication~\ref{eq:cvx-minimizers}
holds is broader.
They are known as smooth \emph{invex} functions,
see~\cite{invex2008}.}:
all critical points are local minima
and all local minima are also global minima,
i.e.~elements of the $\argmin$:

\begin{equation}\equationlabel{cvx-minimizers}
	f\from\Omega\to\R \text{\ smooth, convex\ }
	\Rightarrow \Omega_{\lm} = \argmin_{\Omega} f = \Omega_{\cp}
\end{equation}
\noindent From this, we can deduce
that any algorithm that converges to a generic critical point will converge,
on smooth convex functions, to a minimizer.

This condition is sufficient, but not necessary,
for gradient descent to converge to a local minimizer.
The sticking point is when there are
critical points which are not local minimizers:
$\Theta_{\cp} \supset \Theta_{\lm}$.
Does gradient descent converge to non-minimizing critical points,
or only to local minimizers?

Reviewing the second order approximation of the loss
in~\equationref{taylor-two},
we see that, at a critical point $\theta_\cp$,
the first order term vanishes
\begin{equation}
	L(\theta_\cp + \eps) = \eps^\top\hess{L}{\theta_\cp}\eps + o(\snorm{\eps})
\end{equation}
\noindent leaving only the second-order term.
We can therefore classify critical points according to the
eigenvalue spectrum of their associated Hessian.
The fraction of negative eigenvalues is known as the
\emph{index} of the critical point.

\begin{definition}{Index}{index}
	For a critical point $\theta\in\Theta^L_{\cp}$,
	we define the \emph{index}
	as the number of negative eigenvalues
	of the Hessian of $L$ at $\theta$:
	\begin{equation}\equationlabel{def-index}
		I(\theta) = \frac{1}{N}
		\sum_{\lambda_i \in \Lambda\left(\hess{L}{\theta}\right)}%
		{\Ind\left(\lambda_i < 0\right)}
	\end{equation}
	where $\Lambda\from S\R^{k\times k}\to\R^k$
	is a function that takes in a symmetric real matrix
	and returns its eigenvalues as a vector
	and $\Ind$ is the indicator function.
\end{definition}

Points with index strictly between $0$ and $1$
are \emph{strict saddle points}.
These are points where the gradient is $0$
and the local curvature is upwards in some directions
and downwards in others.
See~\figureref{nblm-example} for an example.
If all eigenvalues are non-zero,
then critical points with index $0$ are local minima
and critical points with index $1$ are local maxima.
If some eigenvalues are zero and the index is $0$,
then the critical point may be a local minimum
or may be a (non-strict) saddle point,
but higher-order derivatives are needed to disambiguate.

If all saddle points are strict saddle points,
a condition known as the \emph{strict saddle property},
then gradient descent converges to a local minimizer~\cite{lee2016,lee2019}.
Furthermore, convergence for a stochastic
version of~\algoref{gd} is fast~\cite{jin2018a}
and can be accelerated via momentum~\cite{jin2018b}.

\begin{example}[float=hp]{1-Dimensional Deep Linear Autoencoder}{1ddlae}
	In \defref{loss}, take $X = Y = 1$,
	$c(y, \hat{y}) = {\left(y - \hat{y}\right)}^2$,
	$r(\theta) = 0$,
	and
	$\mathrm{NN}(\theta) = \theta_2\theta_1$
	for $\Theta = \R^2$.
	The resulting loss function $L$ is
	\begin{equation}\equationlabel{nblm-example}
		L\left(\theta_1, \theta_2\right)
		= {\left(\theta_2\theta_1 - 1\right)}^2
	\end{equation}
	In more standard terminology,
	it corresponds to the choice of
	the mean squared error cost function
	and no regularization
	for a linear autoencoder network
	with one-dimensional inputs
	and a one-dimensional hidden layer.
	\\ \ \\
	The function is not convex,
	because the gradient
	does not provide a global lower bound
	at some points, e.g., at the origin
	where it is the zero vector.
	The origin is a critical point
	that is not a minimizer
	(the loss takes on value $1 > 0$)
	but instead a strict saddle point.
	However, this function can still be optimized effectively with
	gradient-based methods.

	\begin{center}
		\includegraphics[width=0.6\linewidth]{img/chapter1/nblm-example.pdf}
	\end{center}
	\captionof{figure}%
	{\textbf{A Loss Function that Satisfies the Strict Saddle Property}.}%
	{Values of the function defined in \exampleref{1ddlae}
	 on a domain centered at the origin.
	 The value of $L$ is represented by color,
	 with low values in black and high values in white.
	 Contours are logarithmically-spaced for illustrative purposes.
	 Critical points in blue, and global minima in gold.
 	 The isolated critical point at the origin is a strict saddle.}
	\figurelabel{nblm-example}
\end{example}

This is a convenient property, but is it satisfied
by any losses of practical interest?
As an illustrative example, consider the quartic loss function
of two variables, \exampleref{1ddlae} below,
which is based on perhaps the simplest neural network.
The example is artificial---
it is using machine learning to multiply by 1,
which is excessive even in the contemporary era of ML hype
---but it is closely related to principal components analysis (PCA),
as we will see in~\chapterref{two}.
More generally, the strict-saddle property is
satisfied by tensor decomposition problems~\cite{ge2015},
which covers a number of latent variable models, including
PCA,
independent components analysis (ICA~\cite{bell1997,comon2009}),
sparse coding~\cite{olshausen1996}
and other forms of dictionary learning,
and Hidden Markov models~\cite{anandkumar2012}.

In many of these cases,
all local minima are also global minima,
and so gradient descent is sufficient for
those optimization problems.
The question of whether this holds for neural network loss functions
is the subject of the next section.

\section{The No-Bad-Local-Minima Theory}\sectionlabel{nblm}

When all local minima of a function
are approximately global minima,
we will say that the function has the
\emph{no-bad-local-minima property},
or the NBLM property.
While~\exampleref{1ddlae} has the NBLM property,
the function in panel B of~\figureref{convex-vs-nonconvex}
does not, due to the presence of a non-global local minimum.
We will refer to the hypothesis that
the loss functions of large neural networks
satisfy the NBLM property as the
\emph{no-bad-local-minima theory}.

Informally, the argument goes as follows:
at any critical point, imagine that the eigenvalues
of the Hessian are drawn randomly.
If there is even a small chance that any eigenvalue is negative,
then for a sufficiently large network,
there will be almost surely at least one negative eigenvalue,
by the strong law of large numbers.
When the value of the loss is low,
we might expect that the probability of a positive eigenvalue becomes 1,
while when the value of the loss is high,
negative and positive values are both possible.
A random loss function drawn according to these rules
will have the NBLM property with a probability that
rapidly increases with dimension.

In \sectionref{grf},
we will cover the first model of neural network loss functions
meant to formalize this intuition,
the Gaussian random field model of~\cite{dauphin2014}.
Then, in \sectionref{wishart},
we will explain how this model was improved
by incorporating observations about the spectrum
of the Hessians of neural networks~\cite{pennington2017}.
Then, we will discuss, in \sectionref{critic},
weaknesses of and negative analytical results for
the broadest versions of the NBLM theory,
and close with a review of the alternatives,
in~\sectionref{alternative}.

\subsection{The Gaussian Random Field Model of Neural Network Losses}\sectionlabel{grf}

One of the simplest interesting classes of random functions
that have the NBLM property is the class of
\emph{Gaussian random fields}.
Gaussian random fields are random functions
from $\R^n$ to $\R$.
An example of a single draw from a Gaussian random field defined on $\R^2$
appears in~\figureref{2dgrf}.

With most random variables,
discussion can proceed directly from writing down a density.
But with random functions,
the situation is more complicated,
because it is difficult to consider densities and integrals over
the space of functions.
In addition to necessitating that we typically visualize random functions
by looking at a single example
(and hoping that any properties we notice are \enquote{typical}),
rather than drawing a histogram or density,
this further necessitates a somewhat strange definition for the Gaussian random field.
The definition is clearer if we first introduce an analogous definition
for multivariate Gaussian random variables.

\begin{definition}{Multivariate Gaussian}{mvg}
	A random $k$-dimensional vector $v$ is \emph{multivariate Gaussian} if
	the random variable $w^\top v$ is Gaussian-distributed
	for all $w\in\R^k$.
\end{definition}
\noindent As a corollary,
the family of multivariate Gaussians is closed under all linear transformations,
since the composition of a linear transformation and an inner product
is equal to an inner product with another vector.

We can similarly define a Gaussian random field
by requiring that the vectors obtained by
evaluating the random function at any set of test points
have the same property.
\begin{definition}{Gaussian Random Field}{grf}
	A random function $f$ from $\R^k\to\R$ is a \emph{Gaussian random field}
	if the random variable $w^\top f^{\otimes n}(x_1 \dots x_n)$
	is Gaussian-distributed
	for all $w\in\R^n, x_i \in\R^k$
	and for every $n \in \N$.
\end{definition}
\noindent Here $f^{\otimes n}$ is the $n$-fold product of the function $f$,
which applies the function $f$ to $n$ separate inputs,
returning $n$ outputs\footnote{%
Specifically,
$f^{\otimes n}{\left(x_1, \dots, x_n\right)}_i = f\left(x_i\right)$
for all $i$ from $1$ to $n$.}.

There is one example of a Gaussian random field
that is familiar to machine learning practitioners:
the Gaussian process.

\begin{example}{Well-Known Gaussian Processes}{gp}
	Choose $k = 1$ in the \defref{grf}.
	This special case of a Gaussian random field
	is called a \emph{Gaussian process}.
	\\ \ \\
	If $f(x_i)$ is independent of $f(x_j)$
	for all $x_i, x_j \in \R$,
	then $f$ is a
	\emph{white noise Gaussian process}.
	\\ \ \\
	The cumulative integral of a white noise Gaussian process
	is also a Gaussian process,
	called a \emph{Wiener process} or a
	\emph{random walk}.
	\\ \ \\
	Consider a single unit in a neural network with
	independent, Gaussian random weights $\theta$.
	The family of Gaussian random variables
	is closed under linear combinations,
	so the random variable $\theta^\top x$ is Gaussian for all $x$.
	Such a neuron is therefore a Gaussian process.
	With care, this can be extended into a model for
	deep nonlinear neural networks with very large layers and
	non-Gaussian random weights,
	see~\cite{jacot2018}.

\end{example}

Usefully, a Gaussian random field can be defined in terms of its
mean function
$\mu\from\R^k\to\R$
and covariance function, or kernel,
$K\from\R^k\times\R^k\to\R^\k$,
which must be positive semi-definite.
For many choices of kernel,
the field is smooth
(and its partial derivatives are also Gaussian random fields).
See \figureref{2dgrf},
which depicts a single draw from the ensemble of smooth
Gaussian random fields with a particular squared exponential,
or Gaussian, kernel.
Notice that it has local minima, saddles, and maxima.
If we are to use this ensemble as a model of neural network losses
that can explain their optimizability,
we would like a statistical description of these critical points.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.6\linewidth]{img/chapter1/2dgrf.pdf}
	\end{center}
	\caption%
	{\textbf{2-Dimensional Gaussian Random Field}.}
	{Note that this is a single realization of a Gaussian random field,
	akin to a single observation of a random variable.}
	\figurelabel{2dgrf}
\end{figure}

The key work on the critical points of smooth Gaussian random fields
is~\cite{bray2007}.
Their core result showed
that the expected index (\defref{index}) of critical points
is a tight function of the field's value at that point%
\footnote{Deviations of order $O(1/\sqrt{N})$,
where $N$ is the dimension of the field.}.
Furthermore, for many kernels,
including the squared exponential kernel,
the expected index is an increasing function of the loss,
and the deviations are even smaller%
\footnote{Deviations of order $O(1/N)$.}.

As a consequence, if a high-dimensional loss function is a typical member of the
this random field ensemble, then
when the loss is high,
the number of negative eigenvalues at critical points is high,
and so the critical points are all saddles or maxima.
Below a certain value of the loss,
all of the critical points will be minima.
Furthermore,
the minima, including the global minima,
will have approximately the same loss.
This is an effective restatement
of the no-bad-local-minima property.

One benefit of this model is that it
applies only to high-dimensional loss functions.
This would explain the difference in optimizability
of contemporary neural networks,
where parameter counts ($\dim \Theta$)
are in the hundreds of thousands or millions,
and neural networks in the 80s and 90s,
which were orders of magnitude smaller in size.

The hypothesis that neural network loss surfaces
might be well-modeled by typical members
of the Gaussian random field ensemble
was first put forward in~\cite{dauphin2014}.
They reported numerical results on the critical points
of two neural network losses that were in qualitative
agreement with the NBLM property
of Gaussian random fields.
We will later see (\chapterref{three})
that these results have a caveat to them,
due to weaknesses of the numerical methods.
Because~\cite{dauphin2014} predated,
and to some extent inspired,
results on the convergence of gradient descent
in the presence of saddles~\cite{lee2016,jin2018a},
it focused on defining an optimization method,
saddle-free Newton,
that was clearly repelled by saddles.

\subsection{Improving the Gaussian Random Field Model with Wishart Matrices}\sectionlabel{wishart}

In this section, we will see
how the Gaussian random field model misses the mark
in predicting the spectrum of neural network loss Hessians
and develop a better model, following~\cite{pennington2017}.
We begin by considering the random matrix ensembles
from which the Hessians of the Gaussian random field model
are drawn.
For a fuller treatment of the theory of random matrices,
including proofs of the statements below regarding spectra,
see~\cite{feier2012,tao2012}.

The eigenvalues of random matrices enjoy a similar phenomenon
to the Central Limit Theorem for sums of random variables:
when the right independence assumption is made,
the details of the random variables don't matter,
and the result has a stereotypical distribution.
For averages of random variables,
that stereotypical distribution is the Gaussian.
For eigenvalues of random matrices,
it is the Wigner semi-circle distribution.

First, let's define the ensemble of random matrices
associated with this distribution:
the symmetric Wigner random matrices.
\begin{definition}{Symmetric Wigner Random Matrix}{wigner}
	A random square $n\times n$ matrix $M$ is \emph{symmetric Wigner}
	if the random entries $M_{ij}$ satisfy the following:
	\begin{align*}
		\forall i,j &\suchthat 1 \leq i < j \leq n\\
			&\Ex{M_{i, j}} = 0\\
			&\Ex{M_{i, j}^2} = 1/n\\
			&M_{j, i} = M_{i, j} \text{\ and\ } M_{i, j} \text{\ iid}\\
		\forall i\\
			&\Ex{M_{i, i}} = \gamma\\
			&\Ex{M_{i, i}^2} = 1/n\\
			&M_{i, i} \text{\ i.i.d}
	\end{align*}
	where iid is short-hand for independent
	and identically-distributed
	and $\gamma \in \R$.
	Note that the entries along the diagonal,
	defined in the second block,
	may have a different
	distribution than the entries off the diagonal,
	defined in the first block,
	but both groups are iid.
\end{definition}

A single draw from this ensemble is pictured
in~\figureref{wigner-rm},
along with its observed eigenvalue distribution
and cumulative distribution.
As the size of the matrix $n$ goes to $\infty$,
the observed eigenvalue distribution converges,
in the almost sure sense,
to a Wigner semicircular distribution.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=\textwidth]{img/chapter1/wigner-rm.pdf}
	\end{center}
	\caption{\textbf{The Wigner Semicircular Distribution}.}%
	{\textbf{A}: An example $50\times50$ symmetric Wigner random matrix drawn from the ensemble
		in~\defref{wigner}. Entries are Gaussian-distributed.
		More negative values in black, more positive values in white.
	\textbf{B}: The expected spectral density of the matrix in \emph{A} (gold),
		which is given by \equationref{def-wigner-dist},
		and the histogram of observed eigenvalues (blue). Note the close match.
	\textbf{C}: The expected cumulative spectral distribution
		of the matrix in \emph{A} (gold), which is the integral of the
		density in $\emph{B}$, and the cumulative fraction of observed eigenvalues (blue).
		Again, the match is close even for a fairly small matrix.
}
	\figurelabel{wigner-rm}
\end{figure}

\begin{definition}{Wigner Semicircular Distribution}{wigner-dist}
	The ($\gamma$-shifted) \emph{Wigner semicircular distribution}
	is the distribution associated with the density
	$\nu \from \left[-2+\gamma, 2+\gamma\right] \to \R$,
	defined as
	\begin{equation}\equationlabel{def-wigner-dist}
		\nu(x) = \frac{1}{2\pi}\sqrt{4 - {(x-\gamma)}^2}
	\end{equation}
	for $\gamma\in\R$.
	When the shift parameter $\gamma$ is $0$,
	we drop the prefix of \enquote{shifted}.
\end{definition}
This distribution is symmetric about the control parameter $\gamma$.
For $\gamma=0$, this means that, for a large symmetric Wigner matrix,
approximately half of the eigenvalues will
be negative and half of the values will be positive.
As $\gamma$ changes, the fraction of eigenvalues above and below zero changes.

We can now be a bit more specific about the results of~\cite{bray2007}.
They found that the Hessian spectra of
critical points of Gaussian random fields
have $\gamma$-shifted Wigner semicircular distributions
whose shift parameter $\gamma$ depends on the value of the random field
at that critical point.
This gives an index that is similarly dependent.

How well does this spectrum match up against the observed eigenvalues
of neural network loss Hessians?
The expected distribution is
continuous with respect to the Lebesgue measure,
i.e.~it has no atoms, or single points with finite probability mass.
In particular, there is no atom at $0$,
and so the random matrices almost surely have no exactly zero eigenvalues.
This means that their kernel, defined below, consists only of the zero vector,
and the matrices almost surely have inverses.

\begin{definition}{Kernel, Co-Kernel, and Rank}{kernel}
	For a matrix $M\in\R^{m\times n}$,
	we define the $\emph{kernel}$ of $M$,
	denoted $\ker M$,
	as the set of all vectors mapped to $0$ by $M$:
	\begin{equation}\equationlabel{def-ker}
		\ker M \defeq \Set{v \in \R^n \suchthat Mv = 0}
	\end{equation}
	We define the $\emph{co-kernel}$ of $M$,
	denoted $\co\ker M$,
	as the subspace spanned by all vectors not mapped to $0$ by $M$:
	\begin{equation}\equationlabel{def-coker}
		\co\ker M \defeq \Set{v \in \R^n \suchthat Mv \neq 0} \cup \Set{0}
	\end{equation}
	These two subspaces are \emph{complementary}:
	their union is $\R^n$
	and their dimensions add up to $n$.
	The \emph{rank} of $M$, $\rk M$,
	is equal to the dimension of $\co\ker M$.
	A matrix with rank less than the minimum
	of its input and output dimension
	is said to have \emph{low rank}.
\end{definition}

This is in stark contrast to the Hessian matrices
of typical neural network loss functions~\cite{sagun2017}.
There are many $0$ eigenvalues,
along with some eigenvalues at much larger values,
with fewer and lower magnitude negative eigenvalues.
Furthermore, the low-rank structure of the Hessian matrix
can be read off from the clear patterns in the values,
unlike the clearly independent (aside from symmetry) values
in the matrix in~\figureref{wigner-rm}.

There is an alternative random matrix ensemble
that has some of these properties:
the Wishart random matrix ensemble.

\begin{definition}{Wishart Random Matrix}{wishart}
	A random square $n\times n$ matrix $M$ is \emph{Wishart}
	if it can be constructed as
	\begin{align}
		M = XX^\top
	\end{align}
	where $X$ is a random $n \times k$ matrix with
	iid Gaussian entries
	\begin{align}
		X_{ij} \sim \Normal\left(0, 1/k\right)
	\end{align}
	It is the sample covariance matrix of a $k$-element sample
	of an $n$-dimensional Gaussian random vector with
	iid components.
\end{definition}
\noindent The rank of a Wishart random matrix is almost surely equal to
the minimum of $n$ and $k$.
When $k$ is less than $n$,
the matrix is low-rank.

The expected spectrum of a Wishart random matrix
follows the Mar{\c c}enko-Pastur distribution
with parameter $\gamma = \frac{n}{k}$,
defined below.
A single draw from this ensemble is pictured
in~\figureref{wishart-rm},
along with its observed eigenvalue distribution
and cumulative distribution.
Note the presence of a large bulk at $0$.

\begin{definition}{Mar{\c c}enko-Pastur Distribution}{mp-dist}
	The \emph{Mar{\c c}enko-Pastur distribution}
	with parameter $\gamma$ is defined as
	\begin{equation}\equationlabel{def-mp-dist}
		\nu(X) =
			\begin{cases}
				\Ind\left(0 \in X\right)%
					\left(1-\frac{1}{\gamma}\right) + \mu(X),
				& \text{if } \gamma > 1\\
				\mu(X),
				& \text{if } \gamma \leq 1\\
			\end{cases}
	\end{equation}
	where the measure $\mu$,
	which is absolutely continuous, is defined
	by its associated density
	$d\mu\from\left[\lambda_-, \lambda_+\right]\to\R$:
	\begin{equation}\equationlabel{def-mp-density}
		d\mu(\lambda) = \frac{1}{2\pi}%
		\frac{\sqrt{\left(\lambda_+ - \lambda\right)\left(\lambda-\lambda_-\right)}}%
		{\gamma\lambda} d\lambda
	\end{equation}
	where $\lambda_{\pm} = {\left(1 \pm \sqrt{\gamma}\right)}^2$.
\end{definition}

\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=\textwidth]{img/chapter1/wishart-rm.pdf}
	\end{center}
	\caption{\textbf{The Mar{\c c}enko-Pastur Distribution}.}%
	{\textbf{A}: An example $100\times100$ Wishart random matrix with rank $k=10$
		drawn from the ensemble in~\defref{wishart}.
		More negative values in black, more positive values in white.
	\textbf{B}: The expected spectral density of the matrix in \emph{A} (gold)
		outside of the bulk at $0$,
		which is given by~\equationref{def-mp-density},
		with parameter $\gamma=100/10=10$.
		and the histogram of observed eigenvalues (blue). Note the close match.
	\textbf{C}: The expected cumulative spectral distribution
		of the matrix in \emph{A} (gold),
		which is the value of $\nu\left(\left[0, \lambda\right]\right)$
		(defined in~\equationref{def-mp-dist})
		and the cumulative fraction of observed eigenvalues (blue).}
	\figurelabel{wishart-rm}
\end{figure}

But this matrix ensemble is on its own inadequate
for modeling neural network loss Hessians,
since its members are always positive semi-definite,
as the support of the density $d\mu$
in~\equationref{def-mp-density} is non-negative.
A random function whose Hessians are members of this ensemble
will always be convex.

The authors of~\cite{pennington2017}
proposed a parameterized mixture of Wigner and Wishart random matrices
as a better qualitative model of neural network loss Hessians.
The random Hessians are a sum of random matrices:
a Wishart part that contributes singularity
and large positive eigenvalues
and a Wigner part that contributes
moderately-sized positive and negative eigenvalues.
Just as the Fourier transform allows us to calculate
the distribution of a sum of independent random variables
from their separate distributions,
the Stieltjes and $\cR$ transforms combine
to allow the calculation of the expected spectrum of a sum of
freely independent random matrices from knowledge of the separate expected spectra.
In this model, the relative weight of the Wishart part
increases as the loss decreases,
so that at low loss the Hessian is almost surely positive semi-definite,
while it is almost surely indefinite at higher values of the loss.
The details of this derivation are in~\cite{pennington2017},
along with a more sophisticated model
for the loss of a single hidden-layer ReLU network.

Even without any additions,
this model is able to better match the qualitative features of
neural network loss Hessians
(singularity, large positive eigenvalues)
while retaining the NBLM property.
Furthermore,~\cite{pennington2017}
includes numerical experiments indicating
some match between the model and the loss and index
of critical points in a moderately-sized neural network
(though we will see, in~\chapterref{three},
that this evidence is somewhat weak).

\subsection{Criticism of the No-Bad-Local-Minima Theory}\sectionlabel{critic}

While the NBLM theory agrees qualitatively and in some cases quantitatively
with observations of neural network loss functions,
it is not without its flaws.

First, the approach based on critical points
and smooth Gaussian random fields fails
in the case of non-smooth activations or cost functions.
The popular $\ReLU$ activation function,
$\ReLU\left(x\right) = \max(x, 0)$,
is non-smooth because it is not differentiable at $0$,
as is the \emph{hinge} cost function for binary classification,
$c(y, \hat{y}) = \max(1-y\cdot\hat{y}, 0)$,
where the two class labels are $\pm 1$.
Non-smooth functions may have local minima
without having any points where the gradient is $0$,
as in the Euclidean norm $\norm{v}$,
which specializes to the absolute value in the one-dimensional case.

Because neural networks rely on gradients for optimization,
it is typical for activations and costs
to be differentiable almost everywhere,
so it might seem that this is a mere technicality.
However, it can be shown that for networks
with that activation and cost,
all local minima are either non-differentiable or constant%
~\cite{laurent2017}.
An analysis based on differentiable minima would only
consider the constant minima,
which would give an overly-optimistic picture
of the loss for many non-smooth losses.

Second,
it can be shown that the strict form of the NBLM theory is false.
For networks with ReLU neurons, the theory is clearly false:
if the parameters are such that all of the inputs to the
ReLU function are $0$ in one layer,
e.g.~if the biases are large and negative,
then the loss is locally constant
but may have arbitrarily high value.
Recent work has further shown that,
if biases are intialized too small,
ReLU networks converge to bad local minima%
~\cite{holzmller2020}.

For some time, it was believed that in the smooth case,
despite negative results for small networks,
e.g.~\cite{auer1996},
a sufficiently-large network had
no bad local minima,
thanks to a proof in~\cite{yu1995}.
However, this proof was incorrect,
as shown in~\cite{li2018},
and only implies the non-existence of
\emph{strict} local minima,
where the Hessian is positive definite.
Further work demonstrated that non-strict bad local minima
are generically present for a wide variety of activation functions,
not just non-smooth activations,
for arbitrarily large widths%
~\cite{ding2019}.

This suggests that the rosy picture painted by the numerical results
in~\cite{dauphin2014} and~\cite{pennington2017}
was somewhat mis-leading.
It remains possible, however,
that with the right intialization,
these bad local minima
are avoided,
and so some form of the NBLM theory holds,
perhaps for the loss restricted to a region in the vicinity
of the initial point.

\subsection{Alternatives to the No-Bad-Local-Minima Theory}\sectionlabel{alternative}

The alternative method to
demonstrate the convergence of gradient-based methods
on neural network losses is to prove it directly,
rather than relying on generic optimization proofs
by demonstrating that the loss function falls in some
privileged class.
There are two closely-related approaches
that have born fruit in recent years:
an asymptotic approach, based on the Neural Tangent Kernel,
and a direct approach, based on over-parameterization.

\subsubsection{Over-Parameterization}

The random matrix theory approach outlined in%
~\sectionref{grf} and~\sectionref{wishart}
was motivated by the observation that larger networks
were easier to train.
To some, this suggested that there were special properties
of generic (random, from the right ensemble) high-dimensional functions.

Alternatively, it could be the case that the optimizability
of large neural networks follows directly from their size.
Both~\cite{allenzhu2018} and~\cite{du2018}
prove that gradient descent applied to a neural network loss
converges to within $\epsilon$ of a global minimum
from a random initialization,
provided that the number of
neurons in each layer is polynomial in a problem-dependent set of parameters.
Furthermore, this occurs
within a polynomial number of steps in terms of
a similar set of parameters.
In both cases, the convergence rate in terms of $\epsilon$
is the same as in traditional proofs for gradient methods
(i.e.~linear,~\cite{boyd2004}).
The result in~\cite{du2018} has an exponential dependence on depth for networks
without residual connections,
which is absent in~\cite{allenzhu2018}.

While these are useful proofs of principle,
polynomial dependence is still too great to be of practical use
if the degree of the polynomial is high,
as is noted in~\cite{allenzhu2018}.
For example, one of the polynomial terms for the width in~\cite{du2018},
which is applied to the number of samples, has degree $4$.
For the MNIST dataset~\cite{lecun2010},
which has 6e4 examples,
this implies a minimum width of more than 1e19 neurons,
up to linear factors.
The same parameter in~\cite{allenzhu2018} has degree $30$.
Another term in~\cite{du2018}, $\lambda_{\min}$,
can be exponentially small in terms of other natural parameters.
The regime in which these claims operate
is clearly outside the realm of practically implementable networks.

\subsubsection{The Neural Tangent Kernel}

With widths so large,
they might as well be infinite.
In the infinite-width limit,
neural networks with random weights become
Gaussian random fields as in~\sectionref{grf},
thanks to a form of Central Limit Theorem.
Though the correspondence between shallow Bayesian neural networks
and Gaussian processes dates back to the 1990s%
~\cite{neal1996},
it was only recently that this correspondence was extended
to an extremely broad class of neural networks,
dubbed \emph{Tensor Programs}%
~\cite{yang2019}.
This view enabled the automated computation of posteriors
from these networks~\cite{novak2019},
akin to the automated computation of gradients
provided by automatic differentiation packages.
With the right tricks~\cite{li2019},
these posteriors can be competitive
with more traditional networks.

Importantly for the optimization perspective,
this approach can be used to analyze
the convergence of non-Bayesian neural networks.
Indeed, in this view, neural networks trained by
stochastic gradient descent from random initialization
are actually undergoing \emph{kernel gradient descent},
which descends a convex function~\cite{jacot2018},
even when the loss is a non-convex function of the parameters.

While this approach is exciting
and provides a fundamentally different way
to think about and train neural networks,
it is asymptotic.
Whether the claims apply to finite-sized networks was at first unclear.
The authors of~\cite{arora2019}
demonstrated equivalence between certain finite-width networks
and their limit with high probability.
However, the minimum layer width was again polynomial in certain problem parameters,
and again the degrees of the polynomials were high ($4$+).
It remains to be seen whether tighter bounds can be proven.

\section{Conclusion}

Neural networks are, in many cases,
easily trained to approximate global minima
from random initializations.
This suggests that their loss functions may have
a no-bad-local-minima property:
all local minima are nearly global minima.
This NBLM property is shared by a large class of random functions,
the Gaussian random fields~\cite{bray2007,dauphin2014}.
Though the typical Wigner and Wishart ensembles of random matrices
are not sufficient alone to model neural network loss functions,
even qualitatively,
an ensemble derived by mixing them is~\cite{pennington2017}.

However, analytical results~\cite{ding2019}
indicate that this picture is incorrect
and that, analytically speaking,
neural network losses have bad local minima,
even if they aren't found by typical training procedures.
Alternative approaches based on
overparameterization~\cite{allenzhu2018}
and its infinite-width limit~\cite{jacot2018}
have suggested a different path to understanding
the trainability of neural networks,
but they require unreasonably large hidden layers.

In the absence of strong theoretical results,
it is important to obtain better empirical evidence,
building on the work of%
~\cite{dauphin2014} and~\cite{pennington2017}.
In~\chapterref{two},
we will develop algorithms for examining
the critical points of neural network loss functions.
In~\chapterref{three},
we will apply these algorithms to some example
neural network losses,
and observe something interesting:
due to numerical and analytical issues,
previously-used algorithms may not have
been characterizing the critical points of loss functions,
but instead another class of points,
gradient-flat points.

\end{document}
