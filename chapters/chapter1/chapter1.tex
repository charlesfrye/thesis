\documentclass[../../thesis.tex]{subfiles}

\begin{document}

\chapter{Neural Networks, Critical Points, and Critical Point-Finding Algorithms}\chapterlabel{one}

\section{Overview}\sectionlabel{overview}

It is typical to present only the polished final
products of scientific research,
rather than the process itself.
This is exemplified by the advice to
\enquote{write papers backwards},
from the results and conclusions to the introduction and rationale.
While this perhaps makes the research more digestible and
certainly makes it more impressive,
it hides the confusion and failure
that are the day-to-day reality of research.
In this brief overview,
I will try to tell the story of the research project
laid out in this thesis as it was experienced,
warts and all,
and in terms comprehensible to a wide audience.

Neural networks are machine learning systems
that are, as of the writing of this thesis in 2020,
to say nothing of the beginning of this research project in 2016,
widely used but poorly understood.
The original purpose of the research was to
understand how the
architecture, dataset, and training method
interact to determine which neural network training problems
are easy.
The approach was inspired by methods from chemical physics~\cite{ballard2017}
and based on an analogy between a physical system minimizing its energy
and a machine learning system maximizing its performance.
Conceptually, the goal is to characterize all of the configurations
in which the system is stable,
the \emph{critical points} or \emph{stationary points}
of the system.
The details of this problem setup are the substance
of the remainder of this chapter.

Our intent was to build on the work
of~\cite{dauphin2014} and~\cite{pennington2017},
who had reported a characterization of critical points in
some small, simple neural networks.
We hoped to increase the scale of the networks to something
closer to what is used in practice,
to try more types of neural networks,
and especially to examine the role of the dataset.

The first step in characterizing the critical points is finding them.
In general, they can't be derived or written in elementary mathematical terms,
and so need to be discovered numerically,
just as highly-performant neural networks have their parameters set
by numerical algorithms rather than by analyutical derivations.
Our early attempts to reproduce the results
in~\cite{dauphin2014} and~\cite{pennington2017}
appeared to be failures.
The metric usually used to measure how close one is
to a critical point, the squared gradient norm,
stubbornly refused to decrease.

The algorithms used to find critical points are complicated ---
both in terms of their implementation and in terms of the number of knobs
to twiddle to configure them, or \emph{hyperparameters} ---
and they behave quite differently from typical machine learning algorithms,
We had implemented these algorithms ourselves,
due to the absence, at the beginning of this research project,
of important technical tools in typical neural network software packages.
We furthermore had limited expertise and experience in this domain,
so our first thought was that we had either implemented the algorithms incorrectly
or weren't configuring them properly.

As it turned out, both of those hypotheses were correct,
but verifying them turned out to be a research project in itself.
The process of implementing, debugging, and tuning these algorithms
is explained in \chapterref{two}.
The key innovation was the introduction of the \emph{deep linear autoencoder}
as a test problem.
For this very special neural network,
the critical points actually are known mathematically,
and have been since the late 80s~\cite{baldi1989}.
With these \enquote{correct answers} in hand,
we can check the work of our algorithms.
These results were written up for the arXiV
in~\cite{frye2019}
and rejected from ICML2019.

Unfortunately, the process of debugging and tuning critical point-finding
algorithms on the deep linear autoencoder did not solve
our performance problems.
It remained the case that the squared gradient norm metric was abnormally high,
along with other signatures of bad behavior on the part of our algorithms.

However, the exercise of verifying our algorithms on the deep linear autoencoder
gave us the confidence to consider other, more fundamental causes for failure.
In reviewing the literature on the methods used to find critical points,
it became clear that a particular failure mode for these methods
was not well-appreciated.
Implicit in the literature on critical point-finding was the fact that,
whenever a certain vector (the gradient)
was mapped to 0 by a certain matrix (the Hessian),
critical point-finding would fail.
We named this condition \emph{gradient-flatness}
and, on reviewing the outputs of our critical point-finding algorithms
when applied to neural networks,
we observed it ubiquitously.
The concept of, evidence for, and consequences of gradient-flatness
in neural networks is the focus of \chapterref{three}.

The biggest take-home message of our observations for the field
is that the famous results
in~\cite{dauphin2014} and~\cite{pennington2017}
need an asterisk:
the points characterized by those papers appear to be
gradient-flat points, not critical points,
which has distinct consequences for our understanding of neural networks.

In the remainder of this chapter,
I will set up the problem of training neural networks,
describe the critical point-based perspective on it,
and lay out the major algorithms for critical point finding.

\section{Neural Networks}\sectionlabel{nnls}

Neural networks are a highly flexible class of
differentiable functions
suitable to a wide variety of
machine learning tasks~\cite{lecun2015}.
Neural networks are constructed by interleaving
parameterized linear transformations
with (optionally parameterized) non-linear transformations.
The parameters are stored in a vector, $\theta$,
and converted into a function by a network constructor $\mathrm{NN}$:

\begin{equation}
	\mathrm{NN}(\theta) \from \cX \to \cY
\end{equation}

For example,
a \emph{fully-connected network}
has as its parameters a tuple of weight matrices $W_i$
and applies a non-linear function $\sigma$
after applying each in turn:

\begin{equation}
	\mathrm{NN}\left(W_1, \dots W_{k-1}, W_k\right) = W_k \sigma \circ W_{k-1} \sigma \circ \dots \sigma \circ W_1
\end{equation}

While neural networks can be used for any machine learning task,
we will focus on the important special case of supervised learning.
In supervised learning, a collection of inputs $X$ and targets $Y$
are provided, and the goal is to find a function from $\cX$ to $\cY$
such that the value of some cost function $c$ is low, on average, when applied to
pairs of targets and their matching inputs after being passed through the network.
Optionally, certain parameters may be considered more \enquote{costly}
than others, and this is enforced by a regularizer $r$
that is added to the cost.
The result is called the \emph{loss function}
and is defined below.

\begin{equation}\equationlabel{loss-def}
	L(\theta) \defeq \frac{1}{n}\sum_{x_i, y_i \in X, Y} c(\mathrm{NN}(\theta)(x_i), y_i) + r(\theta)
\end{equation}

Note that,
because the parameters are the thing over which we have control,
we think of this as a function of the parameters $\theta$,
even though it incorporates the network architecture,
through $\mathrm{NN}$,
and the input and output data.
This notation is summarized in \tableref{loss-notation}.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{rl}
			\textbf{Name}       & \textbf{Type}                          \\
			Inputs              & $X \from \cX^n \defeq \R^{m\times n}$  \\
			Targets, Outputs    & $Y \from \cY^n \defeq \R^{p\times n}$  \\
			Parameters          & $\theta \from \Theta \iso \R^N$        \\
			Loss Function       & $L \from \Theta \to \R$                \\
			Network Constructor & $\mathrm{NN} \from \Theta \to \cY^\cX$ \\
			Cost Function       & $c \from \cY \times \cY \to \R$        \\
			Regularizer         & $r \from \Theta \to \R$
		\end{tabular}
	\end{center}
	\caption{\textbf{Definitions of Terms and Symbols}.}
	\tablelabel{loss-notation}
\end{table}

The process of \enquote{training} a neural network is the process
of selecting a value of the parameters, $\theta^\star$, that minimizes the loss:

\begin{equation}\equationlabel{minimize}
	\theta^\star \in \argmin_{\theta \in \Theta} L(\theta)
\end{equation}

Almost all methods for training neural networks
are \emph{gradient-based}.
That is, they use the gradient function,
which satisfies the following relation:

\begin{align}
	&\nabla{L}\from \Theta \to \Theta\\
	&L(\theta + \eps) = L(\theta) + \langle\grad{L}{\theta},\eps\rangle + o(\eps)\equationlabel{grad-def}
\end{align}

The gradient at a point $\theta$ is a vector that can be used
as a linear functional applied to a perturbation $\eps$ that approximates
the value of $L$ at $\theta + \eps$.
If we drop the little-$o$ term,
we obtain this linear approximation,
also known as a first-order Taylor expansion:

\begin{align}
	&\hat{L}(\theta + \eps) = L(\theta) + \langle\grad{L}{\theta},\eps\rangle
\end{align}

Minimizing $\hat{L}$ would mean selecting an infinitely-long step $\eps$
direction with negative inner product with the gradient.
But our goal is to minimize $L$, not $\hat{L}$,
and as $\eps$ grows, so does the approximation error $o(\eps)$,
and so we select a finite length vector.
We choose one that minimizes that inner product for its length,
the negative gradient,
and then typically apply a scaling factor $\eta$,
called the \emph{learning rate}.
The resulting optimization method,
defined in \algoref{gd},
is known as \emph{gradient descent}.
\\\\
\begin{algorithm}[H]
	\SetAlgoLined{}
	\textbf{Given} $\theta_0\in\Theta, \eta\in\R^+, \nabla{L}\from\Theta\to\Theta, T\in\N^+$\\
	\While{$t < T$}{$\theta_{t+1} \leftarrow \theta_t - \eta\grad{L}{\theta_t}\\t \leftarrow t+1$\\}
	\algolabel{gd}
	\caption{Gradient Descent}
\end{algorithm}
\ \\
This method,
and its stochastic and accelerated variants,
has some hope of working on smooth functions because
whenever the parameter is a minimizer,
the gradient
is $0$:

\begin{equation}
	\theta \in \argmin_{\theta \in \Theta} L(\theta) \Rightarrow \grad{L}{\theta} = 0
\end{equation}
and so, if initialized from a minimizer,
any gradient-based algorithm will stay there.

It is easy to demonstrate that for
convex\footnote{A smooth function $f$ is convex iff
$f(y) \geq f(x) + \grad{f}{x}(y - x)$.}
smooth functions,
this algorithm, for an appropriate choice of $\eta$,
will converge to a minimizer
(see, e.g.,~\cite{boyd2004}).

Unfortunately,
neural network loss surfaces are not convex,
and so the theory built up around convex optimization
(\cite{bubeck,boyd2004})
would suggest that training neural networks should be hard.
And indeed, the experience of practitioners
working on neural networks in the 80s and 90s
was that training them was difficult.
Nowadays, however,
it is recognized that training large neural networks
with gradient-based methods
is actually quite easy~\cite{sun2019}.
One key hypothesis is the
no-bad-local-minima theory.
To understand it,
we need to consider the kinds of structures
that can appear in a non-convex function,
and which are compatible with gradient-based optimization.

\section{Critical Points}\sectionlabel{cp}

\begin{align}
	\Theta_{\textrm{cp}} &\defeq \Set{\theta \in \Theta \suchthat \grad{L}{\theta} = 0}\\
	\Theta_{\textrm{cp}} &= \argmin_{\theta \in \Theta} \sgn{L}{\theta}\\
	\Theta^\eps_{\textrm{cp}} &\defeq \Set{\theta \in \Theta \suchthat \grad{L}{\theta} \leq \eps}\\
\end{align}

\begin{align}
	&\nabla^2{L}\from \Theta \to \R^{N\times N}\\
	&\grad{L}{\theta + \eps} = \grad{L}{\theta} + \hess{L}{\theta}\eps + o(\eps)
\end{align}

\section{Critical Point-Finding Algorithms}\sectionlabel{cpfa}
We'll often worry about the norm of a vector,
$\normt{\vec{x}}$.
Recall that the norm, which is a map
$\normt{\cdot} \from \R^n \to \R$,
is defined via
\[
	\snormt{\vec{x}} \defeq \trspvec{x}\vec{x}
\]
When unambiguous, we will simply write $\norm{\vec{x}}$
and $\snorm{\vec{x}}$.

The linear subspace a matrix $M$ maps to $\vec{0}$
is denoted by $\ker{M}$,
and its orthogonal complement by $\co{\ker{M}}$.

This will be important whenever
$\grad{f}{\theta} \in \ker\hess{f}{\theta}$,
and
$\sgn{f}{\theta} > 0$.

\onlyinsubfile{\printbibliography}

\end{document}
