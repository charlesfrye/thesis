\documentclass[../../thesis.tex]{subfiles}

\begin{document}

\chapter{Critical Points and The No-Bad-Local-Minima Theory of Neural Network Losses}\chapterlabel{one}
% \onlyinsubfile{\addcontentsline{toc}{chapter}{List of Algorithms}}
\onlyinsubfile{\begin{KeepFromToc}
		\tableofcontents
		\clearpage
		\listoffigures
		\listoftables
	\end{KeepFromToc}}
\onlyinsubfile{\clearpage}
\section{Overview}\sectionlabel{overview}

It is typical to present only the polished final
products of scientific research,
rather than the process itself.
This is exemplified by the advice to
\enquote{write papers backwards},
from the results and conclusions to the introduction and rationale.
While this perhaps makes the research more digestible and
certainly makes it more impressive,
it hides the confusion and failure
that are the day-to-day reality of research.
In this brief overview,
I will try to tell the story of the research project
laid out in this thesis as it was experienced,
warts and all,
and in terms comprehensible to a wide audience.

Neural networks are machine learning systems
that are, as of the writing of this thesis in 2020,
to say nothing of the beginning of this research project in 2016,
widely used but poorly understood.
The original purpose of the research was to
understand how the
architecture, dataset, and training method
interact to determine which neural network training problems
are easy.
The approach was inspired by methods from chemical physics~\cite{ballard2017}
and based on an analogy between a physical system minimizing its energy
and a machine learning system maximizing its performance.
Conceptually, the goal is to characterize all of the configurations
in which the system is stable,
the \emph{critical points} or \emph{stationary points}
of the system.
The details of this problem setup are the substance
of the remainder of this chapter.

Our intent was to build on the work
of~\cite{dauphin2014} and~\cite{pennington2017},
who had reported a characterization of critical points in
some small, simple neural networks.
We hoped to increase the scale of the networks
closer to what is used in practice,
to try more types of neural networks,
and especially to examine the role of the dataset.

The first step in characterizing the critical points is finding them.
In general, they can't be derived or written in elementary mathematical terms,
and so need to be discovered numerically,
just as highly-performant neural networks have their parameters set
by numerical algorithms rather than by analytical derivations.
Our early attempts to reproduce the results
in~\cite{dauphin2014} and~\cite{pennington2017}
appeared to be failures.
The metric usually used to measure how close one is
to a critical point, the squared gradient norm,
stubbornly refused to decrease.

The algorithms used to find critical points are complicated ---
both in terms of their implementation and in terms of the number of knobs
to twiddle to configure them, or \emph{hyperparameters} ---
and they behave quite differently from typical machine learning algorithms.
We had implemented these algorithms ourselves,
due to the absence, at the beginning of this research project,
of important technical tools in typical neural network software packages.
We furthermore had limited expertise and experience in this domain,
so our first thought was that we had either implemented the algorithms incorrectly
or weren't configuring them properly.

As it turned out, both of those hypotheses were correct,
but verifying them turned out to be a research project in itself.
The process of implementing, debugging, and tuning these algorithms
is explained in \chapterref{two}.
The key innovation was the introduction of the \emph{deep linear autoencoder}
as a test problem.
For this very special neural network,
the critical points actually are known mathematically,
and have been since the late 80s~\cite{baldi1989}.
With these \enquote{correct answers} in hand,
we can check the work of our algorithms.
These results were written up for the arXiV
in~\cite{frye2019}
and rejected from ICML2019.

Unfortunately, the process of debugging and tuning critical point-finding
algorithms on the deep linear autoencoder did not solve
our performance problems.
It remained the case that the squared gradient norm metric was abnormally high,
along with other signatures of bad behavior on the part of our algorithms.

However, the exercise of verifying our algorithms on the deep linear autoencoder
gave us the confidence to consider other, more fundamental causes for failure.
In reviewing the literature on the methods used to find critical points,
it became clear that a particular failure mode for these methods
was not well-appreciated.
Implicit in the literature on critical point-finding was the fact that,
whenever a certain vector (the gradient)
was mapped to 0 by a certain matrix (the Hessian),
critical point-finding would fail.
We named this condition \emph{gradient-flatness}
and, on reviewing the outputs of our critical point-finding algorithms
when applied to neural networks,
we observed it ubiquitously.
The concept of, evidence for, and consequences of gradient-flatness
in neural networks is the focus of \chapterref{three}.

The biggest take-home message of our observations for the field
is that the famous results
in~\cite{dauphin2014} and~\cite{pennington2017}
need an asterisk:
the points characterized by those papers appear to be
gradient-flat points, not critical points,
which has distinct consequences for our understanding of neural networks.

In the remainder of this chapter,
I will set up the problem of training neural networks,
describe the critical point-based perspective on it,
and lay out the major algorithms for critical point finding.

\section{Neural Networks Losses}\sectionlabel{nnls}

Neural networks are a highly flexible class of
differentiable functions
suitable to a wide variety of
machine learning tasks~\cite{lecun2015}.
\emph{Neural networks} are constructed by interleaving
parameterized linear transformations
with (optionally parameterized) non-linear transformations.
In order to be able to apply calculus
to our networks, we will assume that
the parameters are stored in a vector, $\theta$,
and converted into a function by a network constructor $\mathrm{NN}$:
\begin{equation}
	\mathrm{NN}(\theta) \from \cX \to \cY
\end{equation}
For example,
a \emph{fully-connected network}
has as its parameters a tuple of weight matrices $W_i$
and applies a non-linear function $\sigma$
after applying each in turn:

\begin{equation}
	\mathrm{NN}\left(W_1, \dots W_{k-1}, W_k\right) = W_k \sigma \circ W_{k-1} \sigma \circ \dots \sigma \circ W_1
\end{equation}

While neural networks can be used for any machine learning task,
we will focus on the important special case of supervised learning.
In supervised learning, a collection of inputs $X$ and targets $Y$
are provided, and the goal is to find a function from $\cX$ to $\cY$
such that the value of some cost function $c$ is low, on average, when applied to
pairs of targets and their matching inputs after being passed through the network.
Optionally, certain parameters may be considered more \enquote{costly}
than others, and this is enforced by a regularizer $r$
that is added to the cost.
The result is called the \emph{loss function}
and is defined below.
\begin{equation}\equationlabel{def-loss}
	L(\theta) \defeq \frac{1}{n}\sum_{x_i, y_i \in X, Y} c(\mathrm{NN}(\theta)(x_i), y_i) + r(\theta)
\end{equation}
Note that,
because the parameters are the thing over which we have control,
we think of this as a function of the parameters $\theta$,
even though it might more properly be considered
a function of the network architecture and the data as well.
This notation and setup is summarized in \tableref{loss-notation}.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{rl}
			\textbf{Name}       & \textbf{Type}                          \\
			Inputs              & $X \from \cX^n \defeq \R^{m\times n}$  \\
			Targets, Outputs    & $Y \from \cY^n \defeq \R^{p\times n}$  \\
			Parameters          & $\theta \from \Theta \iso \R^N$        \\
			Loss Function       & $L \from \Theta \to \R$                \\
			Network Constructor & $\mathrm{NN} \from \Theta \to \cY^\cX$ \\
			Cost Function       & $c \from \cY \times \cY \to \R$        \\
			Regularizer         & $r \from \Theta \to \R$
		\end{tabular}
	\end{center}
	\caption{\textbf{Definitions of Terms and Symbols for Neural Network Loss Functions}.}
	\tablelabel{loss-notation}
\end{table}

The process of \enquote{training} a neural network is the process
of selecting a value of the parameters, $\theta^\star$, that minimizes the loss:
\begin{equation}\equationlabel{minimize}
	\theta^\star \in \argmin_{\theta \in \Theta} L(\theta) = \Theta^\star
\end{equation}
That is, we treat the process of inferring the best parameters for our network,
or programming our machine-learned algorithm,
as an optimization problem.
An element of an $\argmin$ is known as a \emph{global minimum}.
Finding global minima is generically a hard problem in the strictest sense,
because almost any problem in mathematics can be formulated as an
optimization problem\footnote{This is the \emph{variational approach},
used in, e.g.~the Courant-Fischer-Weyl characterization of eigenvalues,
the variational approach to inference~\cite{wainwright2007},
the Lagrangian approach to mechanics,
and even the universal construction approach in category theory~\cite{milewski2014}.
}.
One might think of machine learning in general as
a variational approach to programming computers.
Whether optimization works,
depends on whether the optimization problem
has a fast solution algorithm.

Almost all methods for optimizing neural networks
are \emph{gradient-based}.
That is, they use the gradient function,
which satisfies the following relation:

\begin{align}
	&\nabla{L}\from \Theta \to \Theta\\
	&L(\theta + \eps) = L(\theta) + \langle\grad{L}{\theta},\eps\rangle + o(\eps)\equationlabel{grad-def}
\end{align}

The gradient at a point $\theta$ is a vector that can be used
as a linear functional applied to a perturbation $\eps$ that approximates
the value of $L$ at $\theta + \eps$.
If we drop the little-$o$ term,
we obtain this linear approximation,
also known as a first-order Taylor expansion:

\begin{align}
	&\hat{L}(\theta + \eps) = L(\theta) + \langle\grad{L}{\theta},\eps\rangle
\end{align}

Minimizing $\hat{L}$ would mean selecting an infinitely-long step $\eps$
in a direction with negative inner product with the gradient.
But our goal is to minimize $L$, not $\hat{L}$,
and as $\eps$ grows, so does the approximation error $o(\eps)$,
and so we select a finite length vector.
We choose the one that makes that inner product most negative,
for its length,
the negative gradient,
and then typically apply a scaling factor $\eta$,
called the \emph{learning rate}.
The resulting optimization method,
defined in \algoref{gd},
is known as \emph{gradient descent}.
\\\\
\begin{algorithm}[H]
	\SetAlgoLined{}
	\textbf{Given} $\theta_0\in\Theta, \eta\in\R^+, \nabla{L}\from\Theta\to\Theta, T\in\N^+$\\
	\While{$t < T$}{$\theta_{t+1} \leftarrow \theta_t - \eta\grad{L}{\theta_t}\\t \leftarrow t+1$\\}
	\algolabel{gd}
	\caption{Gradient Descent}
\end{algorithm}
\ \\
This method,
and its stochastic and accelerated variants,
has some hope of working on smooth functions because
whenever the parameter is a minimizer,
the gradient
is $0$:
\begin{equation}\equationlabel{minimizers-critical}
	\theta \in \argmin_{\theta \in \Theta} L(\theta) \Rightarrow \grad{L}{\theta} = 0
\end{equation}
and so, if initialized from a minimizer,
any gradient-based algorithm will stay there.
Below, we will demonstrate that for
convex smooth functions,
this algorithm, for an appropriate choice of $\eta$,
will converge to a minimizer.

Unfortunately,
neural network loss surfaces are not convex,
and so the theory built up around convex optimization
(\cite{bubeck2015,boyd2004})
would suggest that training neural networks should be hard.
And indeed, the experience of practitioners
working on neural networks in the 80s and 90s
was that training them was difficult.
Nowadays, however,
it is recognized that training large neural networks
with gradient-based methods
is actually quite easy~\cite{sun2019}.
One key hypothesis as to why is the
no-bad-local-minima theory.
To understand it,
we need to consider the kinds of structures
that can appear in a non-convex function,
and which of them are compatible with gradient-based optimization.

\section{Critical Points}\sectionlabel{cp}

One approach to analyzing the behavior of optimization algorithms
is to split the task of determining convergence
into two steps:
first, identify the points which are \emph{stationary},
at which the update is $0$,
and then determine which of those points are actual targets of convergence.
We call the stationary points of the loss its \emph{critical points}.
The set of all critical points of a function $L$
on a domain $\Theta$ is defined as
\begin{equation}\equationlabel{def-thetacp}
	\Theta^L_{\cp} \defeq \Set{\theta \in \Theta \suchthat \grad{L}{\theta} = 0}
\end{equation}
When unambiguous, the super-script $L$ will be omitted.
In a na{\"\i}ve first pass,
it would seem that all $\theta \in \Theta_{\cp}$
are also targets of convergence.
If we initialize \algoref{gd} to one of these points
($\theta_{0} \in \Theta_{\cp}$),
then for $\theta_t = \theta_0$ for all $t$.

This picture is mis-leading for practical purposes,
but even this coarse approach is sufficient to guarantee
that gradient-based methods converge on smooth convex functions.
A smooth function $f$ is convex iff
$f(y) \geq f(x) + \langle\grad{f}{x}(y - x)\rangle$
for all $x$ and $y$ in its domain $\Omega$.
The loss functions for linear regression and
logistic regression are convex,
including when convex regularization is applied,
e.g.~LASSO or ridge~\cite{hastie2016}.

Applying this definition of convexity at a point $x_{\cp}$
among the stationary points of $f$, $\Omega^f_{\cp}$,
we have that
\begin{align}
	f(y) &\geq f\left(x_{\cp}\right) + \langle\grad{f}{x_{\cp}}, y - x_{\cp} \rangle\\
	f(y) &\geq f\left(x_{\cp}\right) + 0 = f\left(x_{\cp}\right)
\end{align}
for \emph{any} point $y$.
This is part of the power of convexity:
local information (encoded in the gradient)
gives global information (in the form of a global lower bound).
With it, we can improve~\ref{eq:minimizers-critical}
from a one-way implication to a biconditional,
from \enquote{if-then} to
\enquote{if and only if}:

\begin{equation}\equationlabel{convexity-critical}
	L \text{\ smooth, convex\ } \Rightarrow\\
	\theta \in \argmin_{\theta \in \Theta} L(\theta) \Leftrightarrow \grad{L}{\theta} = 0 \Leftrightarrow \theta \in \Theta_{\cp}
\end{equation}

Another way to characterize smooth convex functions is through their Hessian function.
The Hessian function returns matrices that satisfy the relation below:

\begin{align}
	&\nabla^2{L}\from \Theta \to \R^{N\times N}\\
	&\grad{L}{\theta + \eps} = \grad{L}{\theta} + \hess{L}{\theta}\eps + o(\eps)\equationlabel{def-hess}
\end{align}

It is the \enquote{gradient of the gradient},
in that it returns a linear function (a matrix)
that approximates the gradient function,
which itself returns a linear functional (a vector)
that approximates the original function.
Combined, the gradient function and the Hessian function produce a quadratic approximation
of the original function $L$:

\begin{equation}
	L\left(\theta + \eps\right) =
	L(\theta)
	+ \langle\grad{L}{\theta},\eps\rangle
	+ \eps^\top\hess{L}{\theta}\eps + o(\snormt{\eps})\equationlabel{taylor-two}
\end{equation}

Note that the Hessian appears in~\equationref{taylor-two}
as a quadratic form:
a symmetric matrix pre- and post-multiplied with the same vector.
Quadratic forms are classified,
up to a change of basis, by the eigenvalue spectrum
of the underlying matrix:
the number of positive, negative, and 0 eigenvalues.

Smooth convex functions are precisely those functions whose
Hessian matrix has no negative eigenvalues at any
point.
Such a matrix $M$ is called \emph{positive semi-definite},
often denoted $M \succeq 0$,
where the partial order $\succ$ is defined by
comparing smallest eigenvalues.
If $M \succ 0$, then its smallest eigenvalue is positive,
and the matrix is \emph{positive definite}.

If the Hessian $M$ at a point is positive definite
then $x^\top M x$ is always positive.
This implies that the second-order term in~\equationref{taylor-two}
is positive.
Since the second-order term dominates the higher-order terms
for sufficiently small $\eps$,
at a point $\theta^*$ where the gradient is $0$
and the Hessian is positive definite,
we have that

\begin{align}
	L\left(\theta^* + \eps\right) &=
	L(\theta^*)
	+ \langle\grad{L}{\theta^*},\eps\rangle
	+ \eps^\top\hess{L}{\theta^*}\eps + o(\snormt{\eps})\\
	&=
	L(\theta^*)
	+ 0
	+ \eps^\top\hess{L}{\theta^*}\eps + o(\snormt{\eps})\\
	&\geq
	L(\theta^*)
\end{align}

Such a $\theta^*$ is a called a \emph{local minimum},
since it is the minimizer of $L$ in all its neighborhoods
under a given size.
We call the set of all such $\theta^*$
the \emph{local minima}, $\Theta_{\lm}$.
By a small extension of the above
argument\footnote{The extension demonstrates that,
for convex functions,
points with positive semi-definite Hessians
are still minimizers.
This is untrue in the non-convex case,
and checking whether a point is a minimum becomes NP-Hard at worst,
see~\cite{murty1987}.},
we come to a final characterization
of why gradient descent converges on smooth, convex
functions\footnote{In fact
the class of functions for which the implication~\ref{eq:cvx-minimizers}
holds is broader.
They are known as smooth \emph{invex} functions,
see~\cite{invex2008}.}:
all critical points are local minima
and all local minima are also \emph{global minima},
or elements of the $\argmin$:

\begin{equation}\equationlabel{cvx-minimizers}
	f \text{\ smooth, convex\ }
	\Rightarrow \Omega_{\lm} = \argmin_{\Omega} f = \Omega_{\cp}
\end{equation}

From this, we can deduce
that any algorithm that converges to a generic critical point will converge,
on smooth convex functions, to a minimizer.

This condition is sufficient, but not necessary,
for gradient descent to converge to a local minimizer.
The sticking point is when there are
critical points which are not local minimizers:
$\Theta_{\cp} \supset \Theta_{\lm}$.
Does gradient descent converge to non-minimizing critical points,
or only to local minimizers?

As above, we can classify critical points according to the
eigenvalue spectrum of their associated Hessian.
The fraction of negative eigenvalues is known as the
\emph{index} of the critical point.
Points with non-zero index are \emph{strict saddle points}.
If all eigenvalues are non-zero,
then critical points with index $0$ are local minima
and critical points with index $1$ are local maxima.
If some eigenvalues are zero and the index is $0$,
then the critical point may be a local minimum
or may be a (non-strict) saddle point,
but higher-order derivatives are needed to disambiguate.

If all saddle points are strict saddle points,
a condition known as the \emph{strict saddle property},
then gradient descent converges to a local minimizer~\cite{lee2016}.
Furthermore, convergence for a stochastic
version of~\algoref{gd} is fast~\cite{jin2018a}
and can be accelerated via momentum~\cite{jin2018b}.

If all local minimizers are global minimizers
and the strict saddle property holds,
then the preceding argument implies that
gradient descent
is sufficient to obtain global minima.
This covers many cases:
most critically, tensor decomposition,
which covers a number of latent variable models, including
PCA, ICA~\cite{bell1997,comon2009},
sparse coding~\cite{olshausen1996}
and other forms of dictionary learning,
and Hidden Markov models~\cite{anandkumar2012}.
See~\cite{ge2015}.
But does this cover neural network losses?

\section{The No-Bad-Local-Minima Theory}\sectionlabel{nblm}

When all local minima of a function
are approximately global minima,
we will say that the function has the
\emph{no-bad-local-minima property},
or the NBLM property.
We will refer to the hypothesis that
the loss functions of large neural networks
satisfy the NBLM property as the
\emph{no-bad-local-minima theory}.
In \sectionref{grf},
we will cover the first model of neural network loss functions
meant to specifically capture the NBLM property,
the Gaussian Random Field model of~\cite{dauphin2014}.
Then, in \sectionref{wishart},
we will explain how this model was improved
by incorporating observations about the spectrum
of the Hessians of neural networks~\cite{pennington2017}.
Finally, we will close, in \sectionref{counter},
with theoretical results for actual neural networks,
which are mostly negative.

\subsection{The Gaussian Random Field Model of Neural Network Losses}\sectionlabel{sec:grf}
\subsection{Improving the Gaussian Random Field Model with Wishart Matrices}\sectionlabel{sec:wishart}
\subsection{Counterexamples to the No-Bad-Local-Minima Theory and Alternatives}\sectionlabel{sec:counter}

\onlyinsubfile{\printbibliography}

\end{document}
