\documentclass[../../thesis.tex]{subfiles}

% chktex-file 18
\begin{document}

\chapter{Applying Critical Point-Finding Algorithms to Neural Network Loss Functions
Reveals Gradient-Flat Regions}\chapterlabel{three}

\onlyinsubfile{\begin{KeepFromToc}
		\tableofcontents
		\listoffigures
	\end{KeepFromToc}}
\onlyinsubfile{\clearpage}
\onlyinsubfile{\linenumbers}

\section{Chapter Summary}%
\sectionlabel{ch3-summary}

\chapterref{two} motivated and defined
a number of second-order algorithms
for finding the critical points of neural network loss functions.
As described in \chapterref{one},
information about the local curvature at these points is useful
for understanding the optimizability of neural networks.
For example, the No-Bad-Local-Minima theory
(NBLM\@; see \sectionref{nblm}),
based on a random function model of neural network losses
(\sectionref{grf}, \sectionref{wishart}),
predicts that critical points with strictly positive curvature
should only occur when the value of the loss is low.
This implies that first-order methods like gradient descent
can optimize neural networks%
~\cite{jin2018a}.
Previous work%
~\cite{dauphin2014,pennington2017}
appeared to verify this theory.
More recent analytical work, however,
has indicated that the NBLM theory is false,
and there are in fact bad local minima
on neural network loss functions
(\cite{ding2019}; see \sectionref{critic}).
This disagreement with theory
motivates a closer look at the numerical evidence,
which is the substance of this chapter.

First, in \sectionref{dlae},
we present a test problem,
the deep linear autoencoder,
that is analytically tractable
while remaining sufficiently similar
to the analytically intractable problem
of interest, viz.~non-linear networks.
The analytically-derived critical points
are used to verify that the approximate critical points
recovered by the numerical algorithms are accurate.
Then, we apply the best-performing of these methods,
Newton-MR, to a non-linear network
and observe a tremendous change in its behavior:
qualitative signatures of convergence disappear
and quantitative metrics sharply decline
(\sectionref{nonlinear-failure}).

We return, then, to the analysis of second-order
critical point-finding methods and demonstrate
that a large class of spurious targets of convergence
has been over-looked:
points where the gradient lies in the kernel of the Hessian.
We call these \emph{gradient-flat points}.
They are defined in~\sectionref{gfr}.
We present evidence that these points are encountered
by critical point-finding methods applied to neural network losses in%
~\sectionref{gfr-results}.

The gradient-flat points that cause the most trouble are
bad local minima of the gradient norm.
The core result of this chapter
and this dissertation, then,
is that the second-order methods
used to find the critical points
of neural network loss functions
in an attempt to prove the no-bad-local-minima theory
suffer from a bad-local-minima problem of their own.

\section{The Deep Linear Autoencoder Provides a Useful Test Problem}%
\sectionlabel{dlae}

This section introduces the deep linear autoencoder
as a test problem for critical point-finding methods
for neural network loss functions.
First,
the need for test problems is explained
(\sectionref{test}).
Then,
the deep linear autoencoder is presented
and the critical points derived
\sectionref{def-dlae}.
Finally, the performance of
Newton-MR (\algoref{nmr}),
damped Newton (\algoref{damped-newton}),
and gradient norm minimization by
gradient descent with back-tracking line search
(BTLS-GNM)
are compared
(\sectionref{dlae-results}).

Additional results regarding these critical point-finding methods
and the deep linear autoencoder are
published in~\cite{frye2019}.

\subsection{Test Problems are Necessary}%
\sectionlabel{test}

In the case of optimization for the solution of engineering problems,
the question of convergence to the neighborhood of a true minimizer
is often an academic one.
The goal of the optimization procedure is to find a point
at which the loss is sufficiently low to
e.g.~allow the airplane to fly
or select the better ad to display to users.
The problem of finding critical points is different:
our goal is to check analytical properties,
which can in principle require arbitrary precision.

The usual solution is to prove an upper bound on the precision required
by means of inequalities.
In our case, we are interested in two quantities:
the loss $L$ and
the index $I$ (\defref{index})
at critical points $\theta_{\cp}$.
For the loss,
we might proceed by demonstrating that the function is
$K$-Lipschitz for some constant $K$.

\begin{definition}{Lipschitz Continuity}{lipschitz}
	A function $f\from \R^n \to \R$ is said to be \emph{$K$-Lipschitz} if
	\begin{equation}
		\norm{f(x + \eps) - f(x)} \leq K\norm{\eps}
	\end{equation}
	for some $K\in \R_{\geq 0}$ and for all $x,\eps \in \R^n$.
\end{definition}
\noindent For an almost everywhere differentiable function,
like $\ReLU$, $K$-Lipschitz continuity is equivalent to demonstrating
that the gradient norm is bounded by $K$.
Using this fact,
a bound on the difference between the loss
at an approximate critical point with gradient norm $\eps$
and any nearby true critical points can be derived.

However, the Lipschitz constants of typical neural network
losses are not well-controlled
\cite{gouk2018}, %chktex 2
resulting in worst-case bounds that are overly-pessimistic.
The situation is even worse for calculating the index,
which depends both on Lipschitz bounds on the operator norm
of the Hessian and on guarantees that the kernel changes only slowly.

In the absence of these guarantees,
the results of numerical algorithms must be interpreted with care
and hyperparameters must be tuned cautiously.
The behavior of algorithms during convergence should be closely monitored
and reported along with results.
And critically, it is important to test numerical algorithms
on problems for which the answers are known
before applying them to problems for which the answers are in doubt.
In addition to allowing for hyperparameters to be set to reasonable starting values
and for the signatures of convergence to be identified,
test problems provide an important opportunity to debug implementations
that is missing in cases without ground truth.
Analytical guarantees are of little value
if the algorithms in question are broken.

Furthermore, there are many critical point-finding algorithms that might be chosen.
\chapterref{two} presented two Newton methods
of sufficient robustness for practical use,
plus gradient norm minimization.
Both the damped Newton method~\cite{dauphin2014}
and gradient norm minimization~\cite{pennington2017}
have been used,
but sufficient information for comparing performance
was not published.

It is critical that the test problem be designed
to be as close as possible to the problem of interest.
The following section introduces such a test problem for
neural network loss functions.

\subsection{The Deep Linear Autoencoder has Known Critical Points}%
\sectionlabel{def-dlae}

The key difficulty for deriving analytical expressions
for the critical points of non-linear neural networks
is that the gradients of the loss are non-linear
in an arbitrary fashion.
This turns the problem of finding critical points
into the problem of finding solutions to generic non-linear equations.

Neural networks without non-linearities,
also known as \emph{linear networks},
avoid this problem.
The network is constructed by raveling the entries of $\theta$
into a $D$-element sequence of matrices $W_i(\theta)$
which multiply an input $x$ in turn.
We call these matrices the \emph{weight matrices}
of the network and say that the network has $D$ layers,
each with \emph{layer size} equal to the number of rows
in the matrix $W_i$.
The layers $W_1$ through $W_{D-1}$ are called \emph{hidden layers}.
\begin{align}
	\mathrm{NN}_{\mathrm{DLN}}(\theta)(x)
	&= W_D(\theta) W_{D-1}(\theta) \dots W_{2}(\theta) W_{1}(\theta) x\\
	&\defeq{W(\theta) x}
\end{align}
\noindent As the second line indicates,
this is equivalent to applying
a single matrix $W(\theta)$.
Therefore the function computed by the network as a whole is linear.
Any non-linearity in the gradient function
of the loss of such a network is introduced
only by the cost function and regularizer.
For simplicity, we consider only the unregularized case.

The loss of this network can be construed as a function
of any of three quantities.
As before, it is a function of the vector of parameters $\theta$,
which we continute to denote $L$.
It is also a function of the separate weights of each layer,
the matrices $W_i$,
which we denote $l$
and refer to as the \emph{layerwise} loss.
These functions also exist for non-linear networks.
Finally, in the linear case
the loss can also be written a function
of the \enquote{collapsed} matrix $W$,
which we denote $\cL$
and refer to as the \emph{collapsed} loss.
We further drop the explicit dependence of $W$
and the $W_i$ on $\theta$ when writing the loss in this way.

With this setup,
we can write the gradients of a linear network
of arbitrary depth
with arbitrary cost in simple form.
\begin{theorem}{Gradients of a Deep Linear Network}{dln-grad}
	\emph{%
	Let $l$ be the layerwise loss
	and $\cL$ the collapsed loss
	of a deep linear network
	with $D$ layers.
	The gradient function of the layerwise loss
	with respect to layer $k$,
	denoted $\nabla_{W_k}(l)$, is
	\begin{equation}
		\grad{_{W_k} l}{W_1, \dots, W_D}
		= W_{k+1:}^\top \grad{\cL}{W} W_{:k}^\top\equationlabel{def-dln-grad}
	\end{equation}
	}
\end{theorem}
\noindent where the notation $W_{i:j}$,
inspired by the slicing syntax of Python and other languages,
stands for the products of matrices $i$ to $j-1$,
with an empty entry on the left standing for $1$
and an empty entry on the right standing for $D+1$.

\noindent \emph{Proof of \thmref{dln-grad}}:\\
This proof follows closely that in%
~\cite{laurent2018}.
The gradient is defined in terms of the value
of the function at an input perturbed by $\eps$
\begin{equation}
	l(W_1, \dots, W_{k-1}, W_k + \eps, W_{k+1}, \dots W_D)
\end{equation}
\noindent where here $\eps$ is a matrix of the same shape as $W_k$.
We proceed by converting to the collasped loss $\cL$
and multiplying through the matrix product.
\begin{align}
	l(W_1, \dots, W_{k-1}, W_k + \eps, W_{k+1}, \dots W_D)
	&= \cL\left(W_1 W_2 \dots W_{k-1} (W_k + \eps) W_{k+1} \dots W_D\right)\\
	&= \cL\left(W + W_{k+1:} \eps W_{:k}\right)\\
	&= \grad{\cL}{W} + \langle \grad{\cL}{W}, W_{k+1:} \eps W_{:k}\rangle + o(\eps)
	\equationlabel{taylor-collapsed-loss}
\end{align}
\noindent where the last line follows by pattern-matching
to the definition of the gradient function,
\defref{gradient}.
Note that the inner product here is an inner product of matrices.
It is the Frobenius inner product
\begin{equation}
	\langle A, B \rangle = \tr\left(A^\top B\right)
\end{equation}
\noindent which is defined by unraveling the matrices into vectors
and applying the Euclidean inner product of vectors,
i.e.~as a pullback of that inner product via a raveling isomorphism.
The trace $\tr$ is invariant to cyclic permutations,
and so we can re-organize the middle term of
\equationref{taylor-collapsed-loss}
\begin{align}
	\langle \grad{\cL}{W}, W_{k+1:} \eps W_{:k}\rangle
	&= \tr\left(\grad{\cL}{W}^\top W_{k+1:} \eps W_{:k} \right)\\
	&= \tr\left(W_{:k} \grad{\cL}{W}^\top W_{k+1:} \eps \right)\\
	&= \langle W_{k+1:}^\top \grad{\cL}{W}W_{:k}^\top, \eps\rangle
\end{align}
\noindent which implies, by pattern-matching
to the definition of the gradient function again,
\begin{equation}
	\grad{_{W_k} l}{W_1, \dots, W_D}
	= W_{k+1:}^\top \grad{\cL}{W} W_{:k}^\top
\end{equation}
\QED\\

This suggests that if we wish to be able to
come up with an analytic expression for the critical points,
which make this gradient function $0$,
we choose a collapsed loss function $\cL$
that has a simple form that allows us to set
\equationref{def-dln-grad} equal to $0$ and solve.

We therefore make several simplifying choices
in constructing our test problem.
First, we take the inputs and targets
to be the same.
This type of network is known as an \emph{autoencoder}.
Second, we choose the squared error as the loss function.
Finally, we choose the number of layers $D$ to be $2$.
We refer to this combination as the \emph{deep linear autoencoder}.

The critical points of this network can be characterized as follows:
\begin{theorem}{Critical Points of Deep Linear Autoencoder}{dlae-cps}
	\emph{%
	Let $L$ be the loss function of a linear network such that
	\begin{equation}
		L(\theta) = \snorm{X - W_2(\theta)W_1(\theta) X}
	\end{equation}
	\noindent for some matrix $X\in\R^{k\times n}$
	such that $XX^\top$ is full rank,
	with $W_1\from \R^{2kp}\to \R^{p\times k}$
	$W_2\from \R^{2kp}\to \R^{k \times p}$.
	\\ \ \\
	Then the critical points of this network
	correspond to those $\theta_{\cp}$
	such that the matrix
	$W = W_2(\theta_\cp)W_1(\theta_\cp)$
	acts as the identity on a subspace
	spanned by some subset of the eigenvectors of $XX^\top$
	and the zero matrix otherwise.
	}
\end{theorem}

Before diving into the proof,
we first consider the interpretation and implications.
The matrix $XX^\top$ is the
sample covariance matrix of the data $X$
when the data has sample mean $0$.
The eigenvectors of this matrix are known as the
\emph{principal components}
of the data.
A single critical point can be constructed
according to the criterion in the theorem above
by choosing $m \leq p$ separate principal components
for the rows of $W_1$
(setting the others to $0$ when $m<p$)
and choosing $W_2 = W_1^\top$.
This allows the construction of
a number of distinct critical points equal to the number of ways
to choose from $0$ to $p$ elements
from a $k$ element set:
$\sum_{i=0}^p \binom{k}{i}$.
When the eigenvectors selected are the $m$
with largest eigenvalue,
this network is performing \emph{principal components analysis}.
This value corresponds to the global minimum.
It can be demonstrated that there are no non-global local minima%
~\cite{baldi1989,laurent2018},
and so this loss function has the no-bad-local-minima property.
It is a generalization of~\exampleref{1ddlae}.

An uncountable collection of additional critical points
can be constructed from the points corresponding directly
to the principal components.
Indeed, the key property in~\thmref{dlae-cps}
is defined in terms of the collapsed matrix $W$.
Applying any invertible $p\times p$ matrix $C$ after $W_1$
and its inverse $C^{-1}$ before $W_2$
leaves $W$ unchanged\footnote{%
Note that invertible matrices form a Lie group,
and therefore so do these critical points.
This situation is shared, to an extent,
in $\ReLU$ networks~\cite{freeman2016}.},
and so the resulting new point is also a critical point.
The loss and index are unchanged,
and so we can consider these collections
to be equivalence classes of critical points,
with representatives given by the choice $C=C^{-1}=I$.

\thmref{dlae-cps},
which was first proven in%
~\cite{baldi1989},
is proven in the following section
for completeness.
Readers uninterested in the technical details
are invited to skip to
\sectionref{dlae-results}.

\subsubsection{Proof of \thmref{dlae-cps}}%
\sectionlabel{dlae-proof}

The collapsed loss function of the linear network is
\begin{equation}
	\cL(W) = \snorm{X - WX}
\end{equation}
\noindent Expanding the right-hand side,
we have that
\begin{align}
	\cL(W + \eps)
	&= \tr\left(X^\top X\right) - \tr\left(2X^\top(W+\eps)X\right)
	+ \tr\left(X^\top{(W+\eps)}^\top(W+\eps)X\right)\\
	&= \cL(W) + \tr\left(2 XX^\top W \eps - 2XX^\top\eps\right) + o(\eps)
\end{align}
\noindent which, after re-arrangement, gives the gradient function
for $\cL$ by pattern-matching to \defref{gradient}:
\begin{equation}
	\grad{\cL}{W} = 2 XX^\top (W - I)
\end{equation}

To find the analytical critical points,
we plug this definition into
the expressions for the gradients with respect to
the two weight matrices $W_1$ and $W_2$
given by \thmref{dln-grad}
and solve for 0.

We start with $W_2$:
\begin{align}
	\grad{l_{W_2}}{W_1, W_2} i.e.~when &= \grad{\cL}{W} W_1^\top\\
	0 &= \grad{\cL}{W} W_1^\top\\
	0 &= 2 XX^\top\left(W_2 W_1 - I\right) W_1^\top\\
	W_1^\top &= W_2W_1 W_1^\top\equationlabel{last-line-W2-grad}
\end{align}
\noindent where the transition to \equationref{last-line-W2-grad}
used the invertibility of $XX^\top$.
This equation is satisfied
whenever $W_2 W_1$ is equivalent to the identity
matrix when restricted to the co-kernel of $W_1$
(i.e.~when $W_2W_1$ is a projection matrix onto that subspace),
which is the range of $W_1^\top$.

This isn't sufficient to completely determine
$W_1$ and $W_2$,
so we proceed to the equations
for $W_1$:
\begin{align}
	0 &= W_2^\top\grad{\cL}{W}\\
	0 &= W_2^\top 2 XX^\top\left(W_2 W_1 - I\right)\\
	W_2^\top &= W_2^\top XX^\top W_2 W_1 {\left(X X^\top\right)}^{-1}
\end{align}
\noindent When the matrix $XX^\top$
is simultaneously diagonalizable with the matrix $W_2W_1$,
the matrices commute,
which gives the equation
\begin{align}
	W_2^\top &= W_2^\top W_2 W_1
\end{align}
\noindent
This equation holds when $W_2 W_1$ is equivalent to the identity
when applied to the co-kernel of $W_2^\top$.

For $W_2 W_1$ to be simultaneously diagonalizable with $XX^\top$,
the non-zero eigenvectors of $W_2W_1$
need to be the same as some subset of the eigenvectors of $XX^\top$.
Together, these conditions imply that
at a critical point of the loss,
$W_2W_1$ acts as the identity
on a subspace spanned by some subset of the eigenvectors of $XX^\top$.

\subsection{Newton-MR Outperforms Previous Methods on this Problem}%
\sectionlabel{dlae-results}

We are now ready to set up the deep linear autoencoder
test problem and compare the performance of our
critical point-finding algorithms.

\subsubsection{Data and Network}\sectionlabel{dlae-parameters}

\thmref{dlae-cps} gives a characterization
of the set of all critical points, $\Theta_{\cp}$,
for a given choice of dataset $X\in\R^{k \times n}$ and two-layer network.
The key parameters for the dataset are its covariance matrix,
dimension $k$, and size $n$.
The key architectural hyperparameter of the network is the
size of the hidden layer
($p$, as in \thmref{dlae-cps}).

For simplicity, we choose a multivariate Gaussian distribution
for the columns of $X$,
as this distribution is entirely specified by its mean and covariance.
The connection to PCA
required that $XX^\top$ be the sample covariance matrix of $X$,
and so we enforce that $X$ has row means exactly $0$.
Note that this must be done after sampling,
as setting the distributional means to $0$ does not
result in sample row means of $0$.
We choose a diagonal covariance matrix.
To obtain maximally-spaced eigenvalues while still
controlling the condition number of $XX^\top$,
we space the diagonal values linearly between $1$ and $k$,
inclusive.
We choose $n$, the number of samples, to be $1e4$.

The parameters $k$, for the dimension of the data,
and $p$, for the dimension of the hidden layer of the neural network,
directly determine the number of critical points,
up to equivalence.
Since this count is effectively
given by the number of ways to choose subsets from a set of size $k$,
it grows nearly as fast as $2^k$.
For this reason, we choose $k$ to be small, relative to the typical
inputs to neural networks: $16$, as opposed to order $100$ or $1000$.
Selecting $p$ also to be small, specifically $4$,
gives a total number of equivalence classes of critical points equal to
$2517 = \sum_{i=0}^4 \binom{16}{i}$.
Below, we will call the set of all representatives of the
equivalence classes the \emph{analytical critical points}
of the deep linear autoencoder.

With all of these choices in place,
we can now calculate the loss and index values of the analytical
critical points for a neural network loss function.
These are plotted in \figureref{dlae-cps}.
The squared gradient norms of these points are generally not $0$,
but on the order of 1e-32.
Already, this provides a loose lower bound on the precision we need
in order to recover critical points.
There is one notable exception:
the $0$ vector, which is a critical point for this loss.
Here, the gradient is exactly $0$.
In general, the density of floating point numbers is greatest near $0$,
and so critical points near $0$ can be recovered with greater accuracy.
Note that, in a numerical setting,
the index is defined as the number of eigenvalues
below some small negative value, rather than $0$,
to account for error.
We choose -1e-5,
based on inspection of the spectra of these
analytical critical points.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{img/chapter3/dlae-cps.pdf}
	\end{center}
	\caption{\textbf{The Analytical Critical Points
	of a Deep Linear Autoencoder}.}
	\figurelabel{dlae-cps}
\end{figure}

\subsubsection{Two-Step Process for Sampling Critical Points}%
\sectionlabel{dlae-sampling}

The full set of critical points, $\Theta_{\cp}$,
is uncountably infinite in size,
as is the full set of $\eps$-critical points,
$\Theta_{\ecp}$.
Even the set of critical points up to equivalence is exponentially large.
For a problem as small as this one, it might be feasible to find all of them,
but for larger networks this won't be the case.
Instead, our methods must aim to sample the set of critical points.
If the sample is unbiased, qualitative and quantitative features
of the loss-index relationship
(\figureref{dlae-cps})
should be preserved.

To obtain initial points for our critical point-finding methods,
we first apply an optimization algorithm to the loss.
This generates a trajectory of values of $\theta$ with varying loss,
from near the value of the highest critical point
to near the value of the global minimum.
These values are then chosen as initial points
for critical point-finding algorithms.
We follow~\cite{pennington2017} in selecting these points
uniformly at random based on their loss value.
Due to the exponential rate of first-order optimization algorithms,
selecting points uniformly at random from the trajectory
would over-represent lower loss values.
Furthermore, decreasing gradient norms during training mean that
later parameter values are closer together,
and so this scheme would heavily over-sample a small region.
Both factors should reduce the variety in recovered critical points%
\footnote{These approaches were directly compared in
experiments not presented here and this intuition is borne out.
See~\cite{frye2019}.}.
To half of the initial points attained in this manner,
we add Gaussian noise of with variance 1e-2
(an SNR of approximately 2.8 dB).
In separate experiments,
noise of this magnitude
this was found to increase
the diversity in critical points recovered%
~\cite{frye2019}.

\subsubsection{Critical Point-Finding Methods Have Differential Performance}%
\sectionlabel{dlae-perf-comparison}

When applied to the problem of finding the critical points
of the deep linear autoencoder problem outlined above,
the three critical point finding methods,
Newton-MR, damped Newton, and
gradient norm minimization by gradient descent with backtracking line search
(BTLS-GNM),
have wildly different performance,
even though all three are capable of finding numerical critical points
with the same loss and index as the analytical critical points.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=\textwidth]{img/chapter3/perf-comparison.pdf}
	\end{center}
	\caption{\textbf{Newton-MR, Damped Newton, and BTLS-Gradient Norm Minimization
	can Recover Critical Points of a Deep Linear Autoencoder}.}{%
		\textbf{A-C}. Squared gradient norms at each iteration
		of the three critical point-finding algorithms.
		Runs that terminate with squared gradient norm below 1e-10
		in \successcolor{},
		otherwise in \failcolor{}.
		The y-axis is truncated at 1e-30,
		since only runs that terminate at or near $\theta=0$
		reach substantially below this value.
		Horizontal and vertical axes are shared across
		the top row and the bottom row of panels separately.
	}
	\figurelabel{perf-comparison}
\end{figure}


Results for all three algorithms appear in \figureref{perf-comparison}.
The first row of panels, A-C,
presents squared gradient norms for each algorithm
across iterations.
Results for 100 runs of Newton-MR appear in panel A,
for 100 runs of damped Newton in panel B,
and for 60 runs of BTLS-GNM in panel C.
Runs are terminated either due to hitting the maximum number of iterations
or due to hitting the minimum step size of the backtracking line search
without finding an acceptable update.
Note that BTLS-GNM is allowed 25000 iterations before termination,
while the Newton methods are only allowed 500.
This algorithm only requires a single Hessian-vector multiplication
outside of the backtracking line search phase,
which is much less than the maximum of $O(n)$
multiplications required by the inexact Newton methods.
Traces are plotted transparently to allow a rough estimation of density.
Runs that terminate with squared gradient norm above 1e-10
are in \failcolor{},
others in \successcolor{}.
Note that a large number of damped Newton and BTLS-GNM runs
terminate in a small number of iterations
and with squared gradient norm above 1e-10.

Notably, a number of runs of all three algorithms,
but especially BTLS-GNM,
obtain much lower values of the squared gradient norm.
These are runs that terminate at or near the zero vector,
which is a critical point for this problem.
Floating point numbers are much denser near the origin,
and so more accurate computations,
and so lower gradient norms, are possible.
The loss of this critical point is approximately 8.5
and the index 0.5.

Runs that terminated with squared gradient norm above 1e-10 were considered failures.
This filter value was determined by comparing loss and index values
at candidate critical points to the loss and index at analytical critical points,
as in \figureref{perf-comparison}D-F (only successful runs shown).
A value of 1e-10 was sufficient to obtain the close match there displayed,
while a value of 1e-8 or higher was not.
Note the greater number and diversity of critical points recovered by Newton-MR
(\figureref{perf-comparison}D)
compared to the damped Newton method
(\figureref{perf-comparison}E)
and especially to BTLS-GNM
(\figureref{perf-comparison}F;\@
but note that results from BTLS-GNM
are based on 60 runs, versus 100 for the Newton methods).

The relative performance of the three algorithms
is summarized in \tableref{perf-comparison}.
Newton-MR was clearly superior in terms of the fraction of runs
that ended in successes
(80\% versus 45\% for damped Newton and 35\% for BTLS-GNM)
and in terms of the elapsed walltime per success
(20 min versus 1hr 45 minutes for damped Newton and 35\% for BTLS-GNM).

\begin{table}[h]
	\begin{center}
	\begin{tabular}{rccc}
\textbf{Algorithm} & \textbf{Pseudocode} & \textbf{Fraction Successful} & \textbf{Time per Success} \\
		Newton-MR & \algoref{nmr} & 80\% $\pm$ 4 & 20 min\\
		Damped Newton & \algoref{damped-newton} & 45\% $\pm$ 5 & 1hr 45 min\\
		BTLS-GNM & \algoref{gnm}\footnotemark & 35\% $\pm$ 6 & 1hr 15 min
	\end{tabular}
	\end{center}
	\caption{\textbf{Newton-MR Outperforms Damped Newton and BTLS-GNM}.}{%
		The fraction of successful runs is given as a percentage with a
		standard error based on a Gaussian approximation
		to the sampling distribution of the percentage.
		Newton-MR and Newton-TR results based on 100 runs;
		BTLS-GNM results based on 60 runs.
		Runtimes are based on walltime on commodity hardware
		and are to be regarded as highly approximate.
	}\tablelabel{perf-comparison}
\end{table}
\footnotetext{The indicated pseudocode algorithm is for gradient norm minimization
by gradient descent.
These results are for gradient norm minimization
by gradient descent with backtracking line search,
which adds a line search step akin to \algoref{roosta-btls},
but with a convergence criterion based on the Wolfe conditions%
~\cite{wolfe1971} applied to the squared gradient norm.}

\subsubsection{Cutoffs Must be Set Strictly}%
\sectionlabel{dlae-cutoffs}

As noted in \sectionref{test},
one of the key purposes of applying our algorithms
to a test problem is to obtain estimates for the value
of $\eps$ necessary to guarantee that the loss and index values
for members of $\Theta^L_{\ecp}$
are similar to those of $\Theta^L_{\cp}$.
With such a value in hand,
we can terminate our critical point-finding algorithms early,
as soon as they produce a candidate critical point with
squared gradient norm below $\eps$.

We simulate the results of such a procedure
by applying a \enquote{cutoff}
to the traces from \figureref{perf-comparison}A.
We truncate each run at the first $T$ such that
$\sgn{L}{\theta_T} < \eps$.
As our cutoffs,
we choose the value used to filter the traces in
\figureref{perf-comparison}, 1e-10,
the value used as a filter in~\cite{pennington2017}, 1e-6,
a looser criterion of 1e-4,
and $\infty$.
The latter corresponds to no cutoff,
taking the initial points,
the iterates of the optimization algorithm,
as candidate critical points.
The results are in \figureref{cutoffs}.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{img/chapter3/cutoffs.pdf}
	\end{center}
	\caption{\textbf{Cutoffs Above 1e-10 are Insufficient to Guarantee
	Accurate Loss and Index Recovery}.}{%
	As in \figureref{perf-comparison},
	$\eps$-CPs are plotted in blue
	over analytical CPs in gray.
	For each panel,
	$\eps$-CPs are selected by taking
	the 100 runs of Newton-MR in \figureref{perf-comparison}A
	and taking the first point whose squared gradient norm is below
	the cutoff value, $\eps$, in the top-left corner.
	Horizontal and vertical axes are shared between panels.
}
	\figurelabel{cutoffs}
\end{figure}

Notably, using the value of 1e-10
that, as a filter,
resulted in accurate recovery of loss and index values
(\figureref{perf-comparison}),
introduces a small amount of error,
in particular at low and high values of the index
(\figureref{cutoffs}A).
This suggests that
a strategy of running
critical point-finding methods until termination
and then filtering
gives better results than
a strategy of early termination.
Looser cutoff values
(\figureref{cutoffs}B,C)
result in quantitatively worse
recovery of loss-index values
but qualitatively similar loss-index relationships.
Interestingly, applying no cutoff at all,
as in \figureref{cutoffs}D,
results in a convex upwards relationship
between index and loss,
much like that reported in~\cite{dauphin2014} and~\cite{pennington2017}
for non-linear networks,
despite the fact that the true relationship is linear.


\section{Methods that Work on a Linear Network Fail on a Non-Linear Network}%
\sectionlabel{nonlinear-failure}

Unfortunately,
success on this test problem does not guarantee
success on the original problem of interest.
When applied to a nonlinear network,
even with the same data,
these methods fail.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.67\textwidth]{img/chapter3/nonlinear-failure.pdf}
	\end{center}
	\caption{\textbf{Newton-MR Fails to Find Critical Points
	on a Non-Linear Network}.}
	\textbf{A}. Squared gradient norms across iteration
	for ten runs of Newton-MR applied to a non-linear network.
	\textbf{B}. Loss and index values at 1000 iterations
	for the runs of Newton-MR in \emph{A}.
	\figurelabel{nonlinear-failure}
\end{figure}

\figureref{nonlinear-failure} shows
results for Newton-MR applied to
a network with two hidden layers of sizes $8$ and $16$
and Swish non-linearity~\cite{ramachandran2017},
but the same data as in~\sectionref{dlae-results}.
Results for other algorithms are qualitatively similar.
The squared gradient norms do not drop nearly as low
(greater than 1e-18, versus 1e-30
in the linear case,~\figureref{perf-comparison}A),
even though the iteration budget is doubled.
All runs exhibit a \enquote{plateau},
unlike the successful runs
in~\figureref{perf-comparison}A,
which exhibited linear or faster convergence
to a point at which no further progess could be made.
A handful of experiments with much larger iteration counts
indicated that this plateau persists even as the computational budget increases
(data not shown).
It is unclear whether the loss and index values
(\figureref{nonlinear-failure}B)
recovered in this case are accurate.
The results here are just a single example;
in \sectionref{gfr-results},
we will see that these phenomena recur on many neural network losses.

This motivates a re-analysis of the critical point-finding methods,
in particular in the singular case.
We will see, in the following section,
that the plateau behavior in \figureref{nonlinear-failure}
can be recreated on a very simple test problem,
so long as that problem has a point where
the gradient lies in the Hessian's kernel:
a \emph{gradient-flat} point.
This analysis, and the experiments in
\sectionref{gfr-results} that it motivates,
are also presented in~\cite{frye2020}.


\section{Gradient-Flat Regions can Cause Critical Point-Finding Methods to Fail}%
\sectionlabel{gfr}

In this section,
we introduce and define gradient-flat points and
explain why they are problematic for second-order
critical point-finding methods
(\sectionref{theory}),
with the help of a low-dimensional example
to build intuition
(\sectionref{toy}).
In numerical settings and in high dimensions,
approximately gradient-flat points are also important,
and so we define a quantitative index of gradient-flatness
based on the residual norm of the Newton update
(\sectionref{approxgfp}).
Connected sets of these numerically gradient-flat points
are gradient-flat regions,
which cause trouble for
second-order critical point-finding methods.

\subsection{At Gradient-Flat Points, the Gradient Lies in the Hessian's Kernel}%
\sectionlabel{theory}

Critical points are of interest because
they are points where the first-order approximation
of a function  $f$ at a point
$x+\delta$ based on the local information at $x$
\begin{equation}
    f(x + \delta) \approx f(x) + \grad{f}{x}^\top \delta
\end{equation}
is constant, indicating that they are the stationary points
of first-order optimization algorithms
like gradient descent and its variants.

We can similarly understand the behavior of our second-order
critical point-finding methods by considering their stationary points.
In our construction of these methods in
\chapterref{two},
we used a linear approximation of the behavior
of the gradient function
at a point $x + p$ given the local information at a point $x$
\begin{equation}
    \grad{f}{x + p} \approx \grad{f}{x} + \hess{f}{x}p
\end{equation}

Solving for the value of $p$
that makes the left-hand side $0$
gave us the Newton update
\begin{equation}
	\pinv{\hess{f}{x}}\grad{f}{x}
\end{equation}
\noindent which is only $0$,
for a non-singular Hessian,
if $\grad{f}{x}$ is $0$.
For a singular Hessian,
the update $p$ is zero iff $\grad{f}{x}$ is
in the kernel of the pseudoinverse.

In gradient norm minimization,
we search for critical points
by applying first-order optimization methods
to the squared gradient norm function,
$g(\theta) = \sgn{L}{\theta}$.
These methods also make a linear approximation
of the gradient function of $L$.
The gradient function of $g$ is
\begin{equation}
	\grad{g}{x} = \hess{f}{x}\grad{f}{x}
\end{equation}
\noindent As with Newton methods, in the invertible case
the updates are zero iff $\grad{f}{x}$ is 0.
In the singular case,
the updates are zero if the gradient is in the Hessian's kernel.

In the case of invertible Hessians, then,
these methods can guarantee convergence to critical points.
However, neural network Hessians are generally singular,
especially in the overparameterized case~\cite{sagun2017,ghorbani2019}.
In this case,
neither class of methods can guarantee
convergence to critical points
(for GNM, see~\cite{doye2002};
for Newton methods, see~\cite{powell1970,griewank1983}).

What are the stationary points, besides critical points,
for these two method classes in the case of singular Hessians?
It would seem at first that they are different:
for gradient norm minimization,
when the gradient is in the Hessian's kernel;
for Newton-type methods,
when the gradient is in the Hessian's pseudoinverse's kernel.
In fact, however,
these conditions are identical, due to the Hessian's symmetry,
and so both algorithms share a broad class of stationary points.
A proof, based on the construction of the pseudo-inverse in
\sectionref{pinv-svd}, follows.

\begin{theorem}{Kernel Equals Pseudo-Inverse Kernel for Symmetric $M$}%
{kernel-pinvkernel}
	\emph{%
		Let $M\in\R^{n \times n}$ be a symmetric matrix.
		Then
		\begin{equation}
			\ker M  = \ker\pinv{M}
		\end{equation}
	}
\end{theorem}

\noindent \textit{Proof of \thmref{kernel-pinvkernel}}:\\
We first repeat the commutative diagram
relating the SVDs of a matrix and its pseudo-inverse,
specialized to a square matrix.

\begin{center}
    \begin{tikzcd}
	    \R^n \arrow[rr, "M", bend left] \arrow[dd, "V^\top", swap, two heads, bend right]
	    &  & \R^n \arrow[dd, "U^\top", swap, two heads, bend right] \arrow[ll, "M^+", bend left]                       \\
	\ & \  & \\
	\co \ker M \arrow[uu, "V", harpoon, hook, bend right]  \arrow[rr, "\Sigma", bend right]
	&  & \im M \arrow[uu, "U", harpoon, hook, bend right] \arrow[ll, "\inv{\Sigma}", bend right]
    \end{tikzcd}
\end{center}

Note that $\im M$ is also $\co\ker \pinv{M}$,
by reading the diagram counter-clockwise.
The SVD of the transpose of $M$
can be attained by transposing each of the elements of the SVD\@:
\begin{align}
	M^\top
	&= {\left(U \Sigma V^\top\right)}^\top\\
	&= V \Sigma U^\top
\end{align}

This can be summarized in the commutative diagram below.

\begin{center}
    \begin{tikzcd}
	    \R^n \arrow[rr, "M", bend left] \arrow[dd, "V^\top", swap, two heads, bend right]
	    &  & \R^n \arrow[dd, "U^\top", swap, two heads, bend right] \arrow[ll, "M^\top", bend left]                       \\
	\ & \  & \\
	\co \ker M \arrow[uu, "V", harpoon, hook, bend right]  \arrow[rr, "\Sigma", bend right]
	&  & \im M \arrow[uu, "U", harpoon, hook, bend right] \arrow[ll, "\Sigma", bend right]
    \end{tikzcd}
\end{center}

This implies that $\im M = \co\ker M^\top$.
But for a symmetric matrix,
$M = M^\top$,
and so $\co\ker M = \co\ker M^\top = \im M$.
But above, we saw that $\im M = \co\ker \pinv{M}$,
and therefore $\co\ker M = \co\ker \pinv{M}$,
which further implies that $\ker M = \ker \pinv{M}$.
\[\QED\]

These stationary points have been identified previously,
but nomenclature is not standard:
Doye and Wales, studying gradient norm minimization,
call them \emph{non-stationary points}~\cite{doye2002},
since they are non-stationary with respect to the function $f$,
while Byrd et al., studying Newton methods,
call them \emph{stationary points}~\cite{byrd2004},
since they are stationary with respect to the merit function $g$.
To avoid confusion between these incommensurate conventions
or with the stationary points of the function $f$,
we introduce our own terminology.

\begin{definition}{Gradient-Flat Points}{gfp}
	A point $\theta\in\R^n$ is a \emph{gradient-flat} point
	of a twice continuously-differentable function
	$f\from\R^n\to\R$ if
	\begin{equation}
		\grad{f}{\theta} \in \ker \hess{f}{\theta}
	\end{equation}
\end{definition}

This name was chosen because a function is \emph{flat}
when its Hessian is 0, meaning every direction is in the kernel,
and so it is locally flat around a point in a given direction
whenever that direction is in the kernel of the Hessian at that point.
Note that, because $0 \in \ker$ for all matrices,
every critical point is also a gradient-flat point,
but the reverse is not true.
When we wish to explicitly refer to gradient-flat points
which are not critical points,
we will call them \emph{strict} gradient-flat points.
At a strict gradient-flat point, the function is,
along the direction of the gradient,
locally linear up to second order.

There is an alternative view of gradient-flat points
based on the squared gradient norm function $g$.
All gradient-flat points are stationary points
of the gradient norm:
they are zeroes for $\nabla g$.
They may in principle be local minima, maxima, or saddles,
while the global minima of the gradient norm are critical points.
When they are local minima of the gradient norm,
they can be targets of convergence
for methods that use
first-order approximations of the gradient map,
as in gradient norm minimization and in Newton-type methods.
Strict gradient-flat points, then,
can be bad local minima of the gradient norm,
and therefore prevent the convergence of
second-order root-finding methods
to critical points,
just as bad local minima of the loss function
can prevent convergence of first-order optimization methods
to global optima.

Note that Newton methods cannot be demonstrated to converge
only to gradient-flat points~\cite{powell1970,byrd2004}.
Furthermore, Newton convergence can be substantially slowed
when even a small fraction of the gradient
is in the kernel~\cite{griewank1983}.
Below we will see that,
while Newton-MR applied to a neural network loss
sometimes converges to and almost always encounters strict gradient-flat points,
the final iterate is not always either
a strict gradient-flat point or a critical point.

\subsection{Convergence to Gradient-Flat Points in a Quartic Example}%
\sectionlabel{toy}

The difficulties that gradient-flat points pose for Newton methods
can be demonstrated with a polynomial example in two dimensions,
plotted in \figureref{toy}A.
Below,
we will characterize
the strict gradient-flat (\failcolor{})
and critical (\successcolor{}) points of this function
(\figureref{toy}A).
Then, we will observe the behavior of Newton-MR
when applied to it (\figureref{toy}B)
and note similarities to the results in \figureref{nonlinear-failure}.
We will use this simple, low-dimensional example
to demonstrate principles useful
for understanding the results of applying
second-order critical point-finding methods to more complex,
higher-dimensional neural network losses.

\begin{example}{A Quartic Polynomial with a Gradient-Flat Point}{toy}
	\begin{equation}\equationlabel{toy}
	    f(x, y) = 1/4 x^4 - 3x^2 + 9x + 0.9y^4 + 5y^2 + 40
	\end{equation}
\end{example}

\begin{figure}
	\includegraphics[width=0.9\linewidth]{img/chapter3/toy-problem.pdf}
	\caption{\textbf{Stationarity of and Convergence to
	a Strict Gradient-Flat Point on a Quartic Function}.}%
	{\textbf{A}:
	Critical and strict gradient-flat points
	of quartic $f(x,y)$ (defined in \equationref{toy}).
	Central panel:
	$f(x,y)$
	plotted in black and white
	(black, low values; white, high values),
	along with the direction of the Newton update $p$
	as a (notably non-smooth) vector field (red).
	Stationary points of
	the squared gradient norm merit function $g$ are indicated:
	strict gradient-flat points in \failcolor{},
	the critical point in \successcolor{}.
	Top and bottom panels:
	The value (top) and squared gradient norm (bottom)
	of $f$ as a function of $x$ value
	with $y$ fixed at 0.
	The $x$ axis is shared between panels.
	\textbf{B}:
	Performance and trajectories of Newton-MR
	on~\equationref{toy}.
	Runs that terminate near a strict gradient-flat point
	are in \failcolor{},
	while those that terminate near
	a critical point are in \successcolor{}.
	Central panel:
	Trajectories of Newton-MR laid over
	$f(x, y)$.
	$x$ and $y$ axes are shared with the central panel of
	\emph{A}.
	Initial values indicated with scatter points.
	Top and bottom panels:
	Function values (top) and squared gradient norms (bottom)
	of Newton-MR trajectories as a function of iteration.
	The $x$ axis is shared between panels.}\figurelabel{toy}
\end{figure}

\equationref{toy} is plotted in~\figureref{toy}A,
central panel.
This quartic function has two affine subspaces
of points with non-trivial Hessian kernel,
defined by $[\pm\sqrt{2}, y]$.
The kernel points along the $x$ direction and so
is orthogonal to this affine subspace at every point.
As a function of $y$, $f$ is convex,
with one-dimensional minimizers at $y=0$.
The strict gradient-flat points occur at the intersections
of these two sets:
one strict gradient-flat point at $[\sqrt{2}, 0]$,
which is a local minimum of the gradient norm,
and one at $[-\sqrt{2}, 0]$,
which is a saddle of the same
(\figureref{toy}A, \failcolor{} points, all panels).
In the vicinity of these points, the gradient is,
to first order, constant along the $x$-axis,
and so the function is locally linear or flat.
These points are gradient-flat but
neither is a critical point of $f$.
The only critical point is located at the minimum of the polynomial,
at $[-3, 0]$
(\figureref{toy}A, \successcolor{} point, all panels),
which is also a global minimum of the gradient norm.
The affine subspace that passes through
$[-\sqrt{2}, 0]$ divides the space into two
basins of attraction, loosely defined,
for second-order methods:
one, with initial $x$-coordinate $x_0<-\sqrt{2}$,
for the critical point of $f$
and the other for the strict gradient-flat point.
Note that the vector field in the central panel shows update directions
for the pure Newton method,
which can behave extremely poorly in the vicinity
of singularities~\cite{powell1970,griewank1983},
often oscillating and converging very slowly
or diverging.

Practical Newton methods,
like those introduced in \chapterref{two}
and used in \sectionref{dlae-results} above,
use techniques like
damping and line search to improve behavior.
To demonstrate how a practical Newton method
behaves on this function,
we focus on the case of Newton-MR\@.
Results are qualitatively similar for damped Newton.

The results of applying Newton-MR
to~\equationref{toy} are shown in%
~\figureref{toy}B.
The gradient-flat point is attracting
for some trajectories
(\failcolor{}),
while the critical point is attracting for others
(\successcolor{}).
For trajectories that approach the strict gradient-flat point,
the gradient norm does not converge to 0,
but converges to a non-zero value near 10
(\failcolor{} trajectories; \figureref{toy}B, bottom panel).
This value is typically several orders of magnitude lower
than the initial point, and so would appear to be close to 0
on a linear scale that includes the gradient norm of the initial point.
Since log-scaling of loss functions is uncommon in machine learning,
as losses do not always have minima at 0,
second-order methods apporaching gradient-flat points
can appear to converge to critical points
if typical methods for visually assessing convergence are used.

There are two interesting and atypical behaviors worth noting.
First, the trajectories tend to oscillate
in the vicinity of the gradient-flat point
and converge more slowly
(\figureref{toy}B, central panel, \failcolor{} lines).
Updates from points close to the affine subspace where the Hessian has a kernel,
and so which have an approximate kernel themselves,
sometimes jump to points where the Hessian doesn't have an approximate kernel.
This suggests that, when converging towards a gradient-flat point,
the degree of flatness will change iteration by iteration.
Second, some trajectories begin in the nominal basin
of attraction of the gradient-flat point
but converge to the critical point
(\figureref{toy}B, central panel, \successcolor{} points
with $x$-coordinate $>-\sqrt{2}$).
This is because the combination of back-tracking line search
and large proposed step sizes means that occasionally,
very large steps can be taken, based on non-local features of the function.
Indeed,
back-tracking line search is a limited form of global optimization
and the ability of line searches
to change convergence behaviors predicted from local properties
on nonconvex problems
is known~\cite{nocedal2006}.
Since the back-tracking line search is based on the gradient norm,
the basin of attraction for the true critical point,
which has a lower gradient norm than the gradient-flat point,
is much enlarged relative to that for the gradient-flat point.
This suggests that Newton methods
using the gradient norm merit function will be biased
towards finding gradient-flat points
that also have low gradient norm.

Finally, we note that the behavior of the failed runs
in the bottom panel of \figureref{toy}B
matches that of the runs in \figureref{nonlinear-failure}.
In particular, after a brief period of rapid improvement,
the squared gradient norm plateaus at relatively large value
given by the value of the gradient norm at the gradient-flat point
(in this case, 1e1).

\subsection{Approximate Gradient-Flat Points Form Gradient-Flat Regions}%
\sectionlabel{approxgfp}
Analytical arguments focus on exactly gradient-flat points,
where the Hessian has an exact kernel
and the gradient is entirely within it.
In numerical settings,
it is almost certain no matrix will have an exact kernel,
due to rounding error.
For the same reason, the computed gradient vector will generically not lie entirely
within the exact or approximate kernel.
However, numerical implementations of second-order methods
will struggle even when there is no exact kernel
or when the gradient is only partly in it,
and so a numerical index of flatness is required.
This is analogous to the requirement to specify a tolerance
for the norm of the gradient when deciding whether to consider a point
an approximate critical point or not.

We quantify the degree of gradient-flatness of a point
by means of
the \emph{relative residual norm} ($r$)
and the \emph{relative co-kernel residual norm} ($r_H$)
for the Newton update direction $p$.
The vector $p$ is an inexact solution to the Newton system $Hp + g = 0$,
where $H$ and $g$ are the current iterate's Hessian and gradient.
The residual is equal to $Hp + g$,
and the smaller its norm, the better $p$ is as a solution.
The co-kernel residual is equal to the Hessian times the residual,
and so ignores any component in the kernel of the Hessian.
Its norm quantifies the quality of an inexact Newton solution
in the case that the gradient lies partly in the Hessian kernel,
the unsatisfiable case, where $Hp \neq -g$ for any $p$.
When the residual is large but the co-kernel residual is small
(norms near 1 and 0, respectively, following suitable normalization),
then we are at a point where
the gradient is almost entirely in the kernel of the Hessian:
an approximate gradient-flat point.
In the results below, we consider a point approximately gradient-flat
when the value of $r_H$ is below 5e-4
while the value of $r$ is above $0.9$.
We emphasize that numerical issues for second-order methods
can arise even when the degree of gradient-flatness
is small.

The relative residual norm, $r$,
measures the size of the error
of an approximate solution to the Newton system:
\begin{equation}
    r(p) = \frac{\norm{Hp + g}}{\norm{H}_F \norm{p} + \norm{g}}
\end{equation}
\noindent where $\norm{M}_F$ of a matrix $M$ is its Frobenius norm.
Since all quantities are non-negative,
$r$ is non-negative;
because the denominator bounds the numerator,
by the triangle inequality and the compatibility
of the Frobenius and Euclidean norms,
$r$ is at most 1.
For an exact solution of the Newton system $p^\star$,
$r(p^\star)$ is 0, the minimum value,
while $r(0)$ is 1, the maximum value.
Note that small values of $\norm{p}$
do not imply large values of this quantity,
since $\norm{p}$ goes to 0 when a Newton method
converges towards a critical point,
while $r$ goes to 0.

When $g$ is partially in the kernel of $H$,
the Newton system is unsatisfiable,
as $g$ will also be partly in the co-image of $H$,
the linear subspace into which $H$ cannot map any vector.
In this case, the minimal value for $r$ will no longer be $0$.
The optimal solution for $\norm{Hp + g}$
instead has the property that its residual
is $0$ once restricted to the co-kernel of $H$.
This co-kernel residual can be measured by applying the matrix $H$
to the residual vector $Hp + g$.
After normalization, it becomes
\begin{equation}
    r_H(p) = \frac{\norm{H(Hp + g)}}{\norm{H}_F \norm{Hp + g}}
\end{equation}
\noindent Note that this value is also small when the gradient lies
primarily along the eigenvalues of smallest magnitude.
On each internal iteration, MR-QLP checks whether
either of these values is below a tolerance level
(the hyperparameter \texttt{rtol}; in our experiments, 5e-4)
and if either is, it ceases iteration.
With exact arithmetic,
either one or the other of these values
should go to 0 within a finite number of iterations;
with inexact arithmetic, they should just become small.
See~\cite{choi2011} for details.

Under this relaxed definition of gradient-flatness,
there will be a neighborhood of approximate gradient-flat points
around a strict, exact gradient-flat point
for functions with Lipschitz gradients and Hessians.
Furthermore, there might be connected sets of non-null Lebesgue measure
which all satisfy the approximate gradient-flatness condition
but none of which satisfy the exact gradient-flatness condition.
We call both of these \emph{gradient-flat regions}.

There are multiple reasonable numerical indices of flatness besides
the definition above.
For example,
the Hessian-gradient regularity condition in~\cite{roosta2018},
which is used to prove convergence of Newton-MR,
would suggest creating a basis for the approximate kernel of the Hessian
and projecting the gradient onto it.
Alternatively, one could compute the Rayleigh quotient
of the gradient with respect to the Hessian.
Our method has the advantage of being computed as part of the Newton-MR algorithm.
It furthermore avoids diagonalizing the Hessian or the specification
of an arbitrary eigenvalue cutoff.
The Rayleigh quotient can be computed with only one Hessian-vector product,
plus several vector-vector products,
so it might be a superior choice for larger problems where
computing a high-quality inexact Newton step is computationally infeasible.

\section{Gradient-Flat Regions Abound on Several Neural Network Losses}%
\sectionlabel{gfr-results}

To determine whether gradient-flat regions are
responsible for the poor behavior of Newton methods
on deep neural network (DNN) losses
demonstrated in~\figureref{nonlinear-failure},
we applied Newton-MR to the losses of several neural networks.
We focused on Newton-MR because we
found that a damped Newton method like that in~\cite{dauphin2014}
performed poorly, as reported for the XOR problem in~\cite{coetzee1997}.

\subsection{Gradient-Flat Regions on an Autoencoder Loss}

We first present results for two networks
trained on 10k MNIST~\cite{lecun2010} images downsized to 4$\times$4,
similar to the downsized datasets in~\cite{dauphin2014,pennington2017}.
Images were cropped to 20$\times$20 and rescaled to 4$\times$4
using PyTorch~\cite{paszke2019},
then $z$-scored.
This was done to improve the condition of the data covariance matrix,
which is very poor for MNIST due to the low variance in the border pixels.
It also reduced the size of the network.
Non-linear classification networks trained on this down-sampled data
could still obtain accuracies above 90\%,
better than the performance of logistic regression ($\approx87\%$).

First, we consider a nonlinear autoencoder.
This network had two hidden layers of $8$ and $16$ units,
used Swish non-linearities.
Unlike previous networks, this network had biases,
meaning each layer performed an affine rather than a linear transformation.
Gradient norms for the first 100 iterations of Newton-MR
applied to this loss
appear in \figureref{gfp-dnn}A.
As in the non-linear autoencoder applied to the multivariate Gaussian data
(\figureref{nonlinear-failure}),
we found that, after 500 iterations,
all of the runs had squared gradient norms
over 10 orders of magnitude greater than the typical
values observed after convergence in the linear case
($<$1e-30, \figureref{perf-comparison}A).
14\% of runs terminated with squared gradient norm
below the filtering value of 1e-10
and so found likely critical points
(\successcolor{}).
Twice as many runs terminated above that cutoff
but terminated in a gradient-flat region
(28\%, \failcolor{}),
while the remainder were above
the cutoff but were not in a gradient-flat region
at the final iteration
(black).

\begin{figure}[htpb]
	\begin{center}
	    \includegraphics[width=0.7\linewidth]{img/chapter3/gfp-dnn.pdf}
	\end{center}
	\caption{\textbf{Critical Point-Finding Methods
	More Often Find Gradient-Flat Regions
	on a Neural Network Loss}.}{%
	\textbf{A}:
	Squared gradient norms across the first 100 iterations of Newton-MR
	for 100 separate runs on an auto-encoder loss.
	Gradient norms were flat after 100 iterations.
	See~\appref{app:networks} for details.
	Runs that terminate with squared gradient norm below 1e-10,
	i.e.~at a candidate critical point, in \successcolor{}.
	Runs that terminate above that cutoff and with $r$ above $0.9$,
	i.e.~in a gradient-flat region, in \failcolor{}.
	All other runs in black.
	Asterisks indicate trajectories in $\emph{B}$.
	\textbf{B}:
	The relative residual norm $r$,
	an index of gradient-flatness,
	for the approximate Newton update
	computed by MR-QLP at each iteration
	(solid lines)
	for three representative traces.
	Values are local averages with a window size of 10 iterations.
	Raw values are plotted with reduced opacity underneath.
	Top: non-flat, non-critical point (black).
	Middle: flat, non-critical point (\failcolor{}).
	Bottom: flat, critical point (\successcolor{}).
	\textbf{C}:
	Empirical cumulative distribution functions for
	the final (top) and maximal (bottom) relative residual norm $r$ observed
	during each run of Newton-MR\@.
	Values above the cutoff for approximate gradient-flatness, $r>0.9$,
	in \failcolor{}.
	Observations from runs that terminated below the cutoff for critical points,
	$\sgn{L}{\theta} <$ 1e-10,
	indicated with \successcolor{} ticks.
	\textbf{D}:
	Loss and index for the maximally gradient-flat points
	obtained during application of Newton-MR\@.
	Points with squared gradient norm below 1e-10 in \successcolor{}.
	Other points colored by their gradient-flatness:
	points above $0.9$ in \failcolor{}, points below in black.
	Only points with squared gradient norm below 1e-4 shown.
	}\figurelabel{gfp-dnn}
\end{figure}

The relative residual norm for the Newton solution, $r$,
is an index of gradient-flatness;
see \sectionref{approxgfp} for details.
The values of $r$ for every iteration
of Newton-MR are shown for three representative traces
in \figureref{gfp-dnn}B.
In the top trace,
$r$ is close to 0,
indicating that the iterates are not in a gradient-flat region
($r\ll0.9$, black).
Newton methods can be substantially slowed when even a small fraction
of the gradient is in the kernel~\cite{griewank1983}
and can converge to points that are not gradient-flat~\cite{byrd2004}.
By contrast, in the middle trace (\failcolor{}),
the value of $r$ approaches $1$,
indicating that almost the entirety of the gradient is in the kernel.
This run terminated in a gradient-flat region,
at effectively an exactly gradient-flat point.
Further, the squared gradient norm at 500 iterations, 2e-5,
is five orders of magnitude higher than the cutoff,
1e-10.
This is smaller than the minimum observed during optimization
of this loss (squared gradient norms between 1e-4 and 5e1),
indicating the presence of non-critical gradient-flat regions
with very low gradient norm.
Critical point-finding methods that disqualify points
on the basis of their norm will both converge to
and accept these points,
even though they need not be near true critical points.
In the bottom trace (\successcolor{}),
the behavior of $r$ is the same,
while the gradient norm drops much lower, to 3e-13,
suggesting convergence to a gradient-flat region around a critical point
that has an approximately singular Hessian.

We found that 99 of 100 traces included a point
where at least half of the gradient was in the kernel,
according to our residual measure,
while 89\% of traces included a point that had
a residual greater than $0.9$,
and 50\% included a point with $r > 0.99$
(\figureref{gfp-dnn}C, bottom).
This demonstrates that
there are many regions of substantive gradient-flatness,
in which second-order critical point-finding methods could be substantively slowed.

The original purpose of applying these critical point-finding methods
was to determine whether the no-bad-local-minima property held
for this loss function, and more broadly to characterize
the relationship at the critical points
between the loss and the local curvature,
summarized via the Morse index.
If we look at either the iterates with the highest gradient-flatness
(\figureref{gfp-dnn}D),
we find that the qualitative features of the loss-index relationship reported in%
~\cite{dauphin2014} and~\cite{pennington2017} are recreated:
convex shape, small spread at low index that increases for higher index,
no minima or near-minima at high values of the loss.
However, our analysis suggests that the majority of these points are not critical points
but either strict gradient-flat points (\failcolor{})
or simply points of spurious or incomplete
Newton convergence (black).
The approximately critical points we do see (\successcolor{})
have a very different loss-index relationship:
their loss is equal to the loss of a network
that has constant output equal to the mean of the data,
and their index is low, but not 0.
This suggests that the results presented in%
~\cite{dauphin2014} and~\cite{pennington2017}
are not evidence of the reported loss-index relationship at
critical points of neural network losses.

\subsection{Gradient-Flat Regions on a Classifier Loss}

We repeated these experiments on a fully-connected classifier
(aka a \emph{multilayer perceptron} or MLP)
trained on the same MNIST images via the cross-entropy cost function.
This network also had two hidden layers of 12 and 8 units,
used Swish activations, and included biases.
Unlike all networks considered up to this point,
the regularizer $r$ was non-zero.
Losses based on the cross entropy cost function
can have critical points of infinite norm
in the absence of regularization,
and so we applied $\ell_2$ norm regularization:
$r(\theta) = \snorm{\theta}$.

\begin{figure}[!hp]
	\begin{center}
		\includegraphics[width=0.7\linewidth]{img/chapter3/gfp-mlp.pdf}
	\end{center}
	\caption{\textbf{Gradient-Flat Regions Also Appear on an MLP Loss}.}{%
	\textbf{A}:
	Squared gradient norms across the first 100 iterations of Newton-MR
	for 60 separate runs on an MLP loss.
	Runs that terminate with squared gradient norm below 1e-10
	in \successcolor{}.
	Runs that terminate above that cutoff and with $r$ above $0.9$,
	in \failcolor{}.
	All other runs in black.
	Asterisks indicate trajectories in $\emph{B}$.
	\textbf{B}:
	The relative residual norm $r$,
	for the approximate Newton update
	computed by MR-QLP at each iteration
	for three representative traces.
	Values are local averages with a window size of 10 iterations.
	Raw values are plotted with reduced opacity underneath.
	Top: non-flat, non-critical point (black).
	Middle: flat, non-critical point (\failcolor{}).
	Bottom: non-flat, critical point (\successcolor{}).
	\textbf{C}:
	Empirical cumulative distribution functions for
	the final (top) and maximal (bottom) relative residual norm $r$.
	Values above the cutoff for approximate gradient-flatness, $r>0.9$,
	in \failcolor{}.
	Observations from runs that terminated below the cutoff for critical points,
	$\sgn{L}{\theta} <$ 1e-10,
	indicated with \successcolor{} ticks.
	\textbf{D}:
	Loss and index for the points found
	after 500 iterations of Newton-MR\@.
	Colors as in top-left; only points with squared gradient norm below 1e-4 shown.
	Note that color is determined by
	the value of $r$ on the last iteration,
	rather than on the iteration with maximal $r$,
	as in \figureref{gfp-dnn}.%
	}\figurelabel{gfp-mlp}
\end{figure}

We again found that the performance of
the Newton-MR critical point-finding algorithm was poor
(\figureref{gfp-mlp}A)
and that around 90\% of runs encountered a point
with gradient-flatness above $0.9$
(\figureref{gfp-mlp}C, bottom row).
However, we observed that fewer runs terminated
at a gradient-flat point
(\figureref{gfp-mlp}C, top row).

In many traces,
the value of $r$ oscillates from values close to 1
to middling values,
indicating that the algorithm is bouncing in and out
of one or more gradient-flat regions,
rather than because of another type of spurious Newton convergence.
This can occur when the final target of convergence
given infinite iterations
is a gradient-flat point,
as in the example in~\sectionref{toy}.
This behavior is evident in the traces presented in the top and bottom rows of
\figureref{gfp-mlp}B.
Even for the autoencoding network presented in \figureref{gfp-dnn},
not all traces exhibit the monotonic behavior for the value of $r$
apparent in \figureref{gfp-dnn}B.

If we measure the loss-index relationship at the
(mostly non-gradient-flat) final points,
we see the same pattern as in~\figureref{gfp-dnn}:
convex shape, separation of critical points from gradient-flat points
(\figureref{gfp-mlp}D).
This also holds if we look at the maximally flat points,
as in~\figureref{gfp-dnn}D,
or if we look at the final iterates
of the traces in \figureref{gfp-dnn}
(neither results shown).
This underlines a particular problem with detecting
convergence issues caused by a gradient-flat region.
On any given iterate, the algorithm may be inside
or outside the gradient-flat region,
so it is insufficient just to examine the degree of flatness
on one iteration,
e.g.~the final iteration, as determined by computational budget.

\subsection{Gradient-Flat Regions on an Over-Parameterized Loss}

Many contemporary neural networks have extremely large parameter counts:
in the millions and tens of millions.
By varying definitions of the relationship
between parameter count and size or complexity of the dataset,
these networks are \emph{over-parameterized}.
As described in \sectionref{alternative},
recent theoretical results have suggested that,
even in the possible presence of bad local minima,
neural networks might be easily trainable if they are over-parameterized.
It is furthermore known that increasing the number of parameters
while holding the dataset constant increases the size of the Hessian kernel%
~\cite{sagun2017}.
The loss function of such networks has not been considered from the
critical point perspective.

We repeated our critical point-finding experiments
on the loss function of a fully-connected classifier on a small subset
of 50 0s and 1s from the MNIST dataset.
The images were PCA-downsampled to 32 dimensions using sklearn%
~\cite{pedregosa2011}
the labels permuted, turning the classification task
into a memorization task,
as in~\cite{zhang2016}.
This classifier had two hidden layers of sizes 32 and 4,
no biases, also used Swish activations,
and was trained with $\ell_2$ regularization.
In this setting, the network is over-parameterized in several senses:
it has a hidden layer almost as wide as the number of points in the dataset (32 vs 50),
it has more parameters than there are examples in the dataset (1160 vs 50),
and it is also capable of achieving 100\% accuracy
on the task of random label memorization.

We again observe that the majority of runs of Newton-MR
terminate with high squared gradient norm
(33 out of 50 runs above 1e-8)
and a similar fraction
(31 out of 50 runs)
encounter gradient-flat points
(\figureref{gfp-mem}A and C, bottom panel).
The loss-index relationship looks qualitatively different,
as might be expected for a task with random labels.
Notice the appearance of a bad local minimum:
the \successcolor{} point at index 0 and loss $\ln(2)$.

\begin{figure}[!hp]
	\begin{center}
		\includegraphics[width=0.7\linewidth]{img/chapter3/gfp-mem.pdf}
	\end{center}
	\caption{\textbf{Gradient-Flat Regions Also Appear on an Over-Parameterized Loss}.}{%
	\textbf{A}:
	Squared gradient norms across 500 iterations of Newton-MR
	for 50 separate runs on the loss of an over-parameterized network.
	Runs that terminate with squared gradient norm below 1e-8
	in \successcolor{}.
	Runs that terminate above that cutoff and with $r$ above $0.9$,
	in \failcolor{}.
	All other runs in black.
	Asterisks indicate trajectories in $\emph{B}$.
	\textbf{B}:
	The relative residual norm $r$,
	for the approximate Newton update
	computed by MR-QLP at each iteration
	for three representative traces.
	Values are local averages with a window size of 10 iterations.
	Raw values are plotted with reduced opacity underneath.
	Top: non-flat, non-critical point (black).
	Middle: flat, non-critical point (\failcolor{}).
	Bottom: flat, critical point (\successcolor{}).
	\textbf{C}:
	Empirical cumulative distribution functions for
	the final (top) and maximal (bottom) relative residual norm $r$.
	Values above the cutoff for approximate gradient-flatness, $r>0.9$,
	in \failcolor{}.
	Observations from runs that terminated below the cutoff for critical points,
	$\sgn{L}{\theta} <$ 1e-10,
	indicated with \successcolor{} ticks.
	\textbf{D}:
	Loss and index for the points found
	after 500 iterations of Newton-MR\@.
	Colors as in top-left; only points with squared gradient norm below 1e-4 shown.%
	}\figurelabel{gfp-mem}
\end{figure}


\section{Conclusion}%
\sectionlabel{conclusion}

We began by seeking to understand why neural networks are so easily trained:
despite the substantial non-convexity of their loss functions,
first-order optimization methods produce near-global-optima
from random initial points.
In \chapterref{one},
we developed the \enquote{no-bad-local-minima} (NBLM) theory,
which posits that, like certain classes of random functions,
the loss functions of neural networks have no local minima
that are much worse than the global minima.
While numerical experiments in%
~\cite{dauphin2014} and~\cite{pennington2017}
agreed with this hypothesis,
more recent analytical results%
~\cite{ding2019} suggest that it is false.
These experiments relied on the ability to find
the critical points of the loss function,
and so we developed
an understanding of critical point-finding algorithms
in \chapterref{two} and \sectionref{theory}.
This led us to identify gradient-flat regions,
where the gradient is nearly in the approximate kernel of the Hessian,
as a source of trouble for these algorithms:
effectively, bad local minima for the problem of critical point-finding.
We ended this chapter by observing that gradient-flat regions are a prevalent
feature of three prototypical neural network loss functions:
those of an autoencoder (\figureref{gfp-dnn}),
a classifier (\figureref{gfp-mlp}),
and an overparameterized network (\figureref{gfp-mem})
applied to versions of the MNIST dataset.
We now conclude the thesis by considering the implications
of our observations for critical point-finding experiments,
for the nature of neural network loss functions,
and for the optimization of neural networks.

The strategy of using gradient norm cutoffs to determine
whether a point is near enough to a critical point
for the loss and index to match the true value is natural.
However, in the absence of guarantees on the smoothness of the behavior of
the Hessian (and its spectrum) around the critical point,
the numerical value sufficient to guarantee correctness is unclear.
The observation of gradient-flat regions at extremely low
gradient norm and the separation of these values,
in terms of loss-index relationship,
from the bulk of the observations
suggest that there may be spurious targets
of convergence for critical point-finding methods
even at such low gradient norm.
Alternatively, they may in fact be near real critical points,
and so indicate that the simple, convex picture of loss-index relationship
painted by the numerical results in~\cite{dauphin2014} and~\cite{pennington2017}
is incomplete.
Furthermore, our observation of singular Hessians
at low gradient norm
suggests that some approximate saddle points of neural network losses
may be degenerate (as defined in~\cite{jin2018a})
and non-strict (as defined in~\cite{lee2016}),
which indicates that gradient descent may be attracted to these points,
according to the analyses in~\cite{jin2018a} and~\cite{lee2016}.
These points need not be local minima.
However, in two cases we observe the lowest-index saddles at low values of the loss
(see \figureref{gfp-dnn}, \figureref{gfp-mlp})
and so these analyses still predict that gradient descent will
successfully reduce the loss,
even if it doesn't find a local minimum.
In the third case,
an over-parameterized network \figureref{gfp-mem},
we do observe a bad local minimum,
as predicted in~\cite{ding2019}
for networks capable of achieving 0 training error.

Our results motivate a revisiting of the numerical results
in~\cite{dauphin2014} and~\cite{pennington2017}.
Looking back at Figure 4 of~\cite{dauphin2014},
we see that their non-convex Newton method,
a second-order optimization algorithm designed to avoid saddle points
by reversing the Newton update along directions of negative curvature,
appears to terminate at a gradient norm of order 1.
This is only a single order of magnitude lower than what was observed during training.
It is likely that this point was either in a gradient-flat region
or otherwise had sufficient gradient norm in the Hessian kernel to
slow the progress of their algorithm.
This suggests that second-order methods
designed for optimization,
which use the loss as a merit function,
rather than norms of the gradient,
can terminate in gradient-flat regions.
In this case, the merit function encourages
convergence to points where the loss,
rather than the gradient norm, is small,
but it still cannot guarantee convergence to a critical point.
The authors of~\cite{dauphin2014} do not report a gradient norm cutoff,
among other details needed to recreate their critical point-finding experiments,
so it is unclear to which kind of points they converged.
If, however, the norms are as large as those of the targets of
their non-convex Newton method,
in accordance with our experience with damped Newton methods
and that of~\cite{coetzee1997},
then the loss-index relationships reported in their Figure 1
are likely to be for gradient-flat points,
rather than critical points.

The authors of~\cite{pennington2017}
do report a squared gradient norm cutoff of 1e-6.
This cutoff is right in the middle of the bulk of values
we observed,
and which we labeled gradient-flat regions and
points of spurious convergence,
based on the experiments in \sectionref{dlae-results},
which separates a small fraction of runs from this bulk.
This suggests that some of their putative critical points were gradient-flat points.
Their Figure 6 shows a disagreement between their predictions for the index,
based on a loss-weighted mixture of Wishart and Wigner random matrices,
and their observations.
We speculate that some of this gap is due to their method
recovering approximate gradient-flat points rather than critical points.

Even in the face of results indicating the existence of bad local minima%
~\cite{ding2019},
it remains possible that bad local minima of the loss
are avoided by initialization and optimization strategies.
For example ReLU networks suffer from bad local minima
when one layer's activations are all $0$,
or when the biases are initialized at too small of a value%
~\cite{holzmller2020},
but careful initialization and training can avoid the issue.
Our results do not directly invalidate
this hypothesis,
but they do call the supporting numerical evidence into question.
Our observation of gradient-flat regions on almost every single run
suggests that, while critical points are hard to find
and may even be rare,
regions where gradient norm is extremely small are neither.
For non-smooth losses,
e.g.~those of ReLU networks or networks with max-pooling,
whose loss gradients can have discontinuities,
critical points need not exist,
but gradient-flat regions may.
Indeed, in some cases, the only differentiable minima
in ReLU networks are also flat%
~\cite{laurent2017}.

Other types of critical point-finding methods are not necessarily
attracted to gradient-flat regions, in particular Newton homotopy methods
(first used on neural networks in the 90s~\cite{coetzee1997},
then revived in the 2010s~\cite{ballard2017,mehta2018b}),
which are popular in algebraic geometry~\cite{bates2013}.
However, singular Hessians still cause issues:
for a singular Hessian $H$, the curve to be continued by the homotopy
becomes a manifold with dimension 1 + $\co\rk\left(H\right)$,
and orientation becomes more difficult.
This can be avoided by removing the singularity of the Hessian,
e.g.~by the randomly-weighted regularization method in~\cite{mehta2018a}.
However, while these techniques may make it possible to find
critical points,
they fundamentally alter the loss surface,
limiting their utility in drawing conclusions about other features
of the loss.

The authors of~\cite{sagun2017} emphasize that
when the Hessian is singular everywhere,
the notion of a basin of attraction is misleading,
since targets of convergence form
connected manifolds
and some assumptions in theorems guaranteeing first-order convergence
become invalid~\cite{jin2018a},
though with sufficient, if unrealistic, over-parameterization
convergence can be proven~\cite{du2018}.
They speculate that a better approach
to understanding the behavior of optimizers
focuses on their exploration of the sub-level sets of the loss.
Our results corroborate that speculation and
further indicate that this flatness means using second-order methods
to try to accelerate exploration of these regions
in search of minimizers
is likely to fail:
the alignment of the gradient with the Hessian's approximate kernel
will tend to produce extremely large steps, for some methods,
or no acceleration and even convergence to non-minimizers,
for others.

Our observation of ubiquitous gradient-flatness further
provides an alternative explanation
for the success and popularity
of approximate second-order optimizers for neural networks,
like K-FAC~\cite{martens2015},
which uses a layerwise approximation to the Hessian.
These methods are typically motivated by appeals to the computational cost
of even Hessian-free exact second-order methods
and their brittleness in the stochastic (non-batch) setting.
However, exact second-order methods are only justified
when the second-order model is good,
and at an exact gradient-flat point,
the second-order model can be infinitely bad,
in a sense, along the direction of the gradient.
Approximations need not share this property.
Even more extreme approximations,
like the diagonal approximations in the adaptive gradient family
(e.g.~AdaGrad~\cite{duchi2011}, Adam~\cite{kingma2014}),
behave very reasonably in gradient-flat regions:
they smoothly scale up the gradient in the directions in which
it is small and changing slowly,
without making a quadratic model that is optimal
in a local sense but poor in a global sense.

Overall, our results underscore the difficulty of searching
for critical points of singular non-convex functions,
including deep network loss functions,
and shed new light on other numerical results in this field.
In this setting, second-order methods for finding critical points can fail badly,
by converging to gradient-flat points.
This failure can be hard to detect unless it is specifically measured.
Furthermore, gradient-flat points are generally places where
quadratic approximations become untrustworthy,
and so our observations are of relevance for the
design of exact and approximate second-order optimization methods as well.

\onlyinsubfile{\printbibliography}

\end{document}
